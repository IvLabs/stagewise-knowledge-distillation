# knowledge_distillation

## TODO list
- [x] train teacher network
- [ ] pretrain the child network
- [ ] try using different sized networks (keep decreasing the size of the network, take it where there is a big difference of accuracy between teacher and 
student, then do feature distillation 
- [ ] Use smaller dataset for feature distillation 


