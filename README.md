# knowledge_distillation

## TODO list
- [x] train teacher network
- [ ] pretrain the child network
- [ ] try using different sized networks (keep decreasing the size of the network, take it where there is a big difference of accuracy between teacher and 
student, then do feature distillation 
- [ ] Use smaller dataset for feature distillation 



(if nothing works out we'll take this as a paper reimplementation of https://arxiv.org/abs/1412.6550 so no harm)


