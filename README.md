# knowledge_distillation

## TODO list
- [x] train teacher network

- [ ] try using different sized networks (keep decreasing the size of the network, take it where there is a big difference of accuracy between teacher and 
student, then do feature distillation 
- [ ] Use smaller dataset for feature distillation 

