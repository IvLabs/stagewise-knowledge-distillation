{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "torch.cuda.set_device(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage should be in 0 to 5 and for 0, use -1 (this is due to inconsistency in the model generated by PyTorch)\n",
    "hyper_params = {\n",
    "    \"stage\": 0,\n",
    "    \"repeated\": 1,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 1,\n",
    "    \"learning_rate\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch(name:str, tensor):\n",
    "    new = tensor.clone()\n",
    "    np.save(name, new.detach().cpu().numpy())\n",
    "    \n",
    "def load_np_torch(name):\n",
    "    return torch.from_numpy(np.load(str(name)))\n",
    "\n",
    "def save_features(model, data, stage):\n",
    "    for i in tqdm(range(len(data.train_ds))):\n",
    "        pred = model(data.train_ds[i][0].data.cuda().unsqueeze(0))\n",
    "        save_torch(path_tn_saved_feat/f'{stage}/{data.train_ds.names[i]}.npy', pred)\n",
    "        \n",
    "    for i in tqdm(range(len(data.valid_ds))):\n",
    "        pred = model(data.valid_ds[i][0].data.cuda().unsqueeze(0))\n",
    "        save_torch(path_val_saved_feat/f'{stage}/{data.valid_ds.names[i]}.npy', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_feat = path/'train_feat'\n",
    "path_val_feat = path/'val_feat'\n",
    "\n",
    "path_tn_saved_feat = path/'train_saved_feat'\n",
    "path_val_saved_feat = path/'val_saved_feat'\n",
    "\n",
    "\n",
    "model_save_dir = path/f'saved_models'\n",
    "model_save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "path_tn_saved_feat.mkdir(exist_ok=True)\n",
    "path_val_saved_feat.mkdir(exist_ok=True)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    (path_tn_saved_feat/str(i)).mkdir(exist_ok=True)\n",
    "    (path_val_saved_feat/str(i)).mkdir(exist_ok=True)\n",
    "    \n",
    "# path_tn_saved_feat.ls()[1].ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, path, stage:int, train_val:str=\"train\"):\n",
    "        super().__init__()\n",
    "        assert stage < 7\n",
    "        if stage == 1: self.feat1 = path/f'{train_val}_feat/{stage-1}'\n",
    "        else: self.feat1 = path/f'{train_val}_saved_feat/{stage-1}'\n",
    "        if not stage == 6:\n",
    "            self.feat2 = path/f'{train_val}_feat/{stage}'\n",
    "        else: self.feat2 = None \n",
    "        self.stage = stage\n",
    "        self.label_list = os.listdir(str(path/'train'))\n",
    "        self.names = list(map(lambda x: x.stem, self.feat1.ls()))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feat1.ls())\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         if self.stage == 1: feature1 = torch.from_numpy(\n",
    "#             np.array(Image.open(self.feat1.ls()[idx]).resize((224, 224))))/255\n",
    "#         else: \n",
    "        feature1 = load_np_torch(self.feat1.ls()[idx]).squeeze(0)\n",
    "            \n",
    "        if self.stage == 6:\n",
    "            feature2 = self.label_list.index((str(self.feat1.ls()[idx]).split('/')[-1]).split('_')[0])\n",
    "            feature2 = torch.tensor(feature2)\n",
    "        else: \n",
    "            feature2 = load_np_torch(self.feat2.ls()[idx]).squeeze(0)\n",
    "        return feature1, feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(stage):\n",
    "    train_dataset = FeatureDataset(path, stage, \"train\")\n",
    "    val_dataset = FeatureDataset(path, stage, \"val\")\n",
    "\n",
    "    try: \n",
    "        train_dataset[1][0].shape is type(torch.Size)\n",
    "        val_dataset[1][0].shape is type(torch.Size)\n",
    "    except Exception as e: \n",
    "        print(Exception(e))\n",
    "        print(Warning('test failed: check dataset, see if the feature arrays are present'))\n",
    "    return DataBunch.create(train_dataset, val_dataset, bs=64, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =get_datasets(stage=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = [conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "                    nn.MaxPool2d(3, 2, padding = 1),\n",
    "                    conv_(64),\n",
    "                    conv_and_res(64, 128),\n",
    "                    conv_and_res(128, 256),\n",
    "                    conv_and_res(256, 512),\n",
    "                    nn.Sequential(AdaptiveConcatPool2d(),\n",
    "                    Flatten(),\n",
    "                    nn.Linear(2 * 512, 256),\n",
    "                    nn.Linear(256, hyper_params[\"num_classes\"]))\n",
    "                    ]\n",
    "stages = [(1, 0), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='160' class='' max='201', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      79.60% [160/201 06:43<01:43 0.3444]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nets = []\n",
    "training_id = 1\n",
    "save_dir = path/f'saved_models/{training_id}/'\n",
    "losses_save_dir = path/f'saved_models/{training_id}/losses/'\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "losses_save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for stage in stages:\n",
    "\n",
    "    data = get_datasets(stage=stage[0])\n",
    "    if stage[0] <6: continue \n",
    "#     break \n",
    "    model = net[stage[1]].cuda()\n",
    "    if stage[0] < 6:\n",
    "        learn = Learner(data, model, loss_func=nn.MSELoss())\n",
    "    else: \n",
    "        learn = Learner(data, model, loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "        \n",
    "    learn.fit_one_cycle(hyper_params[\"num_epochs\"],hyper_params[\"learning_rate\"])\n",
    "    \n",
    "    model = learn.model \n",
    "    nets.append(model)\n",
    "    pd.DataFrame(learn.recorder.losses).to_csv(str(losses_save_dir/f'train_stage_{stage[0]}.csv'))\n",
    "    pd.DataFrame(learn.recorder.losses).to_csv(str(losses_save_dir/f'val_stage_{stage[0]}.csv'))\n",
    "    torch.save(model, str(save_dir/f'model_part_{stage[1]}.pth'))\n",
    "    save_features(model, data, stage=stage[0])\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "final = nn.Sequential(*nets)\n",
    "torch.save(final, str(save_dir/f'complete_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FASTAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
