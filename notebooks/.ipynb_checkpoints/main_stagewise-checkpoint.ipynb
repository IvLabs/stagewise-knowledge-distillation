{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "# stage should be in 0 to 5 (5 for classifier stage)\n",
    "hyper_params = {\n",
    "    \"stage\": 5,\n",
    "    \"repeated\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "data = ImageDataBunch.from_folder(path, train = 'train', valid = 'val', bs = hyper_params[\"batch_size\"], size = 224, ds_tfms = tfms).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "True\n",
      "5.0.2.weight torch.Size([512])\n",
      "True\n",
      "5.0.2.bias torch.Size([512])\n",
      "True\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "True\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "True\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "True\n",
      "8.weight torch.Size([256, 1024])\n",
      "False\n",
      "8.bias torch.Size([256])\n",
      "False\n",
      "9.weight torch.Size([10, 256])\n",
      "False\n",
      "9.bias torch.Size([10])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "# learn.summary()\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "if hyper_params['stage'] != 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "    net.load_state_dict(torch.load(filename, map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == str(hyper_params['stage'] + 1) and hyper_params['stage'] != 0 :\n",
    "        param.requires_grad = True\n",
    "    elif name[0] == str(hyper_params['stage']) and hyper_params['stage'] == 0 : \n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "              ReLU-2         [-1, 64, 112, 112]               0\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-9           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "         ResBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12          [-1, 128, 28, 28]          73,728\n",
      "             ReLU-13          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-14          [-1, 128, 28, 28]             256\n",
      "           Conv2d-15          [-1, 128, 28, 28]         147,456\n",
      "             ReLU-16          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
      "         ResBlock-18          [-1, 128, 28, 28]               0\n",
      "           Conv2d-19          [-1, 256, 14, 14]         294,912\n",
      "             ReLU-20          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-21          [-1, 256, 14, 14]             512\n",
      "           Conv2d-22          [-1, 256, 14, 14]         589,824\n",
      "             ReLU-23          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-24          [-1, 256, 14, 14]             512\n",
      "         ResBlock-25          [-1, 256, 14, 14]               0\n",
      "           Conv2d-26            [-1, 512, 7, 7]       1,179,648\n",
      "             ReLU-27            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-28            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-29            [-1, 512, 7, 7]       2,359,296\n",
      "             ReLU-30            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-31            [-1, 512, 7, 7]           1,024\n",
      "         ResBlock-32            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-33            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-34            [-1, 512, 1, 1]               0\n",
      "AdaptiveConcatPool2d-35           [-1, 1024, 1, 1]               0\n",
      "          Flatten-36                 [-1, 1024]               0\n",
      "           Linear-37                  [-1, 256]         262,400\n",
      "           Linear-38                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,996,938\n",
      "Trainable params: 9,536\n",
      "Non-trainable params: 4,987,402\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 40.03\n",
      "Params size (MB): 19.06\n",
      "Estimated Total Size (MB): 59.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x, y = next(iter(data.train_dl))\n",
    "# net(torch.autograd.Variable(x).cuda())\n",
    "summary(net, (3, 224, 224))\n",
    "# print(learn.summary())\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures :\n",
    "    def __init__(self, m) : \n",
    "        self.handle = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, m, inp, outp) : \n",
    "        self.features = outp\n",
    "    def remove(self) :\n",
    "        self.handle.remove()\n",
    "        \n",
    "# saving outputs of all Basic Blocks\n",
    "mdl = learn.model\n",
    "sf = [SaveFeatures(m) for m in [mdl[0][2], mdl[0][4], mdl[0][5], mdl[0][6], mdl[0][7]]]\n",
    "sf2 = [SaveFeatures(m) for m in [net[0], net[2], net[3], net[4], net[5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 112, 112])\n",
      "torch.Size([64, 64, 56, 56])\n",
      "torch.Size([64, 128, 28, 28])\n",
      "torch.Size([64, 256, 14, 14])\n",
      "torch.Size([64, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(data.train_dl))\n",
    "x = torch.autograd.Variable(x).cuda()\n",
    "out1 = mdl(x)\n",
    "out2 = net(x)\n",
    "for i in range(5) : \n",
    "    print(sf[i].features.shape)\n",
    "    assert(sf[i].features.shape == sf2[i].features.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/akshaykvnit/kd0/d5b4b65c9a3c4ba6b252264557d516d2\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [2010]                   : (0.021085122600197792, 0.8147169947624207)\n",
      "COMET INFO:     sys.gpu.0.free_memory [88]    : (5101846528.0, 5139595264.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [88]: (0.0, 99.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory        : (11721506816.0, 11721506816.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [88]    : (6581911552.0, 6619660288.0)\n",
      "COMET INFO:     sys.gpu.1.free_memory [88]    : (1238106112.0, 6221987840.0)\n",
      "COMET INFO:     sys.gpu.1.gpu_utilization [88]: (0.0, 97.0)\n",
      "COMET INFO:     sys.gpu.1.total_memory        : (6233391104.0, 6233391104.0)\n",
      "COMET INFO:     sys.gpu.1.used_memory [88]    : (11403264.0, 4995284992.0)\n",
      "COMET INFO:     train_loss [100]              : (0.02236961764260311, 0.4373542653090918)\n",
      "COMET INFO:     val_loss [100]                : (0.021828623954206705, 0.28351984173059464)\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (2.0.9) detected. current: 2.0.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/3e8621a0783c4bfcbb67c4b4cf60f648\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1  step =  50  of total steps  201  loss =  1.273905873298645\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.8543833494186401\n",
      "epoch =  1  step =  150  of total steps  201  loss =  1.9610918760299683\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.7102090120315552\n",
      "epoch :  1  /  100  | TL :  1.460261870082931  | VL :  1.1243370547890663\n",
      "saving model\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.8931823968887329\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.8252297043800354\n",
      "epoch =  2  step =  150  of total steps  201  loss =  1.7569987773895264\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.9887546300888062\n",
      "epoch :  2  /  100  | TL :  1.0779924440146678  | VL :  0.9869095385074615\n",
      "saving model\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.611712634563446\n",
      "epoch =  3  step =  100  of total steps  201  loss =  1.44060218334198\n",
      "epoch =  3  step =  150  of total steps  201  loss =  1.8025075197219849\n",
      "epoch =  3  step =  200  of total steps  201  loss =  1.0458030700683594\n",
      "epoch :  3  /  100  | TL :  0.9432572858843637  | VL :  0.8165855072438717\n",
      "saving model\n",
      "epoch =  4  step =  50  of total steps  201  loss =  1.0709335803985596\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.7606213688850403\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.7903650999069214\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.5269346833229065\n",
      "epoch :  4  /  100  | TL :  0.9009965468401933  | VL :  0.7783596739172935\n",
      "saving model\n",
      "epoch =  5  step =  50  of total steps  201  loss =  1.3039181232452393\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.4280940294265747\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.5318881869316101\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.5257815718650818\n",
      "epoch :  5  /  100  | TL :  0.8094864604781516  | VL :  0.7364561520516872\n",
      "saving model\n",
      "epoch =  6  step =  50  of total steps  201  loss =  2.100296974182129\n",
      "epoch =  6  step =  100  of total steps  201  loss =  1.6141510009765625\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.4239449203014374\n",
      "epoch =  6  step =  200  of total steps  201  loss =  1.0384490489959717\n",
      "epoch :  6  /  100  | TL :  0.771615018892051  | VL :  0.670272309333086\n",
      "saving model\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.4033263325691223\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.8785440921783447\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.5773699283599854\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.40088126063346863\n",
      "epoch :  7  /  100  | TL :  0.6997126416483922  | VL :  0.6146602109074593\n",
      "saving model\n",
      "epoch =  8  step =  50  of total steps  201  loss =  1.8779785633087158\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.3874003291130066\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.848390519618988\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.6485363841056824\n",
      "epoch :  8  /  100  | TL :  0.6816270923733119  | VL :  0.5936354491859674\n",
      "saving model\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.4221392273902893\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.4105560779571533\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.43161725997924805\n",
      "epoch =  9  step =  200  of total steps  201  loss =  1.2705533504486084\n",
      "epoch :  9  /  100  | TL :  0.6525398780457417  | VL :  0.581926854327321\n",
      "saving model\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.29427480697631836\n",
      "epoch =  10  step =  100  of total steps  201  loss =  1.2952792644500732\n",
      "epoch =  10  step =  150  of total steps  201  loss =  1.0900686979293823\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.48826128244400024\n",
      "epoch :  10  /  100  | TL :  0.6305366624054031  | VL :  0.4934896435588598\n",
      "saving model\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.4500312805175781\n",
      "epoch =  11  step =  100  of total steps  201  loss =  1.1221455335617065\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.498919814825058\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.36322763562202454\n",
      "epoch :  11  /  100  | TL :  0.5985934551972062  | VL :  0.510127192363143\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.2616598308086395\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.38797664642333984\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.32171374559402466\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.5240250825881958\n",
      "epoch :  12  /  100  | TL :  0.592586743520267  | VL :  0.4523712061345577\n",
      "saving model\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.8206365704536438\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.32853978872299194\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.5414849519729614\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.3485201895236969\n",
      "epoch :  13  /  100  | TL :  0.533050712364823  | VL :  0.43940611369907856\n",
      "saving model\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.4187827408313751\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.25954657793045044\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.7732774019241333\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.296394407749176\n",
      "epoch :  14  /  100  | TL :  0.5444998308942093  | VL :  0.47644705325365067\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.3806043267250061\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.8870072364807129\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.5912668704986572\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.22932717204093933\n",
      "epoch :  15  /  100  | TL :  0.5298867929930711  | VL :  0.395844304934144\n",
      "saving model\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.6014696955680847\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.47407788038253784\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.6715388298034668\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.33889415860176086\n",
      "epoch :  16  /  100  | TL :  0.5095088105296615  | VL :  0.3846784410998225\n",
      "saving model\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.3238319456577301\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.4960181415081024\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.936896800994873\n",
      "epoch =  17  step =  200  of total steps  201  loss =  1.4998393058776855\n",
      "epoch :  17  /  100  | TL :  0.4842964243977817  | VL :  0.3848405834287405\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.26415762305259705\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.7549511790275574\n",
      "epoch =  18  step =  150  of total steps  201  loss =  1.0496705770492554\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.19281470775604248\n",
      "epoch :  18  /  100  | TL :  0.5028064101934433  | VL :  0.3874549148604274\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.5426076054573059\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.21028634905815125\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.211297869682312\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.28559866547584534\n",
      "epoch :  19  /  100  | TL :  0.4667181597271962  | VL :  0.37194468080997467\n",
      "saving model\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.2618909180164337\n",
      "epoch =  20  step =  100  of total steps  201  loss =  1.04337477684021\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.4334667921066284\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.23913106322288513\n",
      "epoch :  20  /  100  | TL :  0.4475876155925627  | VL :  0.3763844007626176\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.649196445941925\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.21901315450668335\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.1975056529045105\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.22887136042118073\n",
      "epoch :  21  /  100  | TL :  0.45253225723605844  | VL :  0.3817788576707244\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.799842357635498\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.21176092326641083\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.3904959261417389\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.31786319613456726\n",
      "epoch :  22  /  100  | TL :  0.44278781538579004  | VL :  0.35597958508878946\n",
      "saving model\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.1695030927658081\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.2293725609779358\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.9631608128547668\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.3781968653202057\n",
      "epoch :  23  /  100  | TL :  0.4426744988012077  | VL :  0.3235045848414302\n",
      "saving model\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.4738873839378357\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.1803208291530609\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.20626601576805115\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.26556894183158875\n",
      "epoch :  24  /  100  | TL :  0.44196407630372403  | VL :  0.3142313966527581\n",
      "saving model\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.5385630130767822\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.1865459680557251\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.1575833559036255\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.6709457039833069\n",
      "epoch :  25  /  100  | TL :  0.42853724622904366  | VL :  0.3586214706301689\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.6056458353996277\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.7850422859191895\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.3951433598995209\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.21519352495670319\n",
      "epoch :  26  /  100  | TL :  0.3853900826244212  | VL :  0.287258030846715\n",
      "saving model\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.2911556363105774\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.20873206853866577\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.23970343172550201\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.49221816658973694\n",
      "epoch :  27  /  100  | TL :  0.4132715846175578  | VL :  0.33214706368744373\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.20796054601669312\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.21620796620845795\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.25089535117149353\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.2133672833442688\n",
      "epoch :  28  /  100  | TL :  0.3971877102531604  | VL :  0.31795025151222944\n",
      "epoch =  29  step =  50  of total steps  201  loss =  1.0884391069412231\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.30743247270584106\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.34365227818489075\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.1907598078250885\n",
      "epoch :  29  /  100  | TL :  0.3738938008078295  | VL :  0.3302084803581238\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.17487990856170654\n",
      "epoch =  30  step =  100  of total steps  201  loss =  1.555212140083313\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.753603994846344\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.31519201397895813\n",
      "epoch :  30  /  100  | TL :  0.4023578287801932  | VL :  0.3163811443373561\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.19191540777683258\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.1815880388021469\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.21773606538772583\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.22403249144554138\n",
      "epoch :  31  /  100  | TL :  0.35057140011989063  | VL :  0.31697223614901304\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.18190878629684448\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.9963732361793518\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.22569461166858673\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.1507251113653183\n",
      "epoch :  32  /  100  | TL :  0.3819693702815184  | VL :  0.3115187706425786\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.41653192043304443\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.3640918731689453\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.36486437916755676\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.3077368438243866\n",
      "epoch :  33  /  100  | TL :  0.3652429469485781  | VL :  0.27128284331411123\n",
      "saving model\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.2535001039505005\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.21784217655658722\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.15746726095676422\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.16168375313282013\n",
      "epoch :  34  /  100  | TL :  0.3669183174176003  | VL :  0.26707000518217683\n",
      "saving model\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.16005435585975647\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.30238714814186096\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.34010088443756104\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.26057541370391846\n",
      "epoch :  35  /  100  | TL :  0.3794169814432438  | VL :  0.3053861130028963\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.5515002608299255\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.2103748619556427\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.2172526717185974\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.22783571481704712\n",
      "epoch :  36  /  100  | TL :  0.3879079506987363  | VL :  0.3033919990994036\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.15204878151416779\n",
      "epoch =  37  step =  100  of total steps  201  loss =  1.344142198562622\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.20968589186668396\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.1653449684381485\n",
      "epoch :  37  /  100  | TL :  0.373526034811836  | VL :  0.3163769990205765\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.18273897469043732\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.36882665753364563\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.21030262112617493\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.24062477052211761\n",
      "epoch :  38  /  100  | TL :  0.37091550053055605  | VL :  0.3223495576530695\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.14048424363136292\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.18839754164218903\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.29276758432388306\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.169806107878685\n",
      "epoch :  39  /  100  | TL :  0.3640294079460315  | VL :  0.2588020791299641\n",
      "saving model\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.2429904043674469\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.18454305827617645\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.42479172348976135\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.2779789865016937\n",
      "epoch :  40  /  100  | TL :  0.38104632133571664  | VL :  0.25579153513535857\n",
      "saving model\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.20078055560588837\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.22270835936069489\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.4041571319103241\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.151527538895607\n",
      "epoch :  41  /  100  | TL :  0.3666666124145783  | VL :  0.2604715130291879\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.17508789896965027\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.17920832335948944\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.17568525671958923\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.14330998063087463\n",
      "epoch :  42  /  100  | TL :  0.35086695182679306  | VL :  0.26222449308261275\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.8342626094818115\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.14765024185180664\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.3312012255191803\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.27640843391418457\n",
      "epoch :  43  /  100  | TL :  0.3396143190451522  | VL :  0.26432002941146493\n",
      "epoch =  44  step =  50  of total steps  201  loss =  1.1921671628952026\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.23574887216091156\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.35286641120910645\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.27119341492652893\n",
      "epoch :  44  /  100  | TL :  0.35188591843517264  | VL :  0.23522198107093573\n",
      "saving model\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.5542600154876709\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.3364824056625366\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.33688780665397644\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.22205212712287903\n",
      "epoch :  45  /  100  | TL :  0.3664550842782158  | VL :  0.2579932124353945\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.4324228763580322\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.203750878572464\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.4968254566192627\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.15869607031345367\n",
      "epoch :  46  /  100  | TL :  0.33185048890647606  | VL :  0.25869927695021033\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.554028332233429\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.24977563321590424\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.2913305163383484\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.19290485978126526\n",
      "epoch :  47  /  100  | TL :  0.35409063189776974  | VL :  0.2406193264760077\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.23131245374679565\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.2511237859725952\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.2961461842060089\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.1683422327041626\n",
      "epoch :  48  /  100  | TL :  0.3457893460840728  | VL :  0.2692787856794894\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.3104609549045563\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.275640606880188\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.15839655697345734\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.84759521484375\n",
      "epoch :  49  /  100  | TL :  0.3312603838334036  | VL :  0.27224679477512836\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.293634831905365\n",
      "epoch =  50  step =  100  of total steps  201  loss =  1.1332165002822876\n",
      "epoch =  50  step =  150  of total steps  201  loss =  0.1886720061302185\n",
      "epoch =  50  step =  200  of total steps  201  loss =  0.21695207059383392\n",
      "epoch :  50  /  100  | TL :  0.3474591059322974  | VL :  0.2342588333413005\n",
      "saving model\n",
      "epoch =  51  step =  50  of total steps  201  loss =  0.14630572497844696\n",
      "epoch =  51  step =  100  of total steps  201  loss =  0.14346590638160706\n",
      "epoch =  51  step =  150  of total steps  201  loss =  0.15440478920936584\n",
      "epoch =  51  step =  200  of total steps  201  loss =  0.5590506196022034\n",
      "epoch :  51  /  100  | TL :  0.35116709442577554  | VL :  0.240123909432441\n",
      "epoch =  52  step =  50  of total steps  201  loss =  0.20378796756267548\n",
      "epoch =  52  step =  100  of total steps  201  loss =  0.2592536211013794\n",
      "epoch =  52  step =  150  of total steps  201  loss =  0.8472344279289246\n",
      "epoch =  52  step =  200  of total steps  201  loss =  0.42338642477989197\n",
      "epoch :  52  /  100  | TL :  0.34411800812132914  | VL :  0.2526205498725176\n",
      "epoch =  53  step =  50  of total steps  201  loss =  0.19913645088672638\n",
      "epoch =  53  step =  100  of total steps  201  loss =  0.14322802424430847\n",
      "epoch =  53  step =  150  of total steps  201  loss =  0.7338689565658569\n",
      "epoch =  53  step =  200  of total steps  201  loss =  0.3886088728904724\n",
      "epoch :  53  /  100  | TL :  0.32294572181814346  | VL :  0.26702141808345914\n",
      "epoch =  54  step =  50  of total steps  201  loss =  0.22393646836280823\n",
      "epoch =  54  step =  100  of total steps  201  loss =  0.22544421255588531\n",
      "epoch =  54  step =  150  of total steps  201  loss =  0.19655746221542358\n",
      "epoch =  54  step =  200  of total steps  201  loss =  1.6369030475616455\n",
      "epoch :  54  /  100  | TL :  0.33500533828984447  | VL :  0.23438597563654184\n",
      "epoch =  55  step =  50  of total steps  201  loss =  0.20139293372631073\n",
      "epoch =  55  step =  100  of total steps  201  loss =  0.29811328649520874\n",
      "epoch =  55  step =  150  of total steps  201  loss =  0.32225725054740906\n",
      "epoch =  55  step =  200  of total steps  201  loss =  0.20323877036571503\n",
      "epoch :  55  /  100  | TL :  0.33816029817162463  | VL :  0.23383762780576944\n",
      "saving model\n",
      "epoch =  56  step =  50  of total steps  201  loss =  0.49582305550575256\n",
      "epoch =  56  step =  100  of total steps  201  loss =  0.17866238951683044\n",
      "epoch =  56  step =  150  of total steps  201  loss =  0.24453400075435638\n",
      "epoch =  56  step =  200  of total steps  201  loss =  0.16512952744960785\n",
      "epoch :  56  /  100  | TL :  0.32071802038606717  | VL :  0.2536951843649149\n",
      "epoch =  57  step =  50  of total steps  201  loss =  0.2644751965999603\n",
      "epoch =  57  step =  100  of total steps  201  loss =  0.14611934125423431\n",
      "epoch =  57  step =  150  of total steps  201  loss =  0.20257237553596497\n",
      "epoch =  57  step =  200  of total steps  201  loss =  0.27280014753341675\n",
      "epoch :  57  /  100  | TL :  0.33096419804873156  | VL :  0.24554121494293213\n",
      "epoch =  58  step =  50  of total steps  201  loss =  0.1479516327381134\n",
      "epoch =  58  step =  100  of total steps  201  loss =  0.20351678133010864\n",
      "epoch =  58  step =  150  of total steps  201  loss =  1.1182478666305542\n",
      "epoch =  58  step =  200  of total steps  201  loss =  0.35953372716903687\n",
      "epoch :  58  /  100  | TL :  0.3272969821347526  | VL :  0.24313768092542887\n",
      "epoch =  59  step =  50  of total steps  201  loss =  0.1933315247297287\n",
      "epoch =  59  step =  100  of total steps  201  loss =  0.33537569642066956\n",
      "epoch =  59  step =  150  of total steps  201  loss =  0.18249593675136566\n",
      "epoch =  59  step =  200  of total steps  201  loss =  0.1492420732975006\n",
      "epoch :  59  /  100  | TL :  0.31640807339059773  | VL :  0.23367381747812033\n",
      "saving model\n",
      "epoch =  60  step =  50  of total steps  201  loss =  0.1603693664073944\n",
      "epoch =  60  step =  100  of total steps  201  loss =  0.12470682710409164\n",
      "epoch =  60  step =  150  of total steps  201  loss =  0.16979719698429108\n",
      "epoch =  60  step =  200  of total steps  201  loss =  0.19857876002788544\n",
      "epoch :  60  /  100  | TL :  0.32818217453227116  | VL :  0.25362844206392765\n",
      "epoch =  61  step =  50  of total steps  201  loss =  0.16933679580688477\n",
      "epoch =  61  step =  100  of total steps  201  loss =  0.24177372455596924\n",
      "epoch =  61  step =  150  of total steps  201  loss =  0.24173814058303833\n",
      "epoch =  61  step =  200  of total steps  201  loss =  0.2876597046852112\n",
      "epoch :  61  /  100  | TL :  0.32453000630757106  | VL :  0.22555344039574265\n",
      "saving model\n",
      "epoch =  62  step =  50  of total steps  201  loss =  0.353016197681427\n",
      "epoch =  62  step =  100  of total steps  201  loss =  0.19695277512073517\n",
      "epoch =  62  step =  150  of total steps  201  loss =  0.5441747903823853\n",
      "epoch =  62  step =  200  of total steps  201  loss =  0.6415784955024719\n",
      "epoch :  62  /  100  | TL :  0.3163578874287914  | VL :  0.2551672710105777\n",
      "epoch =  63  step =  50  of total steps  201  loss =  0.5376628041267395\n",
      "epoch =  63  step =  100  of total steps  201  loss =  0.1886863261461258\n",
      "epoch =  63  step =  150  of total steps  201  loss =  0.3289477229118347\n",
      "epoch =  63  step =  200  of total steps  201  loss =  0.1887843757867813\n",
      "epoch :  63  /  100  | TL :  0.30689834829290114  | VL :  0.23421756038442254\n",
      "epoch =  64  step =  50  of total steps  201  loss =  0.25847744941711426\n",
      "epoch =  64  step =  100  of total steps  201  loss =  0.5394814610481262\n",
      "epoch =  64  step =  150  of total steps  201  loss =  0.18940508365631104\n",
      "epoch =  64  step =  200  of total steps  201  loss =  0.5608015060424805\n",
      "epoch :  64  /  100  | TL :  0.3285013893572845  | VL :  0.2386588235385716\n",
      "epoch =  65  step =  50  of total steps  201  loss =  0.24664613604545593\n",
      "epoch =  65  step =  100  of total steps  201  loss =  0.501479983329773\n",
      "epoch =  65  step =  150  of total steps  201  loss =  0.3168862462043762\n",
      "epoch =  65  step =  200  of total steps  201  loss =  0.1703994870185852\n",
      "epoch :  65  /  100  | TL :  0.3421732204322198  | VL :  0.2300665332004428\n",
      "epoch =  66  step =  50  of total steps  201  loss =  0.24505344033241272\n",
      "epoch =  66  step =  100  of total steps  201  loss =  0.6000304222106934\n",
      "epoch =  66  step =  150  of total steps  201  loss =  0.3396349549293518\n",
      "epoch =  66  step =  200  of total steps  201  loss =  0.6962200999259949\n",
      "epoch :  66  /  100  | TL :  0.3148815287508775  | VL :  0.23912197444587946\n",
      "epoch =  67  step =  50  of total steps  201  loss =  0.33691367506980896\n",
      "epoch =  67  step =  100  of total steps  201  loss =  0.39801687002182007\n",
      "epoch =  67  step =  150  of total steps  201  loss =  0.27425822615623474\n",
      "epoch =  67  step =  200  of total steps  201  loss =  0.155582457780838\n",
      "epoch :  67  /  100  | TL :  0.32334450472943227  | VL :  0.234548500739038\n",
      "epoch =  68  step =  50  of total steps  201  loss =  0.190167635679245\n",
      "epoch =  68  step =  100  of total steps  201  loss =  0.18478018045425415\n",
      "epoch =  68  step =  150  of total steps  201  loss =  0.31718048453330994\n",
      "epoch =  68  step =  200  of total steps  201  loss =  0.2865199148654938\n",
      "epoch :  68  /  100  | TL :  0.3270132325923265  | VL :  0.22969383792951703\n",
      "epoch =  69  step =  50  of total steps  201  loss =  0.19023369252681732\n",
      "epoch =  69  step =  100  of total steps  201  loss =  1.0676552057266235\n",
      "epoch =  69  step =  150  of total steps  201  loss =  0.2572728395462036\n",
      "epoch =  69  step =  200  of total steps  201  loss =  0.8964882493019104\n",
      "epoch :  69  /  100  | TL :  0.3312530896408641  | VL :  0.23968875966966152\n",
      "epoch =  70  step =  50  of total steps  201  loss =  0.14645126461982727\n",
      "epoch =  70  step =  100  of total steps  201  loss =  0.18226438760757446\n",
      "epoch =  70  step =  150  of total steps  201  loss =  0.1663394421339035\n",
      "epoch =  70  step =  200  of total steps  201  loss =  0.29995572566986084\n",
      "epoch :  70  /  100  | TL :  0.3274829065176978  | VL :  0.22511491319164634\n",
      "saving model\n",
      "epoch =  71  step =  50  of total steps  201  loss =  0.24306555092334747\n",
      "epoch =  71  step =  100  of total steps  201  loss =  0.17870517075061798\n",
      "epoch =  71  step =  150  of total steps  201  loss =  0.16457267105579376\n",
      "epoch =  71  step =  200  of total steps  201  loss =  0.40110641717910767\n",
      "epoch :  71  /  100  | TL :  0.3263729382584344  | VL :  0.23987749870866537\n",
      "epoch =  72  step =  50  of total steps  201  loss =  0.17320874333381653\n",
      "epoch =  72  step =  100  of total steps  201  loss =  0.20348860323429108\n",
      "epoch =  72  step =  150  of total steps  201  loss =  0.7036870121955872\n",
      "epoch =  72  step =  200  of total steps  201  loss =  0.6554113030433655\n",
      "epoch :  72  /  100  | TL :  0.3393132139868404  | VL :  0.24595042038708925\n",
      "epoch =  73  step =  50  of total steps  201  loss =  0.2464590221643448\n",
      "epoch =  73  step =  100  of total steps  201  loss =  0.2806236147880554\n",
      "epoch =  73  step =  150  of total steps  201  loss =  0.16367192566394806\n",
      "epoch =  73  step =  200  of total steps  201  loss =  0.27815937995910645\n",
      "epoch :  73  /  100  | TL :  0.29724266836002694  | VL :  0.23027447704225779\n",
      "epoch =  74  step =  50  of total steps  201  loss =  0.32494550943374634\n",
      "epoch =  74  step =  100  of total steps  201  loss =  0.14019151031970978\n",
      "epoch =  74  step =  150  of total steps  201  loss =  0.6063637733459473\n",
      "epoch =  74  step =  200  of total steps  201  loss =  0.24009926617145538\n",
      "epoch :  74  /  100  | TL :  0.31878906536606416  | VL :  0.23528428189456463\n",
      "epoch =  75  step =  50  of total steps  201  loss =  0.18385766446590424\n",
      "epoch =  75  step =  100  of total steps  201  loss =  0.18123619258403778\n",
      "epoch =  75  step =  150  of total steps  201  loss =  0.22491376101970673\n",
      "epoch =  75  step =  200  of total steps  201  loss =  0.1795874834060669\n",
      "epoch :  75  /  100  | TL :  0.311023929967216  | VL :  0.22692836076021194\n",
      "epoch =  76  step =  50  of total steps  201  loss =  0.18335911631584167\n",
      "epoch =  76  step =  100  of total steps  201  loss =  0.20551984012126923\n",
      "epoch =  76  step =  150  of total steps  201  loss =  0.5088341236114502\n",
      "epoch =  76  step =  200  of total steps  201  loss =  0.11520571261644363\n",
      "epoch :  76  /  100  | TL :  0.31606154851800766  | VL :  0.2295324793085456\n",
      "epoch =  77  step =  50  of total steps  201  loss =  0.20605862140655518\n",
      "epoch =  77  step =  100  of total steps  201  loss =  0.14058661460876465\n",
      "epoch =  77  step =  150  of total steps  201  loss =  0.2509869337081909\n",
      "epoch =  77  step =  200  of total steps  201  loss =  0.19874514639377594\n",
      "epoch :  77  /  100  | TL :  0.307597608078475  | VL :  0.2383341072127223\n",
      "epoch =  78  step =  50  of total steps  201  loss =  0.21648439764976501\n",
      "epoch =  78  step =  100  of total steps  201  loss =  0.23181460797786713\n",
      "epoch =  78  step =  150  of total steps  201  loss =  0.6550665497779846\n",
      "epoch =  78  step =  200  of total steps  201  loss =  1.092407464981079\n",
      "epoch :  78  /  100  | TL :  0.2972608525583993  | VL :  0.236059439368546\n",
      "epoch =  79  step =  50  of total steps  201  loss =  0.2073356658220291\n",
      "epoch =  79  step =  100  of total steps  201  loss =  0.3114677667617798\n",
      "epoch =  79  step =  150  of total steps  201  loss =  0.6287418007850647\n",
      "epoch =  79  step =  200  of total steps  201  loss =  0.23254071176052094\n",
      "epoch :  79  /  100  | TL :  0.31043979273506656  | VL :  0.2542811478488147\n",
      "epoch =  80  step =  50  of total steps  201  loss =  0.2021595984697342\n",
      "epoch =  80  step =  100  of total steps  201  loss =  0.21034714579582214\n",
      "epoch =  80  step =  150  of total steps  201  loss =  0.2220529019832611\n",
      "epoch =  80  step =  200  of total steps  201  loss =  0.3150079846382141\n",
      "epoch :  80  /  100  | TL :  0.3016640333393913  | VL :  0.2282364722341299\n",
      "epoch =  81  step =  50  of total steps  201  loss =  0.18485814332962036\n",
      "epoch =  81  step =  100  of total steps  201  loss =  0.4080662429332733\n",
      "epoch =  81  step =  150  of total steps  201  loss =  0.20473437011241913\n",
      "epoch =  81  step =  200  of total steps  201  loss =  0.36806347966194153\n",
      "epoch :  81  /  100  | TL :  0.30282718169303674  | VL :  0.2424935195595026\n",
      "epoch =  82  step =  50  of total steps  201  loss =  0.18011264503002167\n",
      "epoch =  82  step =  100  of total steps  201  loss =  0.15462300181388855\n",
      "epoch =  82  step =  150  of total steps  201  loss =  0.19904334843158722\n",
      "epoch =  82  step =  200  of total steps  201  loss =  0.17464320361614227\n",
      "epoch :  82  /  100  | TL :  0.3125668020539023  | VL :  0.228039741050452\n",
      "epoch =  83  step =  50  of total steps  201  loss =  0.42202937602996826\n",
      "epoch =  83  step =  100  of total steps  201  loss =  0.1582627296447754\n",
      "epoch =  83  step =  150  of total steps  201  loss =  0.3075190484523773\n",
      "epoch =  83  step =  200  of total steps  201  loss =  0.21758368611335754\n",
      "epoch :  83  /  100  | TL :  0.31853255344119236  | VL :  0.229285370092839\n",
      "epoch =  84  step =  50  of total steps  201  loss =  0.2112545371055603\n",
      "epoch =  84  step =  100  of total steps  201  loss =  0.5991966128349304\n",
      "epoch =  84  step =  150  of total steps  201  loss =  0.5200459361076355\n",
      "epoch =  84  step =  200  of total steps  201  loss =  0.18191632628440857\n",
      "epoch :  84  /  100  | TL :  0.29327977515423476  | VL :  0.24432192044332623\n",
      "epoch =  85  step =  50  of total steps  201  loss =  0.284702867269516\n",
      "epoch =  85  step =  100  of total steps  201  loss =  0.1189543604850769\n",
      "epoch =  85  step =  150  of total steps  201  loss =  0.2700139284133911\n",
      "epoch =  85  step =  200  of total steps  201  loss =  0.29826539754867554\n",
      "epoch :  85  /  100  | TL :  0.30793841498259883  | VL :  0.2181745539419353\n",
      "saving model\n",
      "epoch =  86  step =  50  of total steps  201  loss =  0.6127117872238159\n",
      "epoch =  86  step =  100  of total steps  201  loss =  0.278363972902298\n",
      "epoch =  86  step =  150  of total steps  201  loss =  0.1527496576309204\n",
      "epoch =  86  step =  200  of total steps  201  loss =  0.13249632716178894\n",
      "epoch :  86  /  100  | TL :  0.29573331980859463  | VL :  0.21850253734737635\n",
      "epoch =  87  step =  50  of total steps  201  loss =  0.2068813592195511\n",
      "epoch =  87  step =  100  of total steps  201  loss =  0.20309793949127197\n",
      "epoch =  87  step =  150  of total steps  201  loss =  0.25325843691825867\n",
      "epoch =  87  step =  200  of total steps  201  loss =  0.21546994149684906\n",
      "epoch :  87  /  100  | TL :  0.2978406936968144  | VL :  0.22527631931006908\n",
      "epoch =  88  step =  50  of total steps  201  loss =  0.3497507870197296\n",
      "epoch =  88  step =  100  of total steps  201  loss =  0.20697835087776184\n",
      "epoch =  88  step =  150  of total steps  201  loss =  0.23812811076641083\n",
      "epoch =  88  step =  200  of total steps  201  loss =  0.2316213697195053\n",
      "epoch :  88  /  100  | TL :  0.2990996762739485  | VL :  0.2264278340153396\n",
      "epoch =  89  step =  50  of total steps  201  loss =  0.19334933161735535\n",
      "epoch =  89  step =  100  of total steps  201  loss =  0.2925891876220703\n",
      "epoch =  89  step =  150  of total steps  201  loss =  0.36038970947265625\n",
      "epoch =  89  step =  200  of total steps  201  loss =  0.13217340409755707\n",
      "epoch :  89  /  100  | TL :  0.3102004316019182  | VL :  0.21996364183723927\n",
      "epoch =  90  step =  50  of total steps  201  loss =  1.362919569015503\n",
      "epoch =  90  step =  100  of total steps  201  loss =  0.29517319798469543\n",
      "epoch =  90  step =  150  of total steps  201  loss =  0.33574551343917847\n",
      "epoch =  90  step =  200  of total steps  201  loss =  0.29711467027664185\n",
      "epoch :  90  /  100  | TL :  0.3223458532966785  | VL :  0.23646545642986894\n",
      "epoch =  91  step =  50  of total steps  201  loss =  0.20453856885433197\n",
      "epoch =  91  step =  100  of total steps  201  loss =  0.23394078016281128\n",
      "epoch =  91  step =  150  of total steps  201  loss =  0.36868059635162354\n",
      "epoch =  91  step =  200  of total steps  201  loss =  0.2632615268230438\n",
      "epoch :  91  /  100  | TL :  0.317760366334844  | VL :  0.216023622546345\n",
      "saving model\n",
      "epoch =  92  step =  50  of total steps  201  loss =  0.18824920058250427\n",
      "epoch =  92  step =  100  of total steps  201  loss =  0.2129351943731308\n",
      "epoch =  92  step =  150  of total steps  201  loss =  0.3171725869178772\n",
      "epoch =  92  step =  200  of total steps  201  loss =  0.3439508378505707\n",
      "epoch :  92  /  100  | TL :  0.3028712416836871  | VL :  0.23336317762732506\n",
      "epoch =  93  step =  50  of total steps  201  loss =  0.6303055286407471\n",
      "epoch =  93  step =  100  of total steps  201  loss =  1.8286683559417725\n",
      "epoch =  93  step =  150  of total steps  201  loss =  0.22214655578136444\n",
      "epoch =  93  step =  200  of total steps  201  loss =  0.22748517990112305\n",
      "epoch :  93  /  100  | TL :  0.29612592237060936  | VL :  0.21799467084929347\n",
      "epoch =  94  step =  50  of total steps  201  loss =  0.37452220916748047\n",
      "epoch =  94  step =  100  of total steps  201  loss =  0.35241660475730896\n",
      "epoch =  94  step =  150  of total steps  201  loss =  0.29772064089775085\n",
      "epoch =  94  step =  200  of total steps  201  loss =  0.22948668897151947\n",
      "epoch :  94  /  100  | TL :  0.30074431406176505  | VL :  0.22906579915434122\n",
      "epoch =  95  step =  50  of total steps  201  loss =  0.15207456052303314\n",
      "epoch =  95  step =  100  of total steps  201  loss =  0.14256656169891357\n",
      "epoch =  95  step =  150  of total steps  201  loss =  0.25758036971092224\n",
      "epoch =  95  step =  200  of total steps  201  loss =  0.5116652250289917\n",
      "epoch :  95  /  100  | TL :  0.30232614842220323  | VL :  0.22160859173163772\n",
      "epoch =  96  step =  50  of total steps  201  loss =  0.171927347779274\n",
      "epoch =  96  step =  100  of total steps  201  loss =  0.35937264561653137\n",
      "epoch =  96  step =  150  of total steps  201  loss =  0.1204661875963211\n",
      "epoch =  96  step =  200  of total steps  201  loss =  0.1957198530435562\n",
      "epoch :  96  /  100  | TL :  0.2963289697816716  | VL :  0.22126971185207367\n",
      "epoch =  97  step =  50  of total steps  201  loss =  1.1092381477355957\n",
      "epoch =  97  step =  100  of total steps  201  loss =  0.2408420741558075\n",
      "epoch =  97  step =  150  of total steps  201  loss =  0.24326711893081665\n",
      "epoch =  97  step =  200  of total steps  201  loss =  0.19018729031085968\n",
      "epoch :  97  /  100  | TL :  0.29424630657802175  | VL :  0.2257811389863491\n",
      "epoch =  98  step =  50  of total steps  201  loss =  0.9095640182495117\n",
      "epoch =  98  step =  100  of total steps  201  loss =  0.23906771838665009\n",
      "epoch =  98  step =  150  of total steps  201  loss =  0.4966137111186981\n",
      "epoch =  98  step =  200  of total steps  201  loss =  0.18544450402259827\n",
      "epoch :  98  /  100  | TL :  0.30748737481103017  | VL :  0.23608849570155144\n",
      "epoch =  99  step =  50  of total steps  201  loss =  0.39526817202568054\n",
      "epoch =  99  step =  100  of total steps  201  loss =  0.30132460594177246\n",
      "epoch =  99  step =  150  of total steps  201  loss =  0.26162636280059814\n",
      "epoch =  99  step =  200  of total steps  201  loss =  0.22055239975452423\n",
      "epoch :  99  /  100  | TL :  0.2917176301206522  | VL :  0.21846559969708323\n",
      "epoch =  100  step =  50  of total steps  201  loss =  0.5993046760559082\n",
      "epoch =  100  step =  100  of total steps  201  loss =  0.14925126731395721\n",
      "epoch =  100  step =  150  of total steps  201  loss =  0.44312769174575806\n",
      "epoch =  100  step =  200  of total steps  201  loss =  0.4762505292892456\n",
      "epoch :  100  /  100  | TL :  0.2981526777889598  | VL :  0.22848335187882185\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "if hyper_params['stage'] == 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "else : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage'] + 1) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "        y_pred2 = mdl(images)\n",
    "\n",
    "        loss = F.mse_loss(sf2[hyper_params[\"stage\"]].features, sf[hyper_params[\"stage\"]].features)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch + 1, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            y_pred2 = mdl(images)\n",
    "            loss = F.mse_loss(sf[hyper_params[\"stage\"]].features, sf2[hyper_params[\"stage\"]].features)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "\n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = Learner(data, net, metrics = accuracy)\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model0.pt', map_location = 'cpu'))\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(hyper_params[\"num_epochs\"]), train_loss_list, label = 'train_loss') \n",
    "plt.plot(range(hyper_params[\"num_epochs\"]), val_loss_list, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/stage5/train_loss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier only after stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "7.weight torch.Size([128, 512])\n",
      "True\n",
      "7.bias torch.Size([128])\n",
      "True\n",
      "8.weight torch.Size([10, 128])\n",
      "True\n",
      "8.bias torch.Size([10])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    \"stage\": 5,\n",
    "    \"repeated\": 3,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 256, 128),\n",
    "    nn.Linear(128, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/small_stage4/model1.pt', map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == '7' or name[0] == '8':\n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = torch.autograd.Variable(images).float()\n",
    "        labels = torch.autograd.Variable(labels).float()\n",
    "        \n",
    "        if torch.cuda.is_available() : \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = Net.forward(images)\n",
    "        outputs = F.log_softmax(outputs, dim = 1)\n",
    "\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        pred_ind = pred_ind.data.cpu().numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = labels - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  50  of total steps  201  loss =  1.994004726409912\n",
      "epoch =  0  step =  100  of total steps  201  loss =  1.7128355503082275\n",
      "epoch =  0  step =  150  of total steps  201  loss =  1.5702192783355713\n",
      "epoch =  0  step =  200  of total steps  201  loss =  1.441495656967163\n",
      "epoch :  1  /  100  | TL :  1.766414149483638  | VL :  1.2715763002634048  | VA :  79.80000000000001\n",
      "saving model\n",
      "epoch =  1  step =  50  of total steps  201  loss =  1.1863903999328613\n",
      "epoch =  1  step =  100  of total steps  201  loss =  1.020689606666565\n",
      "epoch =  1  step =  150  of total steps  201  loss =  0.8921720385551453\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.9054326415061951\n",
      "epoch :  2  /  100  | TL :  1.1065876095449154  | VL :  0.8427877947688103  | VA :  80.80000000000001\n",
      "saving model\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.7955490350723267\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.8560861349105835\n",
      "epoch =  2  step =  150  of total steps  201  loss =  0.7782891988754272\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.795216977596283\n",
      "epoch :  3  /  100  | TL :  0.8275399549090448  | VL :  0.6551304683089256  | VA :  84.0\n",
      "saving model\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.6766407489776611\n",
      "epoch =  3  step =  100  of total steps  201  loss =  0.6921248435974121\n",
      "epoch =  3  step =  150  of total steps  201  loss =  0.7092602849006653\n",
      "epoch =  3  step =  200  of total steps  201  loss =  0.6148146390914917\n",
      "epoch :  4  /  100  | TL :  0.6932472654836095  | VL :  0.554982416331768  | VA :  86.2\n",
      "saving model\n",
      "epoch =  4  step =  50  of total steps  201  loss =  0.799087405204773\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.4513329863548279\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.6104145646095276\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.6172663569450378\n",
      "epoch :  5  /  100  | TL :  0.6223742684618158  | VL :  0.5094095282256603  | VA :  85.6\n",
      "epoch =  5  step =  50  of total steps  201  loss =  0.4854523539543152\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.47129708528518677\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.5657652020454407\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.5219020247459412\n",
      "epoch :  6  /  100  | TL :  0.5786233603657774  | VL :  0.47155270352959633  | VA :  86.6\n",
      "saving model\n",
      "epoch =  6  step =  50  of total steps  201  loss =  0.4518781900405884\n",
      "epoch =  6  step =  100  of total steps  201  loss =  0.6839011311531067\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.6897282600402832\n",
      "epoch =  6  step =  200  of total steps  201  loss =  0.33319929242134094\n",
      "epoch :  7  /  100  | TL :  0.5437812145373121  | VL :  0.43779997900128365  | VA :  87.2\n",
      "saving model\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.42853862047195435\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.287984162569046\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.602865993976593\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.6048055291175842\n",
      "epoch :  8  /  100  | TL :  0.5158450179432162  | VL :  0.41957278922200203  | VA :  87.6\n",
      "saving model\n",
      "epoch =  8  step =  50  of total steps  201  loss =  0.6603819131851196\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.44354820251464844\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.5615027546882629\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.5404024720191956\n",
      "epoch :  9  /  100  | TL :  0.5037910800371597  | VL :  0.40234364569187164  | VA :  88.8\n",
      "saving model\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.5021637082099915\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.41521477699279785\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.3376869559288025\n",
      "epoch =  9  step =  200  of total steps  201  loss =  0.39400750398635864\n",
      "epoch :  10  /  100  | TL :  0.488354152559641  | VL :  0.3942212648689747  | VA :  88.0\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.482181191444397\n",
      "epoch =  10  step =  100  of total steps  201  loss =  0.40974655747413635\n",
      "epoch =  10  step =  150  of total steps  201  loss =  0.3488949239253998\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.33848878741264343\n",
      "epoch :  11  /  100  | TL :  0.46966915507221696  | VL :  0.3876460883766413  | VA :  88.2\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.5475380420684814\n",
      "epoch =  11  step =  100  of total steps  201  loss =  0.3502306640148163\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.2912580966949463\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.4559275507926941\n",
      "epoch :  12  /  100  | TL :  0.4583106745979679  | VL :  0.3784247413277626  | VA :  89.4\n",
      "saving model\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.26512813568115234\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.31638863682746887\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.477070152759552\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.6351313591003418\n",
      "epoch :  13  /  100  | TL :  0.464824587254975  | VL :  0.37284696474671364  | VA :  88.8\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.5754304528236389\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.45081618428230286\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.682049572467804\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.3173021972179413\n",
      "epoch :  14  /  100  | TL :  0.4490220823246448  | VL :  0.36781821213662624  | VA :  88.4\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.4867028594017029\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.4742566645145416\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.46922749280929565\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.5150416493415833\n",
      "epoch :  15  /  100  | TL :  0.44583092444571687  | VL :  0.3603193685412407  | VA :  89.0\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.47783970832824707\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.3867720067501068\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.46306514739990234\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.4329196810722351\n",
      "epoch :  16  /  100  | TL :  0.430012289638543  | VL :  0.3451768923550844  | VA :  89.2\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.32585665583610535\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.3848251402378082\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.34718096256256104\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.5167760848999023\n",
      "epoch :  17  /  100  | TL :  0.43277334091971764  | VL :  0.35582792572677135  | VA :  89.2\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.37425094842910767\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.4421112537384033\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.23526427149772644\n",
      "epoch =  17  step =  200  of total steps  201  loss =  0.4158203601837158\n",
      "epoch :  18  /  100  | TL :  0.4307767234334898  | VL :  0.3535114135593176  | VA :  89.0\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.6175928115844727\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.44718560576438904\n",
      "epoch =  18  step =  150  of total steps  201  loss =  0.2906697988510132\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.47170764207839966\n",
      "epoch :  19  /  100  | TL :  0.4185573349692928  | VL :  0.35211485996842384  | VA :  89.2\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.47338634729385376\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.2227025330066681\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.4420279860496521\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.6209757924079895\n",
      "epoch :  20  /  100  | TL :  0.41768633259173055  | VL :  0.3419429827481508  | VA :  89.4\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.5090665817260742\n",
      "epoch =  20  step =  100  of total steps  201  loss =  0.5204371213912964\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.30095821619033813\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.223828986287117\n",
      "epoch :  21  /  100  | TL :  0.4154900911435559  | VL :  0.33175906352698803  | VA :  90.0\n",
      "saving model\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.3489934504032135\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.4903200566768646\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.43155142664909363\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.44216084480285645\n",
      "epoch :  22  /  100  | TL :  0.4112675126957063  | VL :  0.34113724157214165  | VA :  90.0\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.5479698181152344\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.2646409571170807\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.49842387437820435\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.3953929543495178\n",
      "epoch :  23  /  100  | TL :  0.41070357224537957  | VL :  0.3340036664158106  | VA :  89.60000000000001\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.47319525480270386\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.43122637271881104\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.3962266445159912\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.3884581923484802\n",
      "epoch :  24  /  100  | TL :  0.41265810203196396  | VL :  0.3251472692936659  | VA :  90.0\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.27840456366539\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.6070099472999573\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.5317206978797913\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.4507809281349182\n",
      "epoch :  25  /  100  | TL :  0.39892090063783064  | VL :  0.33318514935672283  | VA :  89.4\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.40301117300987244\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.4625352621078491\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.3702893853187561\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.34256717562675476\n",
      "epoch :  26  /  100  | TL :  0.398239290758745  | VL :  0.33719715289771557  | VA :  89.2\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.45192036032676697\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.41254177689552307\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.48349761962890625\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.29187124967575073\n",
      "epoch :  27  /  100  | TL :  0.39897208101120757  | VL :  0.3289555162191391  | VA :  89.60000000000001\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.5894126892089844\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.2510354816913605\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.3642561733722687\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.26880931854248047\n",
      "epoch :  28  /  100  | TL :  0.39420000993790316  | VL :  0.3260337468236685  | VA :  89.8\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.2360064685344696\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.35376256704330444\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.44217413663864136\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.4550430178642273\n",
      "epoch :  29  /  100  | TL :  0.4014689654704943  | VL :  0.33281487226486206  | VA :  89.8\n",
      "epoch =  29  step =  50  of total steps  201  loss =  0.34186962246894836\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.24677690863609314\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.41636183857917786\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.38680604100227356\n",
      "epoch :  30  /  100  | TL :  0.3921396313466836  | VL :  0.3334405645728111  | VA :  89.8\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.27440783381462097\n",
      "epoch =  30  step =  100  of total steps  201  loss =  0.43923333287239075\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.29991447925567627\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.4169624447822571\n",
      "epoch :  31  /  100  | TL :  0.38805087021927337  | VL :  0.32927232794463634  | VA :  90.0\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.4865821301937103\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.3058277666568756\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.36759182810783386\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.49410027265548706\n",
      "epoch :  32  /  100  | TL :  0.38782277599496034  | VL :  0.31956641003489494  | VA :  90.2\n",
      "saving model\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.3703558146953583\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.3834691643714905\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.3524801731109619\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.5805705785751343\n",
      "epoch :  33  /  100  | TL :  0.39049587320925583  | VL :  0.32008853927254677  | VA :  90.60000000000001\n",
      "saving model\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.2756003737449646\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.23833198845386505\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.4147300124168396\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.342098593711853\n",
      "epoch :  34  /  100  | TL :  0.3855521391280255  | VL :  0.3229656796902418  | VA :  90.0\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.3146381974220276\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.4169440269470215\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.4199042320251465\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.47868767380714417\n",
      "epoch :  35  /  100  | TL :  0.3859285289671884  | VL :  0.3160727620124817  | VA :  90.2\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.14361217617988586\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.46595337986946106\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.43171426653862\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.3402264714241028\n",
      "epoch :  36  /  100  | TL :  0.382858316474293  | VL :  0.32146335393190384  | VA :  90.0\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.4614057242870331\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.5299906134605408\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.33526039123535156\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.42620909214019775\n",
      "epoch :  37  /  100  | TL :  0.38022121542425297  | VL :  0.3171768505126238  | VA :  90.2\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.5991405248641968\n",
      "epoch =  37  step =  100  of total steps  201  loss =  0.3303022086620331\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.376469224691391\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.35166412591934204\n",
      "epoch :  38  /  100  | TL :  0.3733891745408376  | VL :  0.3161949496716261  | VA :  90.4\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.4245753884315491\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.3878622055053711\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.2745332419872284\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.1970108151435852\n",
      "epoch :  39  /  100  | TL :  0.37545491977414086  | VL :  0.3185222279280424  | VA :  89.8\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.2672591507434845\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.31863707304000854\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.21195369958877563\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.3781878352165222\n",
      "epoch :  40  /  100  | TL :  0.38064654682999227  | VL :  0.31844992376863956  | VA :  89.8\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.4009052515029907\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.41056880354881287\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.4086156487464905\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.2171739935874939\n",
      "epoch :  41  /  100  | TL :  0.3728569746017456  | VL :  0.3140565287321806  | VA :  90.2\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.292555570602417\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.3828215003013611\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.33455026149749756\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.3328498601913452\n",
      "epoch :  42  /  100  | TL :  0.3742044566579126  | VL :  0.3148749563843012  | VA :  89.8\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.26318827271461487\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.31036946177482605\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.5400866270065308\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.2330138087272644\n",
      "epoch :  43  /  100  | TL :  0.39086600163237967  | VL :  0.30442517809569836  | VA :  90.0\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.3338939845561981\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.29797565937042236\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.34574654698371887\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.3771408796310425\n",
      "epoch :  44  /  100  | TL :  0.37630519901045517  | VL :  0.3078700639307499  | VA :  90.4\n",
      "epoch =  44  step =  50  of total steps  201  loss =  0.3426211476325989\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.33502835035324097\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.48051154613494873\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.2316565215587616\n",
      "epoch :  45  /  100  | TL :  0.3724975009166186  | VL :  0.31330051459372044  | VA :  89.60000000000001\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.38160884380340576\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.4216848611831665\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.3597188889980316\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.27430665493011475\n",
      "epoch :  46  /  100  | TL :  0.3638438368911174  | VL :  0.31814454309642315  | VA :  89.60000000000001\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.33697324991226196\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.2400602400302887\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.3274328112602234\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.2186160832643509\n",
      "epoch :  47  /  100  | TL :  0.359010563672182  | VL :  0.31073975563049316  | VA :  90.2\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.34481051564216614\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.31965047121047974\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.46410244703292847\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.4649406373500824\n",
      "epoch :  48  /  100  | TL :  0.36158959345141456  | VL :  0.3183520492166281  | VA :  89.8\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.6059803366661072\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.2797611653804779\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.47809505462646484\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.27965837717056274\n",
      "epoch :  49  /  100  | TL :  0.3689968836544758  | VL :  0.3052245397120714  | VA :  90.0\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.2476176917552948\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.33541572093963623\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.295234739780426\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.22733642160892487\n",
      "epoch :  50  /  100  | TL :  0.3710865219730643  | VL :  0.30774369090795517  | VA :  90.4\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.36524859070777893\n",
      "epoch =  50  step =  100  of total steps  201  loss =  0.36259254813194275\n",
      "epoch =  50  step =  150  of total steps  201  loss =  0.3578454256057739\n",
      "epoch =  50  step =  200  of total steps  201  loss =  0.16041766107082367\n",
      "epoch :  51  /  100  | TL :  0.36037468695225405  | VL :  0.3142929505556822  | VA :  90.0\n",
      "epoch =  51  step =  50  of total steps  201  loss =  0.3561081886291504\n",
      "epoch =  51  step =  100  of total steps  201  loss =  0.41811057925224304\n",
      "epoch =  51  step =  150  of total steps  201  loss =  0.2576897144317627\n",
      "epoch =  51  step =  200  of total steps  201  loss =  0.37617576122283936\n",
      "epoch :  52  /  100  | TL :  0.36930256474077405  | VL :  0.3131060693413019  | VA :  90.0\n",
      "epoch =  52  step =  50  of total steps  201  loss =  0.26572731137275696\n",
      "epoch =  52  step =  100  of total steps  201  loss =  0.4101284444332123\n",
      "epoch =  52  step =  150  of total steps  201  loss =  0.41597360372543335\n",
      "epoch =  52  step =  200  of total steps  201  loss =  0.2909945845603943\n",
      "epoch :  53  /  100  | TL :  0.3565555419494857  | VL :  0.303720997646451  | VA :  90.2\n",
      "epoch =  53  step =  50  of total steps  201  loss =  0.3472094237804413\n",
      "epoch =  53  step =  100  of total steps  201  loss =  0.26600462198257446\n",
      "epoch =  53  step =  150  of total steps  201  loss =  0.4798473119735718\n",
      "epoch =  53  step =  200  of total steps  201  loss =  0.46617060899734497\n",
      "epoch :  54  /  100  | TL :  0.36384378647922877  | VL :  0.3058948814868927  | VA :  90.60000000000001\n",
      "epoch =  54  step =  50  of total steps  201  loss =  0.46618688106536865\n",
      "epoch =  54  step =  100  of total steps  201  loss =  0.20313572883605957\n",
      "epoch =  54  step =  150  of total steps  201  loss =  0.5803254842758179\n",
      "epoch =  54  step =  200  of total steps  201  loss =  0.2113276869058609\n",
      "epoch :  55  /  100  | TL :  0.36031999249956503  | VL :  0.30752882175147533  | VA :  90.4\n",
      "epoch =  55  step =  50  of total steps  201  loss =  0.4286620616912842\n",
      "epoch =  55  step =  100  of total steps  201  loss =  0.476318359375\n",
      "epoch =  55  step =  150  of total steps  201  loss =  0.37797999382019043\n",
      "epoch =  55  step =  200  of total steps  201  loss =  0.2979090213775635\n",
      "epoch :  56  /  100  | TL :  0.3547477253633945  | VL :  0.30050213634967804  | VA :  90.60000000000001\n",
      "epoch =  56  step =  50  of total steps  201  loss =  0.2866935431957245\n",
      "epoch =  56  step =  100  of total steps  201  loss =  0.36587756872177124\n",
      "epoch =  56  step =  150  of total steps  201  loss =  0.27235445380210876\n",
      "epoch =  56  step =  200  of total steps  201  loss =  0.1928701102733612\n",
      "epoch :  57  /  100  | TL :  0.3560708099188496  | VL :  0.31034459732472897  | VA :  90.2\n",
      "epoch =  57  step =  50  of total steps  201  loss =  0.2109445035457611\n",
      "epoch =  57  step =  100  of total steps  201  loss =  0.44083699584007263\n",
      "epoch =  57  step =  150  of total steps  201  loss =  0.48360538482666016\n",
      "epoch =  57  step =  200  of total steps  201  loss =  0.2961657643318176\n",
      "epoch :  58  /  100  | TL :  0.360465484768597  | VL :  0.30531631223857403  | VA :  91.4\n",
      "saving model\n",
      "epoch =  58  step =  50  of total steps  201  loss =  0.328976571559906\n",
      "epoch =  58  step =  100  of total steps  201  loss =  0.4343762993812561\n",
      "epoch =  58  step =  150  of total steps  201  loss =  0.22378391027450562\n",
      "epoch =  58  step =  200  of total steps  201  loss =  0.1966010481119156\n",
      "epoch :  59  /  100  | TL :  0.35924341685291544  | VL :  0.30415852554142475  | VA :  90.4\n",
      "epoch =  59  step =  50  of total steps  201  loss =  0.3863176107406616\n",
      "epoch =  59  step =  100  of total steps  201  loss =  0.3533664047718048\n",
      "epoch =  59  step =  150  of total steps  201  loss =  0.4168233275413513\n",
      "epoch =  59  step =  200  of total steps  201  loss =  0.45381516218185425\n",
      "epoch :  60  /  100  | TL :  0.3570414976545827  | VL :  0.2990991659462452  | VA :  90.4\n",
      "epoch =  60  step =  50  of total steps  201  loss =  0.38783541321754456\n",
      "epoch =  60  step =  100  of total steps  201  loss =  0.3405344486236572\n",
      "epoch =  60  step =  150  of total steps  201  loss =  0.276866614818573\n",
      "epoch =  60  step =  200  of total steps  201  loss =  0.5369486212730408\n",
      "epoch :  61  /  100  | TL :  0.35124594788646224  | VL :  0.30273496732115746  | VA :  89.8\n",
      "epoch =  61  step =  50  of total steps  201  loss =  0.5081929564476013\n",
      "epoch =  61  step =  100  of total steps  201  loss =  0.37413203716278076\n",
      "epoch =  61  step =  150  of total steps  201  loss =  0.6128191351890564\n",
      "epoch =  61  step =  200  of total steps  201  loss =  0.25208136439323425\n",
      "epoch :  62  /  100  | TL :  0.34970680383307423  | VL :  0.3056791890412569  | VA :  90.0\n",
      "epoch =  62  step =  50  of total steps  201  loss =  0.3520192503929138\n",
      "epoch =  62  step =  100  of total steps  201  loss =  0.4012647867202759\n",
      "epoch =  62  step =  150  of total steps  201  loss =  0.4403366446495056\n",
      "epoch =  62  step =  200  of total steps  201  loss =  0.3199341297149658\n",
      "epoch :  63  /  100  | TL :  0.35658479367026047  | VL :  0.3032546415925026  | VA :  90.0\n",
      "epoch =  63  step =  50  of total steps  201  loss =  0.4174894094467163\n",
      "epoch =  63  step =  100  of total steps  201  loss =  0.30452072620391846\n",
      "epoch =  63  step =  150  of total steps  201  loss =  0.37033766508102417\n",
      "epoch =  63  step =  200  of total steps  201  loss =  0.29509931802749634\n",
      "epoch :  64  /  100  | TL :  0.3426837443712339  | VL :  0.2929858807474375  | VA :  91.0\n",
      "epoch =  64  step =  50  of total steps  201  loss =  0.22869035601615906\n",
      "epoch =  64  step =  100  of total steps  201  loss =  0.2646186053752899\n",
      "epoch =  64  step =  150  of total steps  201  loss =  0.25507113337516785\n",
      "epoch =  64  step =  200  of total steps  201  loss =  0.23956416547298431\n",
      "epoch :  65  /  100  | TL :  0.35687949334211017  | VL :  0.3035586513578892  | VA :  90.60000000000001\n",
      "epoch =  65  step =  50  of total steps  201  loss =  0.24226263165473938\n",
      "epoch =  65  step =  100  of total steps  201  loss =  0.3378496468067169\n",
      "epoch =  65  step =  150  of total steps  201  loss =  0.15092281997203827\n",
      "epoch =  65  step =  200  of total steps  201  loss =  0.33742555975914\n",
      "epoch :  66  /  100  | TL :  0.3504956679335281  | VL :  0.30054352432489395  | VA :  91.0\n",
      "epoch =  66  step =  50  of total steps  201  loss =  0.34495651721954346\n",
      "epoch =  66  step =  100  of total steps  201  loss =  0.2254663109779358\n",
      "epoch =  66  step =  150  of total steps  201  loss =  0.4187372922897339\n",
      "epoch =  66  step =  200  of total steps  201  loss =  0.4058852195739746\n",
      "epoch :  67  /  100  | TL :  0.35093348139702385  | VL :  0.30456276796758175  | VA :  90.4\n",
      "epoch =  67  step =  50  of total steps  201  loss =  0.32858848571777344\n",
      "epoch =  67  step =  100  of total steps  201  loss =  0.29627177119255066\n",
      "epoch =  67  step =  150  of total steps  201  loss =  0.220051109790802\n",
      "epoch =  67  step =  200  of total steps  201  loss =  0.32611003518104553\n",
      "epoch :  68  /  100  | TL :  0.3485893092641783  | VL :  0.2985038720071316  | VA :  90.2\n",
      "epoch =  68  step =  50  of total steps  201  loss =  0.3468126952648163\n",
      "epoch =  68  step =  100  of total steps  201  loss =  0.43763890862464905\n",
      "epoch =  68  step =  150  of total steps  201  loss =  0.26425814628601074\n",
      "epoch =  68  step =  200  of total steps  201  loss =  0.3296394944190979\n",
      "epoch :  69  /  100  | TL :  0.34880387264104623  | VL :  0.3040479552000761  | VA :  90.60000000000001\n",
      "epoch =  69  step =  50  of total steps  201  loss =  0.35763150453567505\n",
      "epoch =  69  step =  100  of total steps  201  loss =  0.2536100149154663\n",
      "epoch =  69  step =  150  of total steps  201  loss =  0.3063303828239441\n",
      "epoch =  69  step =  200  of total steps  201  loss =  0.43647992610931396\n",
      "epoch :  70  /  100  | TL :  0.3444099765067077  | VL :  0.2987764459103346  | VA :  90.60000000000001\n",
      "epoch =  70  step =  50  of total steps  201  loss =  0.2593029737472534\n",
      "epoch =  70  step =  100  of total steps  201  loss =  0.24526110291481018\n",
      "epoch =  70  step =  150  of total steps  201  loss =  0.2613627314567566\n",
      "epoch =  70  step =  200  of total steps  201  loss =  0.3007062077522278\n",
      "epoch :  71  /  100  | TL :  0.34997476448318854  | VL :  0.29760195687413216  | VA :  91.0\n",
      "epoch =  71  step =  50  of total steps  201  loss =  0.41956794261932373\n",
      "epoch =  71  step =  100  of total steps  201  loss =  0.1934703141450882\n",
      "epoch =  71  step =  150  of total steps  201  loss =  0.359144926071167\n",
      "epoch =  71  step =  200  of total steps  201  loss =  0.35742777585983276\n",
      "epoch :  72  /  100  | TL :  0.3546245057339692  | VL :  0.2999131940305233  | VA :  90.8\n",
      "epoch =  72  step =  50  of total steps  201  loss =  0.4166163504123688\n",
      "epoch =  72  step =  100  of total steps  201  loss =  0.3720986247062683\n",
      "epoch =  72  step =  150  of total steps  201  loss =  0.4210798144340515\n",
      "epoch =  72  step =  200  of total steps  201  loss =  0.5812694430351257\n",
      "epoch :  73  /  100  | TL :  0.34417859909694587  | VL :  0.3013597223907709  | VA :  90.60000000000001\n",
      "epoch =  73  step =  50  of total steps  201  loss =  0.33415907621383667\n",
      "epoch =  73  step =  100  of total steps  201  loss =  0.3331362009048462\n",
      "epoch =  73  step =  150  of total steps  201  loss =  0.34551605582237244\n",
      "epoch =  73  step =  200  of total steps  201  loss =  0.3867236375808716\n",
      "epoch :  74  /  100  | TL :  0.34498622557565345  | VL :  0.29519948177039623  | VA :  90.0\n",
      "epoch =  74  step =  50  of total steps  201  loss =  0.3353192210197449\n",
      "epoch =  74  step =  100  of total steps  201  loss =  0.6105166077613831\n",
      "epoch =  74  step =  150  of total steps  201  loss =  0.3313760459423065\n",
      "epoch =  74  step =  200  of total steps  201  loss =  0.5528744459152222\n",
      "epoch :  75  /  100  | TL :  0.3392637753442152  | VL :  0.2979799844324589  | VA :  91.60000000000001\n",
      "saving model\n",
      "epoch =  75  step =  50  of total steps  201  loss =  0.31190288066864014\n",
      "epoch =  75  step =  100  of total steps  201  loss =  0.23215557634830475\n",
      "epoch =  75  step =  150  of total steps  201  loss =  0.3376186490058899\n",
      "epoch =  75  step =  200  of total steps  201  loss =  0.2590731382369995\n",
      "epoch :  76  /  100  | TL :  0.34266371847088656  | VL :  0.2956543620675802  | VA :  90.4\n",
      "epoch =  76  step =  50  of total steps  201  loss =  0.6859937906265259\n",
      "epoch =  76  step =  100  of total steps  201  loss =  0.4650573134422302\n",
      "epoch =  76  step =  150  of total steps  201  loss =  0.4797555208206177\n",
      "epoch =  76  step =  200  of total steps  201  loss =  0.29347193241119385\n",
      "epoch :  77  /  100  | TL :  0.3455478702463321  | VL :  0.29379344917833805  | VA :  91.2\n",
      "epoch =  77  step =  50  of total steps  201  loss =  0.2508564889431\n",
      "epoch =  77  step =  100  of total steps  201  loss =  0.19049698114395142\n",
      "epoch =  77  step =  150  of total steps  201  loss =  0.3470781445503235\n",
      "epoch =  77  step =  200  of total steps  201  loss =  0.23826491832733154\n",
      "epoch :  78  /  100  | TL :  0.34093674194456924  | VL :  0.2933116592466831  | VA :  91.4\n",
      "epoch =  78  step =  50  of total steps  201  loss =  0.3772948682308197\n",
      "epoch =  78  step =  100  of total steps  201  loss =  0.275852769613266\n",
      "epoch =  78  step =  150  of total steps  201  loss =  0.5196385979652405\n",
      "epoch =  78  step =  200  of total steps  201  loss =  0.23841406404972076\n",
      "epoch :  79  /  100  | TL :  0.348131750205263  | VL :  0.2922601420432329  | VA :  90.60000000000001\n",
      "epoch =  79  step =  50  of total steps  201  loss =  0.2888798713684082\n",
      "epoch =  79  step =  100  of total steps  201  loss =  0.36498427391052246\n",
      "epoch =  79  step =  150  of total steps  201  loss =  0.3356555700302124\n",
      "epoch =  79  step =  200  of total steps  201  loss =  0.2644997239112854\n",
      "epoch :  80  /  100  | TL :  0.34213390049353165  | VL :  0.295005913823843  | VA :  91.4\n",
      "epoch =  80  step =  50  of total steps  201  loss =  0.15677230060100555\n",
      "epoch =  80  step =  100  of total steps  201  loss =  0.481591135263443\n",
      "epoch =  80  step =  150  of total steps  201  loss =  0.28914695978164673\n",
      "epoch =  80  step =  200  of total steps  201  loss =  0.1805611550807953\n",
      "epoch :  81  /  100  | TL :  0.3399582855514626  | VL :  0.29204973950982094  | VA :  91.4\n",
      "epoch =  81  step =  50  of total steps  201  loss =  0.33025631308555603\n",
      "epoch =  81  step =  100  of total steps  201  loss =  0.35809382796287537\n",
      "epoch =  81  step =  150  of total steps  201  loss =  0.35462719202041626\n",
      "epoch =  81  step =  200  of total steps  201  loss =  0.298644483089447\n",
      "epoch :  82  /  100  | TL :  0.3494061511473276  | VL :  0.29075590148568153  | VA :  91.0\n",
      "epoch =  82  step =  50  of total steps  201  loss =  0.27445298433303833\n",
      "epoch =  82  step =  100  of total steps  201  loss =  0.47525227069854736\n",
      "epoch =  82  step =  150  of total steps  201  loss =  0.21927858889102936\n",
      "epoch =  82  step =  200  of total steps  201  loss =  0.40380358695983887\n",
      "epoch :  83  /  100  | TL :  0.3407871204970488  | VL :  0.2870177645236254  | VA :  91.2\n",
      "epoch =  83  step =  50  of total steps  201  loss =  0.3156636655330658\n",
      "epoch =  83  step =  100  of total steps  201  loss =  0.28889602422714233\n",
      "epoch =  83  step =  150  of total steps  201  loss =  0.2207299768924713\n",
      "epoch =  83  step =  200  of total steps  201  loss =  0.24224290251731873\n",
      "epoch :  84  /  100  | TL :  0.338779775064383  | VL :  0.30419014766812325  | VA :  90.60000000000001\n",
      "epoch =  84  step =  50  of total steps  201  loss =  0.3156481385231018\n",
      "epoch =  84  step =  100  of total steps  201  loss =  0.2531225085258484\n",
      "epoch =  84  step =  150  of total steps  201  loss =  0.410500705242157\n",
      "epoch =  84  step =  200  of total steps  201  loss =  0.38541802763938904\n",
      "epoch :  85  /  100  | TL :  0.33567924975459257  | VL :  0.2957302816212177  | VA :  90.60000000000001\n",
      "epoch =  85  step =  50  of total steps  201  loss =  0.31742537021636963\n",
      "epoch =  85  step =  100  of total steps  201  loss =  0.27909284830093384\n",
      "epoch =  85  step =  150  of total steps  201  loss =  0.3997041881084442\n",
      "epoch =  85  step =  200  of total steps  201  loss =  0.4406093955039978\n",
      "epoch :  86  /  100  | TL :  0.3389325517401173  | VL :  0.2892325446009636  | VA :  91.0\n",
      "epoch =  86  step =  50  of total steps  201  loss =  0.2041410207748413\n",
      "epoch =  86  step =  100  of total steps  201  loss =  0.4021955728530884\n",
      "epoch =  86  step =  150  of total steps  201  loss =  0.38552454113960266\n",
      "epoch =  86  step =  200  of total steps  201  loss =  0.6451786756515503\n",
      "epoch :  87  /  100  | TL :  0.3460653291561117  | VL :  0.29216694086790085  | VA :  91.0\n",
      "epoch =  87  step =  50  of total steps  201  loss =  0.4054655432701111\n",
      "epoch =  87  step =  100  of total steps  201  loss =  0.4810572564601898\n",
      "epoch =  87  step =  150  of total steps  201  loss =  0.35543328523635864\n",
      "epoch =  87  step =  200  of total steps  201  loss =  0.2827826142311096\n",
      "epoch :  88  /  100  | TL :  0.33409853561304104  | VL :  0.2989386972039938  | VA :  90.2\n",
      "epoch =  88  step =  50  of total steps  201  loss =  0.25063419342041016\n",
      "epoch =  88  step =  100  of total steps  201  loss =  0.5935989022254944\n",
      "epoch =  88  step =  150  of total steps  201  loss =  0.3266322612762451\n",
      "epoch =  88  step =  200  of total steps  201  loss =  0.3076856732368469\n",
      "epoch :  89  /  100  | TL :  0.3356940423819556  | VL :  0.296197522431612  | VA :  90.2\n",
      "epoch =  89  step =  50  of total steps  201  loss =  0.2513074576854706\n",
      "epoch =  89  step =  100  of total steps  201  loss =  0.2530328035354614\n",
      "epoch =  89  step =  150  of total steps  201  loss =  0.38574838638305664\n",
      "epoch =  89  step =  200  of total steps  201  loss =  0.4195782542228699\n",
      "epoch :  90  /  100  | TL :  0.34460977602064313  | VL :  0.2900779228657484  | VA :  91.8\n",
      "saving model\n",
      "epoch =  90  step =  50  of total steps  201  loss =  0.3992573022842407\n",
      "epoch =  90  step =  100  of total steps  201  loss =  0.47028857469558716\n",
      "epoch =  90  step =  150  of total steps  201  loss =  0.2657051086425781\n",
      "epoch =  90  step =  200  of total steps  201  loss =  0.6813957691192627\n",
      "epoch :  91  /  100  | TL :  0.3411334926512704  | VL :  0.29235687106847763  | VA :  92.0\n",
      "saving model\n",
      "epoch =  91  step =  50  of total steps  201  loss =  0.3328382074832916\n",
      "epoch =  91  step =  100  of total steps  201  loss =  0.3424012064933777\n",
      "epoch =  91  step =  150  of total steps  201  loss =  0.21475200355052948\n",
      "epoch =  91  step =  200  of total steps  201  loss =  0.4433136582374573\n",
      "epoch :  92  /  100  | TL :  0.33287682762341714  | VL :  0.3005650620907545  | VA :  90.8\n",
      "epoch =  92  step =  50  of total steps  201  loss =  0.41163063049316406\n",
      "epoch =  92  step =  100  of total steps  201  loss =  0.2616047263145447\n",
      "epoch =  92  step =  150  of total steps  201  loss =  0.23748043179512024\n",
      "epoch =  92  step =  200  of total steps  201  loss =  0.31219321489334106\n",
      "epoch :  93  /  100  | TL :  0.33373770925832624  | VL :  0.29920632019639015  | VA :  91.2\n",
      "epoch =  93  step =  50  of total steps  201  loss =  0.16823725402355194\n",
      "epoch =  93  step =  100  of total steps  201  loss =  0.2894632816314697\n",
      "epoch =  93  step =  150  of total steps  201  loss =  0.2661753296852112\n",
      "epoch =  93  step =  200  of total steps  201  loss =  0.25470927357673645\n",
      "epoch :  94  /  100  | TL :  0.33680195928509554  | VL :  0.29070165008306503  | VA :  91.60000000000001\n",
      "epoch =  94  step =  50  of total steps  201  loss =  0.29888856410980225\n",
      "epoch =  94  step =  100  of total steps  201  loss =  0.3968518376350403\n",
      "epoch =  94  step =  150  of total steps  201  loss =  0.30761098861694336\n",
      "epoch =  94  step =  200  of total steps  201  loss =  0.4936487674713135\n",
      "epoch :  95  /  100  | TL :  0.33772390910345523  | VL :  0.2910683285444975  | VA :  91.4\n",
      "epoch =  95  step =  50  of total steps  201  loss =  0.3560325801372528\n",
      "epoch =  95  step =  100  of total steps  201  loss =  0.2595869302749634\n",
      "epoch =  95  step =  150  of total steps  201  loss =  0.36593031883239746\n",
      "epoch =  95  step =  200  of total steps  201  loss =  0.11293651163578033\n",
      "epoch :  96  /  100  | TL :  0.3343939493915335  | VL :  0.3071434199810028  | VA :  90.60000000000001\n",
      "epoch =  96  step =  50  of total steps  201  loss =  0.5292490124702454\n",
      "epoch =  96  step =  100  of total steps  201  loss =  0.3155286908149719\n",
      "epoch =  96  step =  150  of total steps  201  loss =  0.3776438534259796\n",
      "epoch =  96  step =  200  of total steps  201  loss =  0.3547317087650299\n",
      "epoch :  97  /  100  | TL :  0.3413460615707274  | VL :  0.2937620263546705  | VA :  90.2\n",
      "epoch =  97  step =  50  of total steps  201  loss =  0.24887195229530334\n",
      "epoch =  97  step =  100  of total steps  201  loss =  0.24135103821754456\n",
      "epoch =  97  step =  150  of total steps  201  loss =  0.2400565892457962\n",
      "epoch =  97  step =  200  of total steps  201  loss =  0.19865097105503082\n",
      "epoch :  98  /  100  | TL :  0.3371154971754373  | VL :  0.29389986023306847  | VA :  91.0\n",
      "epoch =  98  step =  50  of total steps  201  loss =  0.2823192775249481\n",
      "epoch =  98  step =  100  of total steps  201  loss =  0.2948797643184662\n",
      "epoch =  98  step =  150  of total steps  201  loss =  0.3567177951335907\n",
      "epoch =  98  step =  200  of total steps  201  loss =  0.2933807671070099\n",
      "epoch :  99  /  100  | TL :  0.33694643990613926  | VL :  0.29149764217436314  | VA :  90.60000000000001\n",
      "epoch =  99  step =  50  of total steps  201  loss =  0.49624091386795044\n",
      "epoch =  99  step =  100  of total steps  201  loss =  0.41712480783462524\n",
      "epoch =  99  step =  150  of total steps  201  loss =  0.37354010343551636\n",
      "epoch =  99  step =  200  of total steps  201  loss =  0.26627495884895325\n",
      "epoch :  100  /  100  | TL :  0.3309592539398231  | VL :  0.29909798316657543  | VA :  91.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 0\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, labels)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, labels)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc = _get_accuracy(data.valid_dl, net)\n",
    "\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss, ' | VA : ', val_acc * 100)\n",
    "\n",
    "    if (val_acc * 100) > min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_acc * 100\n",
    "        torch.save(net.state_dict(), '../saved_models/small_classifier/model1.pt')savename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "0.936\n"
     ]
    }
   ],
   "source": [
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/small_classifier/model4.pt', map_location = 'cpu'))\n",
    "net.cuda()\n",
    "\n",
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "\n",
    "print(_get_accuracy(data.valid_dl, net))\n",
    "print(_get_accuracy(data.valid_dl, learn.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "epoch =  0  step =  50  of total steps  201  loss =  2.0896389484405518\n",
      "epoch =  0  step =  100  of total steps  201  loss =  1.7182304859161377\n",
      "epoch =  0  step =  150  of total steps  201  loss =  1.5697768926620483\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "for repeated in range(0, 1) : \n",
    "    torch.manual_seed(repeated)\n",
    "    torch.cuda.manual_seed(repeated)\n",
    "\n",
    "    # stage should be in 0 to 5 (5 for classifier stage)\n",
    "    hyper_params = {\n",
    "        \"stage\": 5,\n",
    "        \"repeated\": repeated,\n",
    "        \"num_classes\": 10,\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epochs\": 100,\n",
    "        \"learning_rate\": 1e-4\n",
    "    }\n",
    "\n",
    "    path = untar_data(URLs.IMAGENETTE)\n",
    "    tfms = get_transforms(do_flip=False)\n",
    "    data = ImageDataBunch.from_folder(path, train = 'train', valid = 'val', bs = hyper_params[\"batch_size\"], size = 224, ds_tfms = tfms).normalize(imagenet_stats)\n",
    "    \n",
    "    class Flatten(nn.Module) :\n",
    "        def forward(self, input):\n",
    "            return input.view(input.size(0), -1)\n",
    "\n",
    "    def conv2(ni, nf) : \n",
    "        return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "    class ResBlock(nn.Module):\n",
    "        def __init__(self, nf):\n",
    "            super().__init__()\n",
    "            self.conv1 = conv_layer(nf,nf)\n",
    "\n",
    "        def forward(self, x): \n",
    "            return (x + self.conv1(x))\n",
    "\n",
    "    def conv_and_res(ni, nf): \n",
    "        return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "    def conv_(nf) : \n",
    "        return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "        nn.MaxPool2d(3, 2, padding = 1),\n",
    "        conv_(64),\n",
    "        conv_and_res(64, 128),\n",
    "        conv_and_res(128, 256),\n",
    "        AdaptiveConcatPool2d(),\n",
    "        Flatten(),\n",
    "        nn.Linear(2 * 256, 128),\n",
    "        nn.Linear(128, hyper_params[\"num_classes\"])\n",
    "    )\n",
    "\n",
    "    net.cpu()\n",
    "    filename = '../saved_models/small_stage4/model' + str(repeated) + '.pt'\n",
    "    net.load_state_dict(torch.load(filename, map_location = 'cpu'))\n",
    "\n",
    "    if torch.cuda.is_available() : \n",
    "        net = net.cuda()\n",
    "        print('Model on GPU')\n",
    "\n",
    "    for name, param in net.named_parameters() : \n",
    "        param.requires_grad = False\n",
    "        if name[0] == '7' or name[0] == '8':\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "    total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "    train_loss_list = list()\n",
    "    val_loss_list = list()\n",
    "    min_val = 0\n",
    "    savename = '../saved_models/small_classifier/model' + str(repeated) + '.pt'\n",
    "    for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "        trn = []\n",
    "        net.train()\n",
    "        for i, (images, labels) in enumerate(data.train_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            y_pred = net(images)\n",
    "\n",
    "            loss = F.cross_entropy(y_pred, labels)\n",
    "            trn.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "    #         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 50 == 49 :\n",
    "                print('epoch = ', epoch, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "        train_loss = (sum(trn) / len(trn))\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        net.eval()\n",
    "        val = []\n",
    "        with torch.no_grad() :\n",
    "            for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "                if torch.cuda.is_available():\n",
    "                    images = torch.autograd.Variable(images).cuda().float()\n",
    "                    labels = torch.autograd.Variable(labels).cuda()\n",
    "                else : \n",
    "                    images = torch.autograd.Variable(images).float()\n",
    "                    labels = torch.autograd.Variable(labels)\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = net(images)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred, labels)\n",
    "                val.append(loss.item())\n",
    "\n",
    "        val_loss = sum(val) / len(val)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc = _get_accuracy(data.valid_dl, net)\n",
    "\n",
    "        print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss, ' | VA : ', val_acc * 100)\n",
    "\n",
    "        if (val_acc * 100) > min_val :\n",
    "            print('saving model')\n",
    "            min_val = val_acc * 100\n",
    "            torch.save(net.state_dict(), savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ak_fastai)",
   "language": "python",
   "name": "ak_fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
