{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "# stage should be in 0 to 5 and for 0\n",
    "hyper_params = {\n",
    "    \"stage\": 3,\n",
    "    \"repeated\": 1,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "data = ImageDataBunch.from_folder(path, train = 'train', valid = 'val', bs = hyper_params[\"batch_size\"], size = 224, ds_tfms = tfms).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "True\n",
      "4.0.2.weight torch.Size([256])\n",
      "True\n",
      "4.0.2.bias torch.Size([256])\n",
      "True\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "True\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "True\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "True\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "False\n",
      "5.0.2.weight torch.Size([512])\n",
      "False\n",
      "5.0.2.bias torch.Size([512])\n",
      "False\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "False\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "False\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "False\n",
      "8.weight torch.Size([256, 1024])\n",
      "False\n",
      "8.bias torch.Size([256])\n",
      "False\n",
      "9.weight torch.Size([10, 256])\n",
      "False\n",
      "9.bias torch.Size([10])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "# learn.summary()\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage3/model1.pt', map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == str(hyper_params['stage'] + 1) and hyper_params['stage'] != 0 :\n",
    "        param.requires_grad = True\n",
    "    elif name[0] == str(hyper_params['stage']) and hyper_params['stage'] == 0 : \n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "              ReLU-2         [-1, 64, 112, 112]               0\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-9           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "         ResBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12          [-1, 128, 28, 28]          73,728\n",
      "             ReLU-13          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-14          [-1, 128, 28, 28]             256\n",
      "           Conv2d-15          [-1, 128, 28, 28]         147,456\n",
      "             ReLU-16          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
      "         ResBlock-18          [-1, 128, 28, 28]               0\n",
      "           Conv2d-19          [-1, 256, 14, 14]         294,912\n",
      "             ReLU-20          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-21          [-1, 256, 14, 14]             512\n",
      "           Conv2d-22          [-1, 256, 14, 14]         589,824\n",
      "             ReLU-23          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-24          [-1, 256, 14, 14]             512\n",
      "         ResBlock-25          [-1, 256, 14, 14]               0\n",
      "           Conv2d-26            [-1, 512, 7, 7]       1,179,648\n",
      "             ReLU-27            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-28            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-29            [-1, 512, 7, 7]       2,359,296\n",
      "             ReLU-30            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-31            [-1, 512, 7, 7]           1,024\n",
      "         ResBlock-32            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-33            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-34            [-1, 512, 1, 1]               0\n",
      "AdaptiveConcatPool2d-35           [-1, 1024, 1, 1]               0\n",
      "          Flatten-36                 [-1, 1024]               0\n",
      "           Linear-37                  [-1, 256]         262,400\n",
      "           Linear-38                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,996,938\n",
      "Trainable params: 9,536\n",
      "Non-trainable params: 4,987,402\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 40.03\n",
      "Params size (MB): 19.06\n",
      "Estimated Total Size (MB): 59.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x, y = next(iter(data.train_dl))\n",
    "# net(torch.autograd.Variable(x).cuda())\n",
    "summary(net, (3, 224, 224))\n",
    "# print(learn.summary())\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures :\n",
    "    def __init__(self, m) : \n",
    "        self.handle = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, m, inp, outp) : \n",
    "        self.features = outp\n",
    "    def remove(self) :\n",
    "        self.handle.remove()\n",
    "        \n",
    "# saving outputs of all Basic Blocks\n",
    "mdl = learn.model\n",
    "sf = [SaveFeatures(m) for m in [mdl[0][2], mdl[0][4], mdl[0][5], mdl[0][6], mdl[0][7]]]\n",
    "sf2 = [SaveFeatures(m) for m in [net[0], net[2], net[3], net[4], net[5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 112, 112])\n",
      "torch.Size([64, 64, 56, 56])\n",
      "torch.Size([64, 128, 28, 28])\n",
      "torch.Size([64, 256, 14, 14])\n",
      "torch.Size([64, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(data.train_dl))\n",
    "x = torch.autograd.Variable(x).cuda()\n",
    "out1 = mdl(x)\n",
    "out2 = net(x)\n",
    "for i in range(5) : \n",
    "    print(sf[i].features.shape)\n",
    "    assert(sf[i].features.shape == sf2[i].features.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/akshaykvnit/kd0/382597568a24467c8d03d7a055db4177\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [2010]                    : (0.01984086073935032, 0.7521964907646179)\n",
      "COMET INFO:     sys.gpu.0.free_memory [677]    : (3472359424.0, 3900178432.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [677]: (0.0, 100.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory         : (11721506816.0, 11721506816.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [677]    : (7821328384.0, 8249147392.0)\n",
      "COMET INFO:     sys.gpu.1.free_memory [677]    : (2210201600.0, 2211184640.0)\n",
      "COMET INFO:     sys.gpu.1.gpu_utilization [677]: (0.0, 0.0)\n",
      "COMET INFO:     sys.gpu.1.total_memory         : (6233391104.0, 6233391104.0)\n",
      "COMET INFO:     sys.gpu.1.used_memory [677]    : (4022206464.0, 4023189504.0)\n",
      "COMET INFO:     train_loss [100]               : (0.021222104601080146, 0.489455190019228)\n",
      "COMET INFO:     val_loss [100]                 : (0.021164196077734232, 0.35120435059070587)\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (2.0.9) detected. current: 2.0.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/ee728b5752964df8bfcf88672a998b5b\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  50  of total steps  201  loss =  0.5327513217926025\n",
      "epoch =  0  step =  100  of total steps  201  loss =  0.38474878668785095\n",
      "epoch =  0  step =  150  of total steps  201  loss =  0.3131357431411743\n",
      "epoch =  0  step =  200  of total steps  201  loss =  0.2687986195087433\n",
      "epoch :  1  /  100  | TL :  0.43484806050708635  | VL :  0.26971403881907463\n",
      "saving model\n",
      "epoch =  1  step =  50  of total steps  201  loss =  0.23168496787548065\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.20421501994132996\n",
      "epoch =  1  step =  150  of total steps  201  loss =  0.1819385588169098\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.1690216213464737\n",
      "epoch :  2  /  100  | TL :  0.20939045729328745  | VL :  0.1696688998490572\n",
      "saving model\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "if hyper_params['stage'] == 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "else : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage'] + 1) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "        y_pred2 = mdl(images)\n",
    "\n",
    "        loss = F.mse_loss(sf2[hyper_params[\"stage\"]].features, sf[hyper_params[\"stage\"]].features)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            y_pred2 = mdl(images)\n",
    "            loss = F.mse_loss(sf[hyper_params[\"stage\"]].features, sf2[hyper_params[\"stage\"]].features)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "\n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = Learner(data, net, metrics = accuracy)\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model0.pt', map_location = 'cpu'))\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(hyper_params[\"num_epochs\"]), train_loss_list, label = 'train_loss') \n",
    "plt.plot(range(hyper_params[\"num_epochs\"]), val_loss_list, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/stage5/train_loss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier only after stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "False\n",
      "5.0.2.weight torch.Size([512])\n",
      "False\n",
      "5.0.2.bias torch.Size([512])\n",
      "False\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "False\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "False\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "False\n",
      "8.weight torch.Size([256, 1024])\n",
      "True\n",
      "8.bias torch.Size([256])\n",
      "True\n",
      "9.weight torch.Size([10, 256])\n",
      "True\n",
      "9.bias torch.Size([10])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    \"stage\": 5,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model0.pt', map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == '8' or name[0] == '9':\n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = torch.autograd.Variable(images).float()\n",
    "        labels = torch.autograd.Variable(labels).float()\n",
    "        \n",
    "        if torch.cuda.is_available() : \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = Net.forward(images)\n",
    "        outputs = F.log_softmax(outputs, dim = 1)\n",
    "\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        pred_ind = pred_ind.data.cpu().numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = labels - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/akshaykvnit/kd0/68e29e7c7396443f9d5bc8c1d8fb34f7\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [104]                   : (0.09282064437866211, 2.5855929851531982)\n",
      "COMET INFO:     sys.gpu.0.free_memory [5]    : (10388766720.0, 10388766720.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [5]: (0.0, 39.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory       : (11721506816.0, 11721506816.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [5]    : (1332740096.0, 1332740096.0)\n",
      "COMET INFO:     sys.gpu.1.free_memory [5]    : (6221987840.0, 6221987840.0)\n",
      "COMET INFO:     sys.gpu.1.gpu_utilization [5]: (0.0, 0.0)\n",
      "COMET INFO:     sys.gpu.1.total_memory       : (6233391104.0, 6233391104.0)\n",
      "COMET INFO:     sys.gpu.1.used_memory [5]    : (11403264.0, 11403264.0)\n",
      "COMET INFO:     train_loss [5]               : (0.2542623481643734, 0.6748504299874329)\n",
      "COMET INFO:     val_acc [5]                  : (0.922, 0.934)\n",
      "COMET INFO:     val_loss [5]                 : (0.20247302344068885, 0.27922892197966576)\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/38af5c0d61474e2d9273ada6ea156fe5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  50  of total steps  201  loss =  0.8784895539283752\n",
      "epoch =  0  step =  100  of total steps  201  loss =  0.4596188962459564\n",
      "epoch =  0  step =  150  of total steps  201  loss =  0.3605707585811615\n",
      "epoch =  0  step =  200  of total steps  201  loss =  0.28698939085006714\n",
      "epoch :  1  /  100  | TL :  0.6981583358488272  | VL :  0.2788702854886651  | VA :  94.0\n",
      "saving model\n",
      "epoch =  1  step =  50  of total steps  201  loss =  0.43684956431388855\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.257024884223938\n",
      "epoch =  1  step =  150  of total steps  201  loss =  0.261644184589386\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.2497667670249939\n",
      "epoch :  2  /  100  | TL :  0.3284727025091352  | VL :  0.22625178191810846  | VA :  93.4\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.31416481733322144\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.34493571519851685\n",
      "epoch =  2  step =  150  of total steps  201  loss =  0.43016624450683594\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.5088800191879272\n",
      "epoch :  3  /  100  | TL :  0.2902423296475885  | VL :  0.21517107030376792  | VA :  92.80000000000001\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.23638996481895447\n",
      "epoch =  3  step =  100  of total steps  201  loss =  0.26187020540237427\n",
      "epoch =  3  step =  150  of total steps  201  loss =  0.2791038751602173\n",
      "epoch =  3  step =  200  of total steps  201  loss =  0.264411598443985\n",
      "epoch :  4  /  100  | TL :  0.2746666660208014  | VL :  0.2047434956766665  | VA :  92.80000000000001\n",
      "epoch =  4  step =  50  of total steps  201  loss =  0.25847554206848145\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.3105309307575226\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.27651649713516235\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.261848509311676\n",
      "epoch :  5  /  100  | TL :  0.2633412720922807  | VL :  0.207779704593122  | VA :  93.4\n",
      "epoch =  5  step =  50  of total steps  201  loss =  0.1278153657913208\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.18454846739768982\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.2844318151473999\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.26928406953811646\n",
      "epoch :  6  /  100  | TL :  0.26328115917705186  | VL :  0.20662269508466125  | VA :  93.0\n",
      "epoch =  6  step =  50  of total steps  201  loss =  0.30374234914779663\n",
      "epoch =  6  step =  100  of total steps  201  loss =  0.22687304019927979\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.08077244460582733\n",
      "epoch =  6  step =  200  of total steps  201  loss =  0.13239970803260803\n",
      "epoch :  7  /  100  | TL :  0.24792141174499074  | VL :  0.20776139805093408  | VA :  93.60000000000001\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.18327292799949646\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.312863290309906\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.32132449746131897\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.22595562040805817\n",
      "epoch :  8  /  100  | TL :  0.2514310070494218  | VL :  0.2195680094882846  | VA :  92.60000000000001\n",
      "epoch =  8  step =  50  of total steps  201  loss =  0.21055161952972412\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.25211572647094727\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.19057099521160126\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.3789454698562622\n",
      "epoch :  9  /  100  | TL :  0.24296303695781313  | VL :  0.22576642222702503  | VA :  92.2\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.29526883363723755\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.3362762928009033\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.202084481716156\n",
      "epoch =  9  step =  200  of total steps  201  loss =  0.2655912935733795\n",
      "epoch :  10  /  100  | TL :  0.24155521841339805  | VL :  0.20563008030876517  | VA :  93.60000000000001\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.2488257735967636\n",
      "epoch =  10  step =  100  of total steps  201  loss =  0.372101366519928\n",
      "epoch =  10  step =  150  of total steps  201  loss =  0.14308716356754303\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.14739346504211426\n",
      "epoch :  11  /  100  | TL :  0.24629901071537785  | VL :  0.20945659140124917  | VA :  92.4\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.1840231865644455\n",
      "epoch =  11  step =  100  of total steps  201  loss =  0.40374600887298584\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.19593767821788788\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.18194571137428284\n",
      "epoch :  12  /  100  | TL :  0.24336236707903258  | VL :  0.2202791371382773  | VA :  92.0\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.24920272827148438\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.4710988998413086\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.3104923963546753\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.10895299166440964\n",
      "epoch :  13  /  100  | TL :  0.24499180468160714  | VL :  0.2121834997087717  | VA :  93.0\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.25743669271469116\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.4799787402153015\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.226236030459404\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.25183963775634766\n",
      "epoch :  14  /  100  | TL :  0.237427540864814  | VL :  0.21776134055107832  | VA :  92.60000000000001\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.13919349014759064\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.17916135489940643\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.2095920294523239\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.1525198519229889\n",
      "epoch :  15  /  100  | TL :  0.24328703388794146  | VL :  0.2209763708524406  | VA :  92.4\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.256217360496521\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.4744461476802826\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.3425982594490051\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.19082111120224\n",
      "epoch :  16  /  100  | TL :  0.2318665914756445  | VL :  0.21217101672664285  | VA :  93.2\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.2300536036491394\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.20806503295898438\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.2432381808757782\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.10463928431272507\n",
      "epoch :  17  /  100  | TL :  0.23323839067004212  | VL :  0.21110533736646175  | VA :  93.60000000000001\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.1391129046678543\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.1727377474308014\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.13050580024719238\n",
      "epoch =  17  step =  200  of total steps  201  loss =  0.15745554864406586\n",
      "epoch :  18  /  100  | TL :  0.23132630926904393  | VL :  0.22160714957863092  | VA :  92.60000000000001\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.3065853416919708\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.19926974177360535\n",
      "epoch =  18  step =  150  of total steps  201  loss =  0.16891729831695557\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.20695003867149353\n",
      "epoch :  19  /  100  | TL :  0.22946956911267927  | VL :  0.21192442066967487  | VA :  93.0\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.09637708216905594\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.281436562538147\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.321521520614624\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.22102399170398712\n",
      "epoch :  20  /  100  | TL :  0.23575785522585485  | VL :  0.2135665100067854  | VA :  92.60000000000001\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.20361152291297913\n",
      "epoch =  20  step =  100  of total steps  201  loss =  0.37990137934684753\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.13407979905605316\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.1882648915052414\n",
      "epoch :  21  /  100  | TL :  0.2319928347434274  | VL :  0.1973338397219777  | VA :  93.60000000000001\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.21543817222118378\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.14826373755931854\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.12414486706256866\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.223720520734787\n",
      "epoch :  22  /  100  | TL :  0.2287390042485586  | VL :  0.21034689154475927  | VA :  92.60000000000001\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.16999390721321106\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.5257681608200073\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.12145557999610901\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.2538580000400543\n",
      "epoch :  23  /  100  | TL :  0.2297842168615232  | VL :  0.21566509967669845  | VA :  92.80000000000001\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.21279509365558624\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.1486455500125885\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.2632969617843628\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.16457611322402954\n",
      "epoch :  24  /  100  | TL :  0.21836890515625773  | VL :  0.2090360908769071  | VA :  93.2\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.11507637053728104\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.21350052952766418\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.19461077451705933\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.0924970805644989\n",
      "epoch :  25  /  100  | TL :  0.23034875096743973  | VL :  0.22059228597208858  | VA :  92.80000000000001\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.2834708094596863\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.3684925138950348\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.25094810128211975\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.03063058853149414\n",
      "epoch :  26  /  100  | TL :  0.23034042375746058  | VL :  0.20344295306131244  | VA :  93.60000000000001\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.21462103724479675\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.2937115430831909\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.23049980401992798\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.1635710895061493\n",
      "epoch :  27  /  100  | TL :  0.22604353143949413  | VL :  0.21102545876055956  | VA :  93.0\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.19217555224895477\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.2887077331542969\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.11672937124967575\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.1992422491312027\n",
      "epoch :  28  /  100  | TL :  0.22197365047252593  | VL :  0.2104673981666565  | VA :  92.80000000000001\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.31323811411857605\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.28009775280952454\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.22451332211494446\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.37403005361557007\n",
      "epoch :  29  /  100  | TL :  0.22964832032868518  | VL :  0.22578905010595918  | VA :  92.80000000000001\n",
      "epoch =  29  step =  50  of total steps  201  loss =  0.11209139972925186\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.31813567876815796\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.2965307831764221\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.27872875332832336\n",
      "epoch :  30  /  100  | TL :  0.22967274550033445  | VL :  0.20579060819000006  | VA :  93.2\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.35499805212020874\n",
      "epoch =  30  step =  100  of total steps  201  loss =  0.0613543838262558\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.43352049589157104\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.39686667919158936\n",
      "epoch :  31  /  100  | TL :  0.225666430347891  | VL :  0.2129749502055347  | VA :  92.60000000000001\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.14546364545822144\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.2166927009820938\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.2029963731765747\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.1869591474533081\n",
      "epoch :  32  /  100  | TL :  0.22809847485070206  | VL :  0.21089945826679468  | VA :  93.0\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.1965678334236145\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.09901664406061172\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.21999090909957886\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.2442198246717453\n",
      "epoch :  33  /  100  | TL :  0.22197863491094527  | VL :  0.23099641920998693  | VA :  92.4\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.3653484284877777\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.11656630039215088\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.34430477023124695\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.3477349281311035\n",
      "epoch :  34  /  100  | TL :  0.21995753474274085  | VL :  0.22129679331555963  | VA :  93.0\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.1861451119184494\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.2736620008945465\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.20288968086242676\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.23123054206371307\n",
      "epoch :  35  /  100  | TL :  0.2158615339677132  | VL :  0.20560796977952123  | VA :  93.8\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.344775915145874\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.35674190521240234\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.10270797461271286\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.1683659553527832\n",
      "epoch :  36  /  100  | TL :  0.23095136784498965  | VL :  0.1976895914413035  | VA :  93.60000000000001\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.26144957542419434\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.17073455452919006\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.24032054841518402\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.256711483001709\n",
      "epoch :  37  /  100  | TL :  0.21890872589020588  | VL :  0.22587262652814388  | VA :  92.0\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.15837545692920685\n",
      "epoch =  37  step =  100  of total steps  201  loss =  0.24520274996757507\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.30873435735702515\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.1842978298664093\n",
      "epoch :  38  /  100  | TL :  0.22020536244137964  | VL :  0.20285974629223347  | VA :  93.60000000000001\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.3237845301628113\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.3796781301498413\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.3336086869239807\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.2904944121837616\n",
      "epoch :  39  /  100  | TL :  0.2288036356999803  | VL :  0.21355656068772078  | VA :  93.4\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.25732648372650146\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.1593049168586731\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.08520694822072983\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.26996099948883057\n",
      "epoch :  40  /  100  | TL :  0.22804281508448113  | VL :  0.20556521648541093  | VA :  93.0\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.22465980052947998\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.12247709184885025\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.2194310575723648\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.15610189735889435\n",
      "epoch :  41  /  100  | TL :  0.22599742822905086  | VL :  0.20361698511987925  | VA :  93.2\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.18349871039390564\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.36401087045669556\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.16551190614700317\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.4256124198436737\n",
      "epoch :  42  /  100  | TL :  0.2139500911481938  | VL :  0.20956390630453825  | VA :  93.0\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.1806209683418274\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.10414179414510727\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.3842158913612366\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.16441592574119568\n",
      "epoch :  43  /  100  | TL :  0.2172781045164042  | VL :  0.20362974982708693  | VA :  93.8\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.2632856070995331\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.35145777463912964\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.0932120680809021\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.1606590896844864\n",
      "epoch :  44  /  100  | TL :  0.21670372085414122  | VL :  0.2213744418695569  | VA :  93.0\n",
      "epoch =  44  step =  50  of total steps  201  loss =  0.3351249098777771\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.3722446858882904\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.13034334778785706\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.15353786945343018\n",
      "epoch :  45  /  100  | TL :  0.2181326214650377  | VL :  0.22048906376585364  | VA :  92.80000000000001\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.3352915048599243\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.3088238835334778\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.1644262671470642\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.10344810038805008\n",
      "epoch :  46  /  100  | TL :  0.21565163083633973  | VL :  0.21943769324570894  | VA :  92.80000000000001\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.10426834970712662\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.22308982908725739\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.26826727390289307\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.12887458503246307\n",
      "epoch :  47  /  100  | TL :  0.2080977896960517  | VL :  0.2306790049187839  | VA :  92.80000000000001\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.3430963158607483\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.29293397068977356\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.1422756165266037\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.20967553555965424\n",
      "epoch :  48  /  100  | TL :  0.21608938198925845  | VL :  0.20691866660490632  | VA :  93.4\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.2575813829898834\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.18993297219276428\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.2413366585969925\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.2662010192871094\n",
      "epoch :  49  /  100  | TL :  0.22168623643060822  | VL :  0.2203666465356946  | VA :  92.60000000000001\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.07282127439975739\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.3335210978984833\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.14121118187904358\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.20040209591388702\n",
      "epoch :  50  /  100  | TL :  0.21073319017887115  | VL :  0.20464605884626508  | VA :  93.2\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.1587088406085968\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 0\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, labels)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, labels)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc = _get_accuracy(data.valid_dl, net)\n",
    "\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss, ' | VA : ', val_acc * 100)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "    experiment.log_metric(\"val_acc\", val_acc)\n",
    "\n",
    "    if (val_acc * 100) > min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_acc * 100\n",
    "        torch.save(net.state_dict(), '../saved_models/classifier/model0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942\n",
      "0.936\n"
     ]
    }
   ],
   "source": [
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/classifier/model0.pt', map_location = 'cpu'))\n",
    "net.cuda()\n",
    "\n",
    "print(_get_accuracy(data.valid_dl, net))\n",
    "print(_get_accuracy(data.valid_dl, learn.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ak_fastai)",
   "language": "python",
   "name": "ak_fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
