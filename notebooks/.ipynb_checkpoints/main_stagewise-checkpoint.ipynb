{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(2)\n",
    "torch.cuda.manual_seed(2)\n",
    "\n",
    "# stage should be in 0 to 5 (5 for classifier stage)\n",
    "hyper_params = {\n",
    "    \"stage\": 4,\n",
    "    \"repeated\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "data = ImageDataBunch.from_folder(path, train = 'train', valid = 'val', bs = hyper_params[\"batch_size\"], size = 224, ds_tfms = tfms).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "True\n",
      "5.0.2.weight torch.Size([512])\n",
      "True\n",
      "5.0.2.bias torch.Size([512])\n",
      "True\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "True\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "True\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "True\n",
      "8.weight torch.Size([256, 1024])\n",
      "False\n",
      "8.bias torch.Size([256])\n",
      "False\n",
      "9.weight torch.Size([10, 256])\n",
      "False\n",
      "9.bias torch.Size([10])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "# learn.summary()\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "if hyper_params['stage'] != 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "    net.load_state_dict(torch.load(filename, map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == str(hyper_params['stage'] + 1) and hyper_params['stage'] != 0 :\n",
    "        param.requires_grad = True\n",
    "    elif name[0] == str(hyper_params['stage']) and hyper_params['stage'] == 0 : \n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "              ReLU-2         [-1, 64, 112, 112]               0\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-9           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "         ResBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12          [-1, 128, 28, 28]          73,728\n",
      "             ReLU-13          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-14          [-1, 128, 28, 28]             256\n",
      "           Conv2d-15          [-1, 128, 28, 28]         147,456\n",
      "             ReLU-16          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
      "         ResBlock-18          [-1, 128, 28, 28]               0\n",
      "           Conv2d-19          [-1, 256, 14, 14]         294,912\n",
      "             ReLU-20          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-21          [-1, 256, 14, 14]             512\n",
      "           Conv2d-22          [-1, 256, 14, 14]         589,824\n",
      "             ReLU-23          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-24          [-1, 256, 14, 14]             512\n",
      "         ResBlock-25          [-1, 256, 14, 14]               0\n",
      "           Conv2d-26            [-1, 512, 7, 7]       1,179,648\n",
      "             ReLU-27            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-28            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-29            [-1, 512, 7, 7]       2,359,296\n",
      "             ReLU-30            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-31            [-1, 512, 7, 7]           1,024\n",
      "         ResBlock-32            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-33            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-34            [-1, 512, 1, 1]               0\n",
      "AdaptiveConcatPool2d-35           [-1, 1024, 1, 1]               0\n",
      "          Flatten-36                 [-1, 1024]               0\n",
      "           Linear-37                  [-1, 256]         262,400\n",
      "           Linear-38                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,996,938\n",
      "Trainable params: 9,536\n",
      "Non-trainable params: 4,987,402\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 40.03\n",
      "Params size (MB): 19.06\n",
      "Estimated Total Size (MB): 59.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x, y = next(iter(data.train_dl))\n",
    "# net(torch.autograd.Variable(x).cuda())\n",
    "summary(net, (3, 224, 224))\n",
    "# print(learn.summary())\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures :\n",
    "    def __init__(self, m) : \n",
    "        self.handle = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, m, inp, outp) : \n",
    "        self.features = outp\n",
    "    def remove(self) :\n",
    "        self.handle.remove()\n",
    "        \n",
    "# saving outputs of all Basic Blocks\n",
    "mdl = learn.model\n",
    "sf = [SaveFeatures(m) for m in [mdl[0][2], mdl[0][4], mdl[0][5], mdl[0][6], mdl[0][7]]]\n",
    "sf2 = [SaveFeatures(m) for m in [net[0], net[2], net[3], net[4], net[5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 112, 112])\n",
      "torch.Size([64, 64, 56, 56])\n",
      "torch.Size([64, 128, 28, 28])\n",
      "torch.Size([64, 256, 14, 14])\n",
      "torch.Size([64, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(data.train_dl))\n",
    "x = torch.autograd.Variable(x).cuda()\n",
    "out1 = mdl(x)\n",
    "out2 = net(x)\n",
    "for i in range(5) : \n",
    "    print(sf[i].features.shape)\n",
    "    assert(sf[i].features.shape == sf2[i].features.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/akshaykvnit/kd0/d5b4b65c9a3c4ba6b252264557d516d2\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [2010]                   : (0.021085122600197792, 0.8147169947624207)\n",
      "COMET INFO:     sys.gpu.0.free_memory [88]    : (5101846528.0, 5139595264.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [88]: (0.0, 99.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory        : (11721506816.0, 11721506816.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [88]    : (6581911552.0, 6619660288.0)\n",
      "COMET INFO:     sys.gpu.1.free_memory [88]    : (1238106112.0, 6221987840.0)\n",
      "COMET INFO:     sys.gpu.1.gpu_utilization [88]: (0.0, 97.0)\n",
      "COMET INFO:     sys.gpu.1.total_memory        : (6233391104.0, 6233391104.0)\n",
      "COMET INFO:     sys.gpu.1.used_memory [88]    : (11403264.0, 4995284992.0)\n",
      "COMET INFO:     train_loss [100]              : (0.02236961764260311, 0.4373542653090918)\n",
      "COMET INFO:     val_loss [100]                : (0.021828623954206705, 0.28351984173059464)\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (2.0.9) detected. current: 2.0.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/3e8621a0783c4bfcbb67c4b4cf60f648\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1  step =  50  of total steps  201  loss =  1.273905873298645\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.8543833494186401\n",
      "epoch =  1  step =  150  of total steps  201  loss =  1.9610918760299683\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.7102090120315552\n",
      "epoch :  1  /  100  | TL :  1.460261870082931  | VL :  1.1243370547890663\n",
      "saving model\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.8931823968887329\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.8252297043800354\n",
      "epoch =  2  step =  150  of total steps  201  loss =  1.7569987773895264\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.9887546300888062\n",
      "epoch :  2  /  100  | TL :  1.0779924440146678  | VL :  0.9869095385074615\n",
      "saving model\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.611712634563446\n",
      "epoch =  3  step =  100  of total steps  201  loss =  1.44060218334198\n",
      "epoch =  3  step =  150  of total steps  201  loss =  1.8025075197219849\n",
      "epoch =  3  step =  200  of total steps  201  loss =  1.0458030700683594\n",
      "epoch :  3  /  100  | TL :  0.9432572858843637  | VL :  0.8165855072438717\n",
      "saving model\n",
      "epoch =  4  step =  50  of total steps  201  loss =  1.0709335803985596\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.7606213688850403\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.7903650999069214\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.5269346833229065\n",
      "epoch :  4  /  100  | TL :  0.9009965468401933  | VL :  0.7783596739172935\n",
      "saving model\n",
      "epoch =  5  step =  50  of total steps  201  loss =  1.3039181232452393\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.4280940294265747\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.5318881869316101\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.5257815718650818\n",
      "epoch :  5  /  100  | TL :  0.8094864604781516  | VL :  0.7364561520516872\n",
      "saving model\n",
      "epoch =  6  step =  50  of total steps  201  loss =  2.100296974182129\n",
      "epoch =  6  step =  100  of total steps  201  loss =  1.6141510009765625\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.4239449203014374\n",
      "epoch =  6  step =  200  of total steps  201  loss =  1.0384490489959717\n",
      "epoch :  6  /  100  | TL :  0.771615018892051  | VL :  0.670272309333086\n",
      "saving model\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.4033263325691223\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.8785440921783447\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.5773699283599854\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.40088126063346863\n",
      "epoch :  7  /  100  | TL :  0.6997126416483922  | VL :  0.6146602109074593\n",
      "saving model\n",
      "epoch =  8  step =  50  of total steps  201  loss =  1.8779785633087158\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.3874003291130066\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.848390519618988\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.6485363841056824\n",
      "epoch :  8  /  100  | TL :  0.6816270923733119  | VL :  0.5936354491859674\n",
      "saving model\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.4221392273902893\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.4105560779571533\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.43161725997924805\n",
      "epoch =  9  step =  200  of total steps  201  loss =  1.2705533504486084\n",
      "epoch :  9  /  100  | TL :  0.6525398780457417  | VL :  0.581926854327321\n",
      "saving model\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.29427480697631836\n",
      "epoch =  10  step =  100  of total steps  201  loss =  1.2952792644500732\n",
      "epoch =  10  step =  150  of total steps  201  loss =  1.0900686979293823\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.48826128244400024\n",
      "epoch :  10  /  100  | TL :  0.6305366624054031  | VL :  0.4934896435588598\n",
      "saving model\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.4500312805175781\n",
      "epoch =  11  step =  100  of total steps  201  loss =  1.1221455335617065\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.498919814825058\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.36322763562202454\n",
      "epoch :  11  /  100  | TL :  0.5985934551972062  | VL :  0.510127192363143\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.2616598308086395\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.38797664642333984\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.32171374559402466\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.5240250825881958\n",
      "epoch :  12  /  100  | TL :  0.592586743520267  | VL :  0.4523712061345577\n",
      "saving model\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.8206365704536438\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.32853978872299194\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.5414849519729614\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.3485201895236969\n",
      "epoch :  13  /  100  | TL :  0.533050712364823  | VL :  0.43940611369907856\n",
      "saving model\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.4187827408313751\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.25954657793045044\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.7732774019241333\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.296394407749176\n",
      "epoch :  14  /  100  | TL :  0.5444998308942093  | VL :  0.47644705325365067\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.3806043267250061\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.8870072364807129\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.5912668704986572\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.22932717204093933\n",
      "epoch :  15  /  100  | TL :  0.5298867929930711  | VL :  0.395844304934144\n",
      "saving model\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.6014696955680847\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.47407788038253784\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.6715388298034668\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.33889415860176086\n",
      "epoch :  16  /  100  | TL :  0.5095088105296615  | VL :  0.3846784410998225\n",
      "saving model\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.3238319456577301\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.4960181415081024\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.936896800994873\n",
      "epoch =  17  step =  200  of total steps  201  loss =  1.4998393058776855\n",
      "epoch :  17  /  100  | TL :  0.4842964243977817  | VL :  0.3848405834287405\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.26415762305259705\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.7549511790275574\n",
      "epoch =  18  step =  150  of total steps  201  loss =  1.0496705770492554\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.19281470775604248\n",
      "epoch :  18  /  100  | TL :  0.5028064101934433  | VL :  0.3874549148604274\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.5426076054573059\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.21028634905815125\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.211297869682312\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.28559866547584534\n",
      "epoch :  19  /  100  | TL :  0.4667181597271962  | VL :  0.37194468080997467\n",
      "saving model\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.2618909180164337\n",
      "epoch =  20  step =  100  of total steps  201  loss =  1.04337477684021\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.4334667921066284\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.23913106322288513\n",
      "epoch :  20  /  100  | TL :  0.4475876155925627  | VL :  0.3763844007626176\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.649196445941925\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.21901315450668335\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.1975056529045105\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.22887136042118073\n",
      "epoch :  21  /  100  | TL :  0.45253225723605844  | VL :  0.3817788576707244\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.799842357635498\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.21176092326641083\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.3904959261417389\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.31786319613456726\n",
      "epoch :  22  /  100  | TL :  0.44278781538579004  | VL :  0.35597958508878946\n",
      "saving model\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.1695030927658081\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.2293725609779358\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.9631608128547668\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.3781968653202057\n",
      "epoch :  23  /  100  | TL :  0.4426744988012077  | VL :  0.3235045848414302\n",
      "saving model\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.4738873839378357\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.1803208291530609\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.20626601576805115\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.26556894183158875\n",
      "epoch :  24  /  100  | TL :  0.44196407630372403  | VL :  0.3142313966527581\n",
      "saving model\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.5385630130767822\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.1865459680557251\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.1575833559036255\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.6709457039833069\n",
      "epoch :  25  /  100  | TL :  0.42853724622904366  | VL :  0.3586214706301689\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.6056458353996277\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.7850422859191895\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.3951433598995209\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.21519352495670319\n",
      "epoch :  26  /  100  | TL :  0.3853900826244212  | VL :  0.287258030846715\n",
      "saving model\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.2911556363105774\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.20873206853866577\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.23970343172550201\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.49221816658973694\n",
      "epoch :  27  /  100  | TL :  0.4132715846175578  | VL :  0.33214706368744373\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.20796054601669312\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.21620796620845795\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.25089535117149353\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.2133672833442688\n",
      "epoch :  28  /  100  | TL :  0.3971877102531604  | VL :  0.31795025151222944\n",
      "epoch =  29  step =  50  of total steps  201  loss =  1.0884391069412231\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.30743247270584106\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.34365227818489075\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.1907598078250885\n",
      "epoch :  29  /  100  | TL :  0.3738938008078295  | VL :  0.3302084803581238\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.17487990856170654\n",
      "epoch =  30  step =  100  of total steps  201  loss =  1.555212140083313\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.753603994846344\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.31519201397895813\n",
      "epoch :  30  /  100  | TL :  0.4023578287801932  | VL :  0.3163811443373561\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.19191540777683258\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.1815880388021469\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.21773606538772583\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.22403249144554138\n",
      "epoch :  31  /  100  | TL :  0.35057140011989063  | VL :  0.31697223614901304\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.18190878629684448\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.9963732361793518\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.22569461166858673\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.1507251113653183\n",
      "epoch :  32  /  100  | TL :  0.3819693702815184  | VL :  0.3115187706425786\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.41653192043304443\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.3640918731689453\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.36486437916755676\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.3077368438243866\n",
      "epoch :  33  /  100  | TL :  0.3652429469485781  | VL :  0.27128284331411123\n",
      "saving model\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.2535001039505005\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.21784217655658722\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.15746726095676422\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.16168375313282013\n",
      "epoch :  34  /  100  | TL :  0.3669183174176003  | VL :  0.26707000518217683\n",
      "saving model\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.16005435585975647\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.30238714814186096\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.34010088443756104\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.26057541370391846\n",
      "epoch :  35  /  100  | TL :  0.3794169814432438  | VL :  0.3053861130028963\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.5515002608299255\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.2103748619556427\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.2172526717185974\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.22783571481704712\n",
      "epoch :  36  /  100  | TL :  0.3879079506987363  | VL :  0.3033919990994036\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.15204878151416779\n",
      "epoch =  37  step =  100  of total steps  201  loss =  1.344142198562622\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.20968589186668396\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.1653449684381485\n",
      "epoch :  37  /  100  | TL :  0.373526034811836  | VL :  0.3163769990205765\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.18273897469043732\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.36882665753364563\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.21030262112617493\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.24062477052211761\n",
      "epoch :  38  /  100  | TL :  0.37091550053055605  | VL :  0.3223495576530695\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.14048424363136292\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.18839754164218903\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.29276758432388306\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.169806107878685\n",
      "epoch :  39  /  100  | TL :  0.3640294079460315  | VL :  0.2588020791299641\n",
      "saving model\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.2429904043674469\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.18454305827617645\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.42479172348976135\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.2779789865016937\n",
      "epoch :  40  /  100  | TL :  0.38104632133571664  | VL :  0.25579153513535857\n",
      "saving model\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.20078055560588837\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.22270835936069489\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.4041571319103241\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.151527538895607\n",
      "epoch :  41  /  100  | TL :  0.3666666124145783  | VL :  0.2604715130291879\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.17508789896965027\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.17920832335948944\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.17568525671958923\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.14330998063087463\n",
      "epoch :  42  /  100  | TL :  0.35086695182679306  | VL :  0.26222449308261275\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.8342626094818115\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.14765024185180664\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.3312012255191803\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.27640843391418457\n",
      "epoch :  43  /  100  | TL :  0.3396143190451522  | VL :  0.26432002941146493\n",
      "epoch =  44  step =  50  of total steps  201  loss =  1.1921671628952026\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.23574887216091156\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.35286641120910645\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.27119341492652893\n",
      "epoch :  44  /  100  | TL :  0.35188591843517264  | VL :  0.23522198107093573\n",
      "saving model\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.5542600154876709\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.3364824056625366\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.33688780665397644\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.22205212712287903\n",
      "epoch :  45  /  100  | TL :  0.3664550842782158  | VL :  0.2579932124353945\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.4324228763580322\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.203750878572464\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.4968254566192627\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.15869607031345367\n",
      "epoch :  46  /  100  | TL :  0.33185048890647606  | VL :  0.25869927695021033\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.554028332233429\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.24977563321590424\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.2913305163383484\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.19290485978126526\n",
      "epoch :  47  /  100  | TL :  0.35409063189776974  | VL :  0.2406193264760077\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.23131245374679565\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.2511237859725952\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.2961461842060089\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.1683422327041626\n",
      "epoch :  48  /  100  | TL :  0.3457893460840728  | VL :  0.2692787856794894\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.3104609549045563\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.275640606880188\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.15839655697345734\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.84759521484375\n",
      "epoch :  49  /  100  | TL :  0.3312603838334036  | VL :  0.27224679477512836\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.293634831905365\n",
      "epoch =  50  step =  100  of total steps  201  loss =  1.1332165002822876\n",
      "epoch =  50  step =  150  of total steps  201  loss =  0.1886720061302185\n",
      "epoch =  50  step =  200  of total steps  201  loss =  0.21695207059383392\n",
      "epoch :  50  /  100  | TL :  0.3474591059322974  | VL :  0.2342588333413005\n",
      "saving model\n",
      "epoch =  51  step =  50  of total steps  201  loss =  0.14630572497844696\n",
      "epoch =  51  step =  100  of total steps  201  loss =  0.14346590638160706\n",
      "epoch =  51  step =  150  of total steps  201  loss =  0.15440478920936584\n",
      "epoch =  51  step =  200  of total steps  201  loss =  0.5590506196022034\n",
      "epoch :  51  /  100  | TL :  0.35116709442577554  | VL :  0.240123909432441\n",
      "epoch =  52  step =  50  of total steps  201  loss =  0.20378796756267548\n",
      "epoch =  52  step =  100  of total steps  201  loss =  0.2592536211013794\n",
      "epoch =  52  step =  150  of total steps  201  loss =  0.8472344279289246\n",
      "epoch =  52  step =  200  of total steps  201  loss =  0.42338642477989197\n",
      "epoch :  52  /  100  | TL :  0.34411800812132914  | VL :  0.2526205498725176\n",
      "epoch =  53  step =  50  of total steps  201  loss =  0.19913645088672638\n",
      "epoch =  53  step =  100  of total steps  201  loss =  0.14322802424430847\n",
      "epoch =  53  step =  150  of total steps  201  loss =  0.7338689565658569\n",
      "epoch =  53  step =  200  of total steps  201  loss =  0.3886088728904724\n",
      "epoch :  53  /  100  | TL :  0.32294572181814346  | VL :  0.26702141808345914\n",
      "epoch =  54  step =  50  of total steps  201  loss =  0.22393646836280823\n",
      "epoch =  54  step =  100  of total steps  201  loss =  0.22544421255588531\n",
      "epoch =  54  step =  150  of total steps  201  loss =  0.19655746221542358\n",
      "epoch =  54  step =  200  of total steps  201  loss =  1.6369030475616455\n",
      "epoch :  54  /  100  | TL :  0.33500533828984447  | VL :  0.23438597563654184\n",
      "epoch =  55  step =  50  of total steps  201  loss =  0.20139293372631073\n",
      "epoch =  55  step =  100  of total steps  201  loss =  0.29811328649520874\n",
      "epoch =  55  step =  150  of total steps  201  loss =  0.32225725054740906\n",
      "epoch =  55  step =  200  of total steps  201  loss =  0.20323877036571503\n",
      "epoch :  55  /  100  | TL :  0.33816029817162463  | VL :  0.23383762780576944\n",
      "saving model\n",
      "epoch =  56  step =  50  of total steps  201  loss =  0.49582305550575256\n",
      "epoch =  56  step =  100  of total steps  201  loss =  0.17866238951683044\n",
      "epoch =  56  step =  150  of total steps  201  loss =  0.24453400075435638\n",
      "epoch =  56  step =  200  of total steps  201  loss =  0.16512952744960785\n",
      "epoch :  56  /  100  | TL :  0.32071802038606717  | VL :  0.2536951843649149\n",
      "epoch =  57  step =  50  of total steps  201  loss =  0.2644751965999603\n",
      "epoch =  57  step =  100  of total steps  201  loss =  0.14611934125423431\n",
      "epoch =  57  step =  150  of total steps  201  loss =  0.20257237553596497\n",
      "epoch =  57  step =  200  of total steps  201  loss =  0.27280014753341675\n",
      "epoch :  57  /  100  | TL :  0.33096419804873156  | VL :  0.24554121494293213\n",
      "epoch =  58  step =  50  of total steps  201  loss =  0.1479516327381134\n",
      "epoch =  58  step =  100  of total steps  201  loss =  0.20351678133010864\n",
      "epoch =  58  step =  150  of total steps  201  loss =  1.1182478666305542\n",
      "epoch =  58  step =  200  of total steps  201  loss =  0.35953372716903687\n",
      "epoch :  58  /  100  | TL :  0.3272969821347526  | VL :  0.24313768092542887\n",
      "epoch =  59  step =  50  of total steps  201  loss =  0.1933315247297287\n",
      "epoch =  59  step =  100  of total steps  201  loss =  0.33537569642066956\n",
      "epoch =  59  step =  150  of total steps  201  loss =  0.18249593675136566\n",
      "epoch =  59  step =  200  of total steps  201  loss =  0.1492420732975006\n",
      "epoch :  59  /  100  | TL :  0.31640807339059773  | VL :  0.23367381747812033\n",
      "saving model\n",
      "epoch =  60  step =  50  of total steps  201  loss =  0.1603693664073944\n",
      "epoch =  60  step =  100  of total steps  201  loss =  0.12470682710409164\n",
      "epoch =  60  step =  150  of total steps  201  loss =  0.16979719698429108\n",
      "epoch =  60  step =  200  of total steps  201  loss =  0.19857876002788544\n",
      "epoch :  60  /  100  | TL :  0.32818217453227116  | VL :  0.25362844206392765\n",
      "epoch =  61  step =  50  of total steps  201  loss =  0.16933679580688477\n",
      "epoch =  61  step =  100  of total steps  201  loss =  0.24177372455596924\n",
      "epoch =  61  step =  150  of total steps  201  loss =  0.24173814058303833\n",
      "epoch =  61  step =  200  of total steps  201  loss =  0.2876597046852112\n",
      "epoch :  61  /  100  | TL :  0.32453000630757106  | VL :  0.22555344039574265\n",
      "saving model\n",
      "epoch =  62  step =  50  of total steps  201  loss =  0.353016197681427\n",
      "epoch =  62  step =  100  of total steps  201  loss =  0.19695277512073517\n",
      "epoch =  62  step =  150  of total steps  201  loss =  0.5441747903823853\n",
      "epoch =  62  step =  200  of total steps  201  loss =  0.6415784955024719\n",
      "epoch :  62  /  100  | TL :  0.3163578874287914  | VL :  0.2551672710105777\n",
      "epoch =  63  step =  50  of total steps  201  loss =  0.5376628041267395\n",
      "epoch =  63  step =  100  of total steps  201  loss =  0.1886863261461258\n",
      "epoch =  63  step =  150  of total steps  201  loss =  0.3289477229118347\n",
      "epoch =  63  step =  200  of total steps  201  loss =  0.1887843757867813\n",
      "epoch :  63  /  100  | TL :  0.30689834829290114  | VL :  0.23421756038442254\n",
      "epoch =  64  step =  50  of total steps  201  loss =  0.25847744941711426\n",
      "epoch =  64  step =  100  of total steps  201  loss =  0.5394814610481262\n",
      "epoch =  64  step =  150  of total steps  201  loss =  0.18940508365631104\n",
      "epoch =  64  step =  200  of total steps  201  loss =  0.5608015060424805\n",
      "epoch :  64  /  100  | TL :  0.3285013893572845  | VL :  0.2386588235385716\n",
      "epoch =  65  step =  50  of total steps  201  loss =  0.24664613604545593\n",
      "epoch =  65  step =  100  of total steps  201  loss =  0.501479983329773\n",
      "epoch =  65  step =  150  of total steps  201  loss =  0.3168862462043762\n",
      "epoch =  65  step =  200  of total steps  201  loss =  0.1703994870185852\n",
      "epoch :  65  /  100  | TL :  0.3421732204322198  | VL :  0.2300665332004428\n",
      "epoch =  66  step =  50  of total steps  201  loss =  0.24505344033241272\n",
      "epoch =  66  step =  100  of total steps  201  loss =  0.6000304222106934\n",
      "epoch =  66  step =  150  of total steps  201  loss =  0.3396349549293518\n",
      "epoch =  66  step =  200  of total steps  201  loss =  0.6962200999259949\n",
      "epoch :  66  /  100  | TL :  0.3148815287508775  | VL :  0.23912197444587946\n",
      "epoch =  67  step =  50  of total steps  201  loss =  0.33691367506980896\n",
      "epoch =  67  step =  100  of total steps  201  loss =  0.39801687002182007\n",
      "epoch =  67  step =  150  of total steps  201  loss =  0.27425822615623474\n",
      "epoch =  67  step =  200  of total steps  201  loss =  0.155582457780838\n",
      "epoch :  67  /  100  | TL :  0.32334450472943227  | VL :  0.234548500739038\n",
      "epoch =  68  step =  50  of total steps  201  loss =  0.190167635679245\n",
      "epoch =  68  step =  100  of total steps  201  loss =  0.18478018045425415\n",
      "epoch =  68  step =  150  of total steps  201  loss =  0.31718048453330994\n",
      "epoch =  68  step =  200  of total steps  201  loss =  0.2865199148654938\n",
      "epoch :  68  /  100  | TL :  0.3270132325923265  | VL :  0.22969383792951703\n",
      "epoch =  69  step =  50  of total steps  201  loss =  0.19023369252681732\n",
      "epoch =  69  step =  100  of total steps  201  loss =  1.0676552057266235\n",
      "epoch =  69  step =  150  of total steps  201  loss =  0.2572728395462036\n",
      "epoch =  69  step =  200  of total steps  201  loss =  0.8964882493019104\n",
      "epoch :  69  /  100  | TL :  0.3312530896408641  | VL :  0.23968875966966152\n",
      "epoch =  70  step =  50  of total steps  201  loss =  0.14645126461982727\n",
      "epoch =  70  step =  100  of total steps  201  loss =  0.18226438760757446\n",
      "epoch =  70  step =  150  of total steps  201  loss =  0.1663394421339035\n",
      "epoch =  70  step =  200  of total steps  201  loss =  0.29995572566986084\n",
      "epoch :  70  /  100  | TL :  0.3274829065176978  | VL :  0.22511491319164634\n",
      "saving model\n",
      "epoch =  71  step =  50  of total steps  201  loss =  0.24306555092334747\n",
      "epoch =  71  step =  100  of total steps  201  loss =  0.17870517075061798\n",
      "epoch =  71  step =  150  of total steps  201  loss =  0.16457267105579376\n",
      "epoch =  71  step =  200  of total steps  201  loss =  0.40110641717910767\n",
      "epoch :  71  /  100  | TL :  0.3263729382584344  | VL :  0.23987749870866537\n",
      "epoch =  72  step =  50  of total steps  201  loss =  0.17320874333381653\n",
      "epoch =  72  step =  100  of total steps  201  loss =  0.20348860323429108\n",
      "epoch =  72  step =  150  of total steps  201  loss =  0.7036870121955872\n",
      "epoch =  72  step =  200  of total steps  201  loss =  0.6554113030433655\n",
      "epoch :  72  /  100  | TL :  0.3393132139868404  | VL :  0.24595042038708925\n",
      "epoch =  73  step =  50  of total steps  201  loss =  0.2464590221643448\n",
      "epoch =  73  step =  100  of total steps  201  loss =  0.2806236147880554\n",
      "epoch =  73  step =  150  of total steps  201  loss =  0.16367192566394806\n",
      "epoch =  73  step =  200  of total steps  201  loss =  0.27815937995910645\n",
      "epoch :  73  /  100  | TL :  0.29724266836002694  | VL :  0.23027447704225779\n",
      "epoch =  74  step =  50  of total steps  201  loss =  0.32494550943374634\n",
      "epoch =  74  step =  100  of total steps  201  loss =  0.14019151031970978\n",
      "epoch =  74  step =  150  of total steps  201  loss =  0.6063637733459473\n",
      "epoch =  74  step =  200  of total steps  201  loss =  0.24009926617145538\n",
      "epoch :  74  /  100  | TL :  0.31878906536606416  | VL :  0.23528428189456463\n",
      "epoch =  75  step =  50  of total steps  201  loss =  0.18385766446590424\n",
      "epoch =  75  step =  100  of total steps  201  loss =  0.18123619258403778\n",
      "epoch =  75  step =  150  of total steps  201  loss =  0.22491376101970673\n",
      "epoch =  75  step =  200  of total steps  201  loss =  0.1795874834060669\n",
      "epoch :  75  /  100  | TL :  0.311023929967216  | VL :  0.22692836076021194\n",
      "epoch =  76  step =  50  of total steps  201  loss =  0.18335911631584167\n",
      "epoch =  76  step =  100  of total steps  201  loss =  0.20551984012126923\n",
      "epoch =  76  step =  150  of total steps  201  loss =  0.5088341236114502\n",
      "epoch =  76  step =  200  of total steps  201  loss =  0.11520571261644363\n",
      "epoch :  76  /  100  | TL :  0.31606154851800766  | VL :  0.2295324793085456\n",
      "epoch =  77  step =  50  of total steps  201  loss =  0.20605862140655518\n",
      "epoch =  77  step =  100  of total steps  201  loss =  0.14058661460876465\n",
      "epoch =  77  step =  150  of total steps  201  loss =  0.2509869337081909\n",
      "epoch =  77  step =  200  of total steps  201  loss =  0.19874514639377594\n",
      "epoch :  77  /  100  | TL :  0.307597608078475  | VL :  0.2383341072127223\n",
      "epoch =  78  step =  50  of total steps  201  loss =  0.21648439764976501\n",
      "epoch =  78  step =  100  of total steps  201  loss =  0.23181460797786713\n",
      "epoch =  78  step =  150  of total steps  201  loss =  0.6550665497779846\n",
      "epoch =  78  step =  200  of total steps  201  loss =  1.092407464981079\n",
      "epoch :  78  /  100  | TL :  0.2972608525583993  | VL :  0.236059439368546\n",
      "epoch =  79  step =  50  of total steps  201  loss =  0.2073356658220291\n",
      "epoch =  79  step =  100  of total steps  201  loss =  0.3114677667617798\n",
      "epoch =  79  step =  150  of total steps  201  loss =  0.6287418007850647\n",
      "epoch =  79  step =  200  of total steps  201  loss =  0.23254071176052094\n",
      "epoch :  79  /  100  | TL :  0.31043979273506656  | VL :  0.2542811478488147\n",
      "epoch =  80  step =  50  of total steps  201  loss =  0.2021595984697342\n",
      "epoch =  80  step =  100  of total steps  201  loss =  0.21034714579582214\n",
      "epoch =  80  step =  150  of total steps  201  loss =  0.2220529019832611\n",
      "epoch =  80  step =  200  of total steps  201  loss =  0.3150079846382141\n",
      "epoch :  80  /  100  | TL :  0.3016640333393913  | VL :  0.2282364722341299\n",
      "epoch =  81  step =  50  of total steps  201  loss =  0.18485814332962036\n",
      "epoch =  81  step =  100  of total steps  201  loss =  0.4080662429332733\n",
      "epoch =  81  step =  150  of total steps  201  loss =  0.20473437011241913\n",
      "epoch =  81  step =  200  of total steps  201  loss =  0.36806347966194153\n",
      "epoch :  81  /  100  | TL :  0.30282718169303674  | VL :  0.2424935195595026\n",
      "epoch =  82  step =  50  of total steps  201  loss =  0.18011264503002167\n",
      "epoch =  82  step =  100  of total steps  201  loss =  0.15462300181388855\n",
      "epoch =  82  step =  150  of total steps  201  loss =  0.19904334843158722\n",
      "epoch =  82  step =  200  of total steps  201  loss =  0.17464320361614227\n",
      "epoch :  82  /  100  | TL :  0.3125668020539023  | VL :  0.228039741050452\n",
      "epoch =  83  step =  50  of total steps  201  loss =  0.42202937602996826\n",
      "epoch =  83  step =  100  of total steps  201  loss =  0.1582627296447754\n",
      "epoch =  83  step =  150  of total steps  201  loss =  0.3075190484523773\n",
      "epoch =  83  step =  200  of total steps  201  loss =  0.21758368611335754\n",
      "epoch :  83  /  100  | TL :  0.31853255344119236  | VL :  0.229285370092839\n",
      "epoch =  84  step =  50  of total steps  201  loss =  0.2112545371055603\n",
      "epoch =  84  step =  100  of total steps  201  loss =  0.5991966128349304\n",
      "epoch =  84  step =  150  of total steps  201  loss =  0.5200459361076355\n",
      "epoch =  84  step =  200  of total steps  201  loss =  0.18191632628440857\n",
      "epoch :  84  /  100  | TL :  0.29327977515423476  | VL :  0.24432192044332623\n",
      "epoch =  85  step =  50  of total steps  201  loss =  0.284702867269516\n",
      "epoch =  85  step =  100  of total steps  201  loss =  0.1189543604850769\n",
      "epoch =  85  step =  150  of total steps  201  loss =  0.2700139284133911\n",
      "epoch =  85  step =  200  of total steps  201  loss =  0.29826539754867554\n",
      "epoch :  85  /  100  | TL :  0.30793841498259883  | VL :  0.2181745539419353\n",
      "saving model\n",
      "epoch =  86  step =  50  of total steps  201  loss =  0.6127117872238159\n",
      "epoch =  86  step =  100  of total steps  201  loss =  0.278363972902298\n",
      "epoch =  86  step =  150  of total steps  201  loss =  0.1527496576309204\n",
      "epoch =  86  step =  200  of total steps  201  loss =  0.13249632716178894\n",
      "epoch :  86  /  100  | TL :  0.29573331980859463  | VL :  0.21850253734737635\n",
      "epoch =  87  step =  50  of total steps  201  loss =  0.2068813592195511\n",
      "epoch =  87  step =  100  of total steps  201  loss =  0.20309793949127197\n",
      "epoch =  87  step =  150  of total steps  201  loss =  0.25325843691825867\n",
      "epoch =  87  step =  200  of total steps  201  loss =  0.21546994149684906\n",
      "epoch :  87  /  100  | TL :  0.2978406936968144  | VL :  0.22527631931006908\n",
      "epoch =  88  step =  50  of total steps  201  loss =  0.3497507870197296\n",
      "epoch =  88  step =  100  of total steps  201  loss =  0.20697835087776184\n",
      "epoch =  88  step =  150  of total steps  201  loss =  0.23812811076641083\n",
      "epoch =  88  step =  200  of total steps  201  loss =  0.2316213697195053\n",
      "epoch :  88  /  100  | TL :  0.2990996762739485  | VL :  0.2264278340153396\n",
      "epoch =  89  step =  50  of total steps  201  loss =  0.19334933161735535\n",
      "epoch =  89  step =  100  of total steps  201  loss =  0.2925891876220703\n",
      "epoch =  89  step =  150  of total steps  201  loss =  0.36038970947265625\n",
      "epoch =  89  step =  200  of total steps  201  loss =  0.13217340409755707\n",
      "epoch :  89  /  100  | TL :  0.3102004316019182  | VL :  0.21996364183723927\n",
      "epoch =  90  step =  50  of total steps  201  loss =  1.362919569015503\n",
      "epoch =  90  step =  100  of total steps  201  loss =  0.29517319798469543\n",
      "epoch =  90  step =  150  of total steps  201  loss =  0.33574551343917847\n",
      "epoch =  90  step =  200  of total steps  201  loss =  0.29711467027664185\n",
      "epoch :  90  /  100  | TL :  0.3223458532966785  | VL :  0.23646545642986894\n",
      "epoch =  91  step =  50  of total steps  201  loss =  0.20453856885433197\n",
      "epoch =  91  step =  100  of total steps  201  loss =  0.23394078016281128\n",
      "epoch =  91  step =  150  of total steps  201  loss =  0.36868059635162354\n",
      "epoch =  91  step =  200  of total steps  201  loss =  0.2632615268230438\n",
      "epoch :  91  /  100  | TL :  0.317760366334844  | VL :  0.216023622546345\n",
      "saving model\n",
      "epoch =  92  step =  50  of total steps  201  loss =  0.18824920058250427\n",
      "epoch =  92  step =  100  of total steps  201  loss =  0.2129351943731308\n",
      "epoch =  92  step =  150  of total steps  201  loss =  0.3171725869178772\n",
      "epoch =  92  step =  200  of total steps  201  loss =  0.3439508378505707\n",
      "epoch :  92  /  100  | TL :  0.3028712416836871  | VL :  0.23336317762732506\n",
      "epoch =  93  step =  50  of total steps  201  loss =  0.6303055286407471\n",
      "epoch =  93  step =  100  of total steps  201  loss =  1.8286683559417725\n",
      "epoch =  93  step =  150  of total steps  201  loss =  0.22214655578136444\n",
      "epoch =  93  step =  200  of total steps  201  loss =  0.22748517990112305\n",
      "epoch :  93  /  100  | TL :  0.29612592237060936  | VL :  0.21799467084929347\n",
      "epoch =  94  step =  50  of total steps  201  loss =  0.37452220916748047\n",
      "epoch =  94  step =  100  of total steps  201  loss =  0.35241660475730896\n",
      "epoch =  94  step =  150  of total steps  201  loss =  0.29772064089775085\n",
      "epoch =  94  step =  200  of total steps  201  loss =  0.22948668897151947\n",
      "epoch :  94  /  100  | TL :  0.30074431406176505  | VL :  0.22906579915434122\n",
      "epoch =  95  step =  50  of total steps  201  loss =  0.15207456052303314\n",
      "epoch =  95  step =  100  of total steps  201  loss =  0.14256656169891357\n",
      "epoch =  95  step =  150  of total steps  201  loss =  0.25758036971092224\n",
      "epoch =  95  step =  200  of total steps  201  loss =  0.5116652250289917\n",
      "epoch :  95  /  100  | TL :  0.30232614842220323  | VL :  0.22160859173163772\n",
      "epoch =  96  step =  50  of total steps  201  loss =  0.171927347779274\n",
      "epoch =  96  step =  100  of total steps  201  loss =  0.35937264561653137\n",
      "epoch =  96  step =  150  of total steps  201  loss =  0.1204661875963211\n",
      "epoch =  96  step =  200  of total steps  201  loss =  0.1957198530435562\n",
      "epoch :  96  /  100  | TL :  0.2963289697816716  | VL :  0.22126971185207367\n",
      "epoch =  97  step =  50  of total steps  201  loss =  1.1092381477355957\n",
      "epoch =  97  step =  100  of total steps  201  loss =  0.2408420741558075\n",
      "epoch =  97  step =  150  of total steps  201  loss =  0.24326711893081665\n",
      "epoch =  97  step =  200  of total steps  201  loss =  0.19018729031085968\n",
      "epoch :  97  /  100  | TL :  0.29424630657802175  | VL :  0.2257811389863491\n",
      "epoch =  98  step =  50  of total steps  201  loss =  0.9095640182495117\n",
      "epoch =  98  step =  100  of total steps  201  loss =  0.23906771838665009\n",
      "epoch =  98  step =  150  of total steps  201  loss =  0.4966137111186981\n",
      "epoch =  98  step =  200  of total steps  201  loss =  0.18544450402259827\n",
      "epoch :  98  /  100  | TL :  0.30748737481103017  | VL :  0.23608849570155144\n",
      "epoch =  99  step =  50  of total steps  201  loss =  0.39526817202568054\n",
      "epoch =  99  step =  100  of total steps  201  loss =  0.30132460594177246\n",
      "epoch =  99  step =  150  of total steps  201  loss =  0.26162636280059814\n",
      "epoch =  99  step =  200  of total steps  201  loss =  0.22055239975452423\n",
      "epoch :  99  /  100  | TL :  0.2917176301206522  | VL :  0.21846559969708323\n",
      "epoch =  100  step =  50  of total steps  201  loss =  0.5993046760559082\n",
      "epoch =  100  step =  100  of total steps  201  loss =  0.14925126731395721\n",
      "epoch =  100  step =  150  of total steps  201  loss =  0.44312769174575806\n",
      "epoch =  100  step =  200  of total steps  201  loss =  0.4762505292892456\n",
      "epoch :  100  /  100  | TL :  0.2981526777889598  | VL :  0.22848335187882185\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "if hyper_params['stage'] == 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "else : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage'] + 1) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "        y_pred2 = mdl(images)\n",
    "\n",
    "        loss = F.mse_loss(sf2[hyper_params[\"stage\"]].features, sf[hyper_params[\"stage\"]].features)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch + 1, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            y_pred2 = mdl(images)\n",
    "            loss = F.mse_loss(sf[hyper_params[\"stage\"]].features, sf2[hyper_params[\"stage\"]].features)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "\n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = Learner(data, net, metrics = accuracy)\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model0.pt', map_location = 'cpu'))\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(hyper_params[\"num_epochs\"]), train_loss_list, label = 'train_loss') \n",
    "plt.plot(range(hyper_params[\"num_epochs\"]), val_loss_list, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/stage5/train_loss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier only after stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "False\n",
      "5.0.2.weight torch.Size([512])\n",
      "False\n",
      "5.0.2.bias torch.Size([512])\n",
      "False\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "False\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "False\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "False\n",
      "8.weight torch.Size([256, 1024])\n",
      "True\n",
      "8.bias torch.Size([256])\n",
      "True\n",
      "9.weight torch.Size([10, 256])\n",
      "True\n",
      "9.bias torch.Size([10])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    \"stage\": 5,\n",
    "    \"repeated\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model2.pt', map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == '8' or name[0] == '9':\n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = torch.autograd.Variable(images).float()\n",
    "        labels = torch.autograd.Variable(labels).float()\n",
    "        \n",
    "        if torch.cuda.is_available() : \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = Net.forward(images)\n",
    "        outputs = F.log_softmax(outputs, dim = 1)\n",
    "\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        pred_ind = pred_ind.data.cpu().numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = labels - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (2.0.9) detected. current: 2.0.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/af89f6de47194a17b5f0dfdf05468871\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  50  of total steps  201  loss =  0.7408864498138428\n",
      "epoch =  0  step =  100  of total steps  201  loss =  0.5132940411567688\n",
      "epoch =  0  step =  150  of total steps  201  loss =  0.351601779460907\n",
      "epoch =  0  step =  200  of total steps  201  loss =  0.19308796525001526\n",
      "epoch :  1  /  100  | TL :  0.6714505333805558  | VL :  0.27163530327379704  | VA :  92.2\n",
      "saving model\n",
      "epoch =  1  step =  50  of total steps  201  loss =  0.4602231979370117\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.39048829674720764\n",
      "epoch =  1  step =  150  of total steps  201  loss =  0.3176841139793396\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.3912574350833893\n",
      "epoch :  2  /  100  | TL :  0.32952072111824854  | VL :  0.2342266859486699  | VA :  91.60000000000001\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.37428712844848633\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.35687628388404846\n",
      "epoch =  2  step =  150  of total steps  201  loss =  0.22912931442260742\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.3629746437072754\n",
      "epoch :  3  /  100  | TL :  0.29864228587245467  | VL :  0.23301326110959053  | VA :  92.4\n",
      "saving model\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.36090537905693054\n",
      "epoch =  3  step =  100  of total steps  201  loss =  0.18136218190193176\n",
      "epoch =  3  step =  150  of total steps  201  loss =  0.2268356829881668\n",
      "epoch =  3  step =  200  of total steps  201  loss =  0.21864399313926697\n",
      "epoch :  4  /  100  | TL :  0.2695888211552183  | VL :  0.22139371978119016  | VA :  92.4\n",
      "epoch =  4  step =  50  of total steps  201  loss =  0.33223265409469604\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.39998412132263184\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.23379865288734436\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.3479842245578766\n",
      "epoch :  5  /  100  | TL :  0.2761070822824293  | VL :  0.21114017721265554  | VA :  92.2\n",
      "epoch =  5  step =  50  of total steps  201  loss =  0.19364100694656372\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.31116729974746704\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.3413110673427582\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.2717195451259613\n",
      "epoch :  6  /  100  | TL :  0.260031623700958  | VL :  0.21819885913282633  | VA :  92.2\n",
      "epoch =  6  step =  50  of total steps  201  loss =  0.30858951807022095\n",
      "epoch =  6  step =  100  of total steps  201  loss =  0.32702207565307617\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.2819536328315735\n",
      "epoch =  6  step =  200  of total steps  201  loss =  0.20822030305862427\n",
      "epoch :  7  /  100  | TL :  0.26469009766234686  | VL :  0.2265666900202632  | VA :  92.0\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.16001349687576294\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.1659914255142212\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.30884236097335815\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.24663642048835754\n",
      "epoch :  8  /  100  | TL :  0.24764147469431014  | VL :  0.21137145068496466  | VA :  92.60000000000001\n",
      "saving model\n",
      "epoch =  8  step =  50  of total steps  201  loss =  0.1836736500263214\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.22347605228424072\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.23337018489837646\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.27879321575164795\n",
      "epoch :  9  /  100  | TL :  0.2501028362494796  | VL :  0.2240984053350985  | VA :  92.0\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.2285662591457367\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.10306183993816376\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.0868687629699707\n",
      "epoch =  9  step =  200  of total steps  201  loss =  0.20526811480522156\n",
      "epoch :  10  /  100  | TL :  0.25331234813329595  | VL :  0.21919936127960682  | VA :  92.0\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.21253246068954468\n",
      "epoch =  10  step =  100  of total steps  201  loss =  0.3310966491699219\n",
      "epoch =  10  step =  150  of total steps  201  loss =  0.20771533250808716\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.21741469204425812\n",
      "epoch :  11  /  100  | TL :  0.2545538525009037  | VL :  0.21117641823366284  | VA :  92.2\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.3275660276412964\n",
      "epoch =  11  step =  100  of total steps  201  loss =  0.21428591012954712\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.3626702129840851\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.2068428248167038\n",
      "epoch :  12  /  100  | TL :  0.24866677820682526  | VL :  0.21403455594554543  | VA :  92.4\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.30675482749938965\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.17101030051708221\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.060577698051929474\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.15049180388450623\n",
      "epoch :  13  /  100  | TL :  0.24414458907955322  | VL :  0.22185493353754282  | VA :  92.4\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.15956881642341614\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.262673020362854\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.15445274114608765\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.28934651613235474\n",
      "epoch :  14  /  100  | TL :  0.24235480161967562  | VL :  0.2245291881263256  | VA :  92.4\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.2759888172149658\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.13369044661521912\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.2822178304195404\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.3360580801963806\n",
      "epoch :  15  /  100  | TL :  0.24450586408154287  | VL :  0.23007707996293902  | VA :  92.2\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.48205626010894775\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.28692930936813354\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.21967008709907532\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.27265095710754395\n",
      "epoch :  16  /  100  | TL :  0.2362222035614709  | VL :  0.20833501359447837  | VA :  93.4\n",
      "saving model\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.339569091796875\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.16147571802139282\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.20190508663654327\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.3787437677383423\n",
      "epoch :  17  /  100  | TL :  0.2271601983637952  | VL :  0.21885762084275484  | VA :  92.4\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.32858800888061523\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.5286391973495483\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.26348716020584106\n",
      "epoch =  17  step =  200  of total steps  201  loss =  0.18255239725112915\n",
      "epoch :  18  /  100  | TL :  0.23583893589119412  | VL :  0.22264914400875568  | VA :  92.60000000000001\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.21516185998916626\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.37101566791534424\n",
      "epoch =  18  step =  150  of total steps  201  loss =  0.1493423879146576\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.09781249612569809\n",
      "epoch :  19  /  100  | TL :  0.2332508404790169  | VL :  0.2385348677635193  | VA :  92.0\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.2228868305683136\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.2491724044084549\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.26403868198394775\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.31710872054100037\n",
      "epoch :  20  /  100  | TL :  0.2384604972170953  | VL :  0.22248131595551968  | VA :  92.60000000000001\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.20500223338603973\n",
      "epoch =  20  step =  100  of total steps  201  loss =  0.13705021142959595\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.22059547901153564\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.29056304693222046\n",
      "epoch :  21  /  100  | TL :  0.233449700293108  | VL :  0.2089843018911779  | VA :  92.80000000000001\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.24888509511947632\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.1881161779165268\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.22920477390289307\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.5292215943336487\n",
      "epoch :  22  /  100  | TL :  0.23267091590151265  | VL :  0.2127046398818493  | VA :  92.80000000000001\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.28932374715805054\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.12504081428050995\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.2001253366470337\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.1799333691596985\n",
      "epoch :  23  /  100  | TL :  0.23040149446150557  | VL :  0.20718318317085505  | VA :  93.60000000000001\n",
      "saving model\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.10558164864778519\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.32191044092178345\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.22493121027946472\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.17309486865997314\n",
      "epoch :  24  /  100  | TL :  0.23063936266139964  | VL :  0.22344593051820993  | VA :  92.4\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.24035264551639557\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.15998177230358124\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.169114351272583\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.18062151968479156\n",
      "epoch :  25  /  100  | TL :  0.23000088854882847  | VL :  0.21288898400962353  | VA :  92.60000000000001\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.18001078069210052\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.3831630349159241\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.12488008290529251\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.13715454936027527\n",
      "epoch :  26  /  100  | TL :  0.23611975261079732  | VL :  0.22860707994550467  | VA :  92.60000000000001\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.13671569526195526\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.15766943991184235\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.2611788511276245\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.14701099693775177\n",
      "epoch :  27  /  100  | TL :  0.23059006858227857  | VL :  0.2050548647530377  | VA :  92.80000000000001\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.3072746694087982\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.2986478805541992\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.20293046534061432\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.15807193517684937\n",
      "epoch :  28  /  100  | TL :  0.23086961149353885  | VL :  0.21681979252025485  | VA :  92.80000000000001\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.2551749348640442\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.0760655626654625\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.3290224075317383\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.19828417897224426\n",
      "epoch :  29  /  100  | TL :  0.22867654124969866  | VL :  0.2172375163063407  | VA :  92.60000000000001\n",
      "epoch =  29  step =  50  of total steps  201  loss =  0.2831938862800598\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.1380540430545807\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.37056002020835876\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.24212154746055603\n",
      "epoch :  30  /  100  | TL :  0.2327501147763053  | VL :  0.21576699521392584  | VA :  92.60000000000001\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.14677712321281433\n",
      "epoch =  30  step =  100  of total steps  201  loss =  0.15723292529582977\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.3104439377784729\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.3404615819454193\n",
      "epoch :  31  /  100  | TL :  0.22519151951691405  | VL :  0.2184156123548746  | VA :  93.4\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.326709121465683\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.12761569023132324\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.2559663653373718\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.1253221482038498\n",
      "epoch :  32  /  100  | TL :  0.2219603784493546  | VL :  0.21655200514942408  | VA :  92.60000000000001\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.2833608388900757\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.11192525923252106\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.22640904784202576\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.14235997200012207\n",
      "epoch :  33  /  100  | TL :  0.22491007883898653  | VL :  0.22083161305636168  | VA :  92.2\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.27729204297065735\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.25578075647354126\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.24622441828250885\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.13103191554546356\n",
      "epoch :  34  /  100  | TL :  0.22403144332306896  | VL :  0.21461752150207758  | VA :  92.80000000000001\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.44239193201065063\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.11144425719976425\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.29690778255462646\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.41807788610458374\n",
      "epoch :  35  /  100  | TL :  0.22710113353397124  | VL :  0.20344212092459202  | VA :  93.2\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.18230047821998596\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.36336636543273926\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.25608259439468384\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.26827725768089294\n",
      "epoch :  36  /  100  | TL :  0.22448914082934016  | VL :  0.23261360405012965  | VA :  91.8\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.25550809502601624\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.31065037846565247\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.41302022337913513\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.2582031190395355\n",
      "epoch :  37  /  100  | TL :  0.22142068346712127  | VL :  0.21339206444099545  | VA :  92.80000000000001\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.24689564108848572\n",
      "epoch =  37  step =  100  of total steps  201  loss =  0.16751287877559662\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.13460023701190948\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.15758885443210602\n",
      "epoch :  38  /  100  | TL :  0.22299771600249987  | VL :  0.20606548245996237  | VA :  93.8\n",
      "saving model\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.07918863743543625\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.12333933264017105\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.16063271462917328\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.18019823729991913\n",
      "epoch :  39  /  100  | TL :  0.22222131876210074  | VL :  0.21895143296569586  | VA :  93.0\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.22325941920280457\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.17303091287612915\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.311824232339859\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.10941948741674423\n",
      "epoch :  40  /  100  | TL :  0.22643702784877512  | VL :  0.22380350716412067  | VA :  93.2\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.16146621108055115\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.20183512568473816\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.13148853182792664\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.14522163569927216\n",
      "epoch :  41  /  100  | TL :  0.22063979313741275  | VL :  0.20487325126305223  | VA :  93.0\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.16722214221954346\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.14134256541728973\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.24291867017745972\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.25900334119796753\n",
      "epoch :  42  /  100  | TL :  0.21856259769616435  | VL :  0.2073471238836646  | VA :  93.4\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.1736549437046051\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.08446693420410156\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.17053963243961334\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.20594987273216248\n",
      "epoch :  43  /  100  | TL :  0.22205601769745054  | VL :  0.21610149089246988  | VA :  93.0\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.18485644459724426\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.30392566323280334\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.29726189374923706\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.3540552854537964\n",
      "epoch :  44  /  100  | TL :  0.21532692960393962  | VL :  0.21468963148072362  | VA :  93.2\n",
      "epoch =  44  step =  50  of total steps  201  loss =  0.31632286310195923\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.11005589365959167\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.2961762249469757\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.29292380809783936\n",
      "epoch :  45  /  100  | TL :  0.22815896244487952  | VL :  0.20999452844262123  | VA :  93.4\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.13197365403175354\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.47970834374427795\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.23349091410636902\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.3855178654193878\n",
      "epoch :  46  /  100  | TL :  0.2295654841249262  | VL :  0.22801045747473836  | VA :  92.80000000000001\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.2380802035331726\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.21349821984767914\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.1505163311958313\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.4549354016780853\n",
      "epoch :  47  /  100  | TL :  0.22630215567810025  | VL :  0.2344201933592558  | VA :  93.0\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.21332834661006927\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.43340468406677246\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.2767162322998047\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.2360355257987976\n",
      "epoch :  48  /  100  | TL :  0.2237546488790963  | VL :  0.22727365419268608  | VA :  92.2\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.20010995864868164\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.19155243039131165\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.07088829576969147\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.341708779335022\n",
      "epoch :  49  /  100  | TL :  0.21750526267349424  | VL :  0.2478177836164832  | VA :  92.60000000000001\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.057865239679813385\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.17845740914344788\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.22639337182044983\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.15931159257888794\n",
      "epoch :  50  /  100  | TL :  0.21903447984759486  | VL :  0.2096480899490416  | VA :  93.2\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.2600221633911133\n",
      "epoch =  50  step =  100  of total steps  201  loss =  0.2329310178756714\n",
      "epoch =  50  step =  150  of total steps  201  loss =  0.12735575437545776\n",
      "epoch =  50  step =  200  of total steps  201  loss =  0.35113537311553955\n",
      "epoch :  51  /  100  | TL :  0.21196250081877804  | VL :  0.21579505782574415  | VA :  92.80000000000001\n",
      "epoch =  51  step =  50  of total steps  201  loss =  0.23542292416095734\n",
      "epoch =  51  step =  100  of total steps  201  loss =  0.28040826320648193\n",
      "epoch =  51  step =  150  of total steps  201  loss =  0.3495481014251709\n",
      "epoch =  51  step =  200  of total steps  201  loss =  0.21599259972572327\n",
      "epoch :  52  /  100  | TL :  0.21570172841053697  | VL :  0.2188274529762566  | VA :  92.4\n",
      "epoch =  52  step =  50  of total steps  201  loss =  0.11748022586107254\n",
      "epoch =  52  step =  100  of total steps  201  loss =  0.15999415516853333\n",
      "epoch =  52  step =  150  of total steps  201  loss =  0.2052406519651413\n",
      "epoch =  52  step =  200  of total steps  201  loss =  0.09662473946809769\n",
      "epoch :  53  /  100  | TL :  0.21284125063253279  | VL :  0.2222574343904853  | VA :  93.0\n",
      "epoch =  53  step =  50  of total steps  201  loss =  0.17673121392726898\n",
      "epoch =  53  step =  100  of total steps  201  loss =  0.04911589249968529\n",
      "epoch =  53  step =  150  of total steps  201  loss =  0.16568294167518616\n",
      "epoch =  53  step =  200  of total steps  201  loss =  0.2155735045671463\n",
      "epoch :  54  /  100  | TL :  0.21410036989631345  | VL :  0.2166351107880473  | VA :  92.4\n",
      "epoch =  54  step =  50  of total steps  201  loss =  0.19903182983398438\n",
      "epoch =  54  step =  100  of total steps  201  loss =  0.20600199699401855\n",
      "epoch =  54  step =  150  of total steps  201  loss =  0.15550214052200317\n",
      "epoch =  54  step =  200  of total steps  201  loss =  0.22733238339424133\n",
      "epoch :  55  /  100  | TL :  0.2138394539369576  | VL :  0.22163846995681524  | VA :  92.60000000000001\n",
      "epoch =  55  step =  50  of total steps  201  loss =  0.30509451031684875\n",
      "epoch =  55  step =  100  of total steps  201  loss =  0.25233784317970276\n",
      "epoch =  55  step =  150  of total steps  201  loss =  0.3079931139945984\n",
      "epoch =  55  step =  200  of total steps  201  loss =  0.34136873483657837\n",
      "epoch :  56  /  100  | TL :  0.21910494849530618  | VL :  0.23464615922421217  | VA :  92.0\n",
      "epoch =  56  step =  50  of total steps  201  loss =  0.08887510746717453\n",
      "epoch =  56  step =  100  of total steps  201  loss =  0.12233848869800568\n",
      "epoch =  56  step =  150  of total steps  201  loss =  0.1368435025215149\n",
      "epoch =  56  step =  200  of total steps  201  loss =  0.2521130442619324\n",
      "epoch :  57  /  100  | TL :  0.20913741463304159  | VL :  0.20680765807628632  | VA :  93.2\n",
      "epoch =  57  step =  50  of total steps  201  loss =  0.15628059208393097\n",
      "epoch =  57  step =  100  of total steps  201  loss =  0.3746146559715271\n",
      "epoch =  57  step =  150  of total steps  201  loss =  0.1921289712190628\n",
      "epoch =  57  step =  200  of total steps  201  loss =  0.16071461141109467\n",
      "epoch :  58  /  100  | TL :  0.21659665565882155  | VL :  0.21585349552333355  | VA :  93.0\n",
      "epoch =  58  step =  50  of total steps  201  loss =  0.21971318125724792\n",
      "epoch =  58  step =  100  of total steps  201  loss =  0.1850091516971588\n",
      "epoch =  58  step =  150  of total steps  201  loss =  0.2039916068315506\n",
      "epoch =  58  step =  200  of total steps  201  loss =  0.15670354664325714\n",
      "epoch :  59  /  100  | TL :  0.2180135145561019  | VL :  0.22126871347427368  | VA :  93.0\n",
      "epoch =  59  step =  50  of total steps  201  loss =  0.24184083938598633\n",
      "epoch =  59  step =  100  of total steps  201  loss =  0.13166475296020508\n",
      "epoch =  59  step =  150  of total steps  201  loss =  0.09908207505941391\n",
      "epoch =  59  step =  200  of total steps  201  loss =  0.1432340145111084\n",
      "epoch :  60  /  100  | TL :  0.2135644897781498  | VL :  0.2023649667389691  | VA :  93.8\n",
      "epoch =  60  step =  50  of total steps  201  loss =  0.21672320365905762\n",
      "epoch =  60  step =  100  of total steps  201  loss =  0.4361885190010071\n",
      "epoch =  60  step =  150  of total steps  201  loss =  0.14405381679534912\n",
      "epoch =  60  step =  200  of total steps  201  loss =  0.12985333800315857\n",
      "epoch :  61  /  100  | TL :  0.21295150032090904  | VL :  0.22283355938270688  | VA :  93.0\n",
      "epoch =  61  step =  50  of total steps  201  loss =  0.30116507411003113\n",
      "epoch =  61  step =  100  of total steps  201  loss =  0.21089695394039154\n",
      "epoch =  61  step =  150  of total steps  201  loss =  0.09590837359428406\n",
      "epoch =  61  step =  200  of total steps  201  loss =  0.3398354947566986\n",
      "epoch :  62  /  100  | TL :  0.21468175893918198  | VL :  0.217323943041265  | VA :  93.2\n",
      "epoch =  62  step =  50  of total steps  201  loss =  0.12823927402496338\n",
      "epoch =  62  step =  100  of total steps  201  loss =  0.29121583700180054\n",
      "epoch =  62  step =  150  of total steps  201  loss =  0.2849883437156677\n",
      "epoch =  62  step =  200  of total steps  201  loss =  0.07477464526891708\n",
      "epoch :  63  /  100  | TL :  0.21279777656888488  | VL :  0.21872594999149442  | VA :  92.60000000000001\n",
      "epoch =  63  step =  50  of total steps  201  loss =  0.08516825735569\n",
      "epoch =  63  step =  100  of total steps  201  loss =  0.19171486794948578\n",
      "epoch =  63  step =  150  of total steps  201  loss =  0.3099043667316437\n",
      "epoch =  63  step =  200  of total steps  201  loss =  0.09056325256824493\n",
      "epoch :  64  /  100  | TL :  0.20373933955063275  | VL :  0.211288470774889  | VA :  93.4\n",
      "epoch =  64  step =  50  of total steps  201  loss =  0.0925993025302887\n",
      "epoch =  64  step =  100  of total steps  201  loss =  0.20410172641277313\n",
      "epoch =  64  step =  150  of total steps  201  loss =  0.33708763122558594\n",
      "epoch =  64  step =  200  of total steps  201  loss =  0.26652204990386963\n",
      "epoch :  65  /  100  | TL :  0.20958793900944106  | VL :  0.2088394770398736  | VA :  92.60000000000001\n",
      "epoch =  65  step =  50  of total steps  201  loss =  0.1271839737892151\n",
      "epoch =  65  step =  100  of total steps  201  loss =  0.20924293994903564\n",
      "epoch =  65  step =  150  of total steps  201  loss =  0.2981450855731964\n",
      "epoch =  65  step =  200  of total steps  201  loss =  0.26962918043136597\n",
      "epoch :  66  /  100  | TL :  0.21658367737757034  | VL :  0.22462734812870622  | VA :  92.2\n",
      "epoch =  66  step =  50  of total steps  201  loss =  0.29612961411476135\n",
      "epoch =  66  step =  100  of total steps  201  loss =  0.2231341451406479\n",
      "epoch =  66  step =  150  of total steps  201  loss =  0.17259982228279114\n",
      "epoch =  66  step =  200  of total steps  201  loss =  0.1960023045539856\n",
      "epoch :  67  /  100  | TL :  0.21290186303320216  | VL :  0.20976783521473408  | VA :  93.2\n",
      "epoch =  67  step =  50  of total steps  201  loss =  0.2632938325405121\n",
      "epoch =  67  step =  100  of total steps  201  loss =  0.08520326018333435\n",
      "epoch =  67  step =  150  of total steps  201  loss =  0.3144782781600952\n",
      "epoch =  67  step =  200  of total steps  201  loss =  0.16364531219005585\n",
      "epoch :  68  /  100  | TL :  0.21049685273968166  | VL :  0.2181786224246025  | VA :  93.8\n",
      "epoch =  68  step =  50  of total steps  201  loss =  0.23583394289016724\n",
      "epoch =  68  step =  100  of total steps  201  loss =  0.11100178956985474\n",
      "epoch =  68  step =  150  of total steps  201  loss =  0.15890252590179443\n",
      "epoch =  68  step =  200  of total steps  201  loss =  0.2063591033220291\n",
      "epoch :  69  /  100  | TL :  0.21529127247816887  | VL :  0.23344858409836888  | VA :  92.80000000000001\n",
      "epoch =  69  step =  50  of total steps  201  loss =  0.23477736115455627\n",
      "epoch =  69  step =  100  of total steps  201  loss =  0.23759907484054565\n",
      "epoch =  69  step =  150  of total steps  201  loss =  0.3032304644584656\n",
      "epoch =  69  step =  200  of total steps  201  loss =  0.152845561504364\n",
      "epoch :  70  /  100  | TL :  0.21153147837415856  | VL :  0.216890849173069  | VA :  93.2\n",
      "epoch =  70  step =  50  of total steps  201  loss =  0.25573986768722534\n",
      "epoch =  70  step =  100  of total steps  201  loss =  0.20710201561450958\n",
      "epoch =  70  step =  150  of total steps  201  loss =  0.27721545100212097\n",
      "epoch =  70  step =  200  of total steps  201  loss =  0.10927440971136093\n",
      "epoch :  71  /  100  | TL :  0.21706169154216995  | VL :  0.2254275530576706  | VA :  93.0\n",
      "epoch =  71  step =  50  of total steps  201  loss =  0.2658355236053467\n",
      "epoch =  71  step =  100  of total steps  201  loss =  0.11199010163545609\n",
      "epoch =  71  step =  150  of total steps  201  loss =  0.581838071346283\n",
      "epoch =  71  step =  200  of total steps  201  loss =  0.09946152567863464\n",
      "epoch :  72  /  100  | TL :  0.21001520834455442  | VL :  0.235808698926121  | VA :  91.60000000000001\n",
      "epoch =  72  step =  50  of total steps  201  loss =  0.24384799599647522\n",
      "epoch =  72  step =  100  of total steps  201  loss =  0.07644940167665482\n",
      "epoch =  72  step =  150  of total steps  201  loss =  0.32008183002471924\n",
      "epoch =  72  step =  200  of total steps  201  loss =  0.08495869487524033\n",
      "epoch :  73  /  100  | TL :  0.21533918115704215  | VL :  0.22301424155011773  | VA :  93.0\n",
      "epoch =  73  step =  50  of total steps  201  loss =  0.11973357945680618\n",
      "epoch =  73  step =  100  of total steps  201  loss =  0.16077575087547302\n",
      "epoch =  73  step =  150  of total steps  201  loss =  0.3094155192375183\n",
      "epoch =  73  step =  200  of total steps  201  loss =  0.3146418631076813\n",
      "epoch :  74  /  100  | TL :  0.20648528772651853  | VL :  0.22553996788337827  | VA :  92.4\n",
      "epoch =  74  step =  50  of total steps  201  loss =  0.0931418165564537\n",
      "epoch =  74  step =  100  of total steps  201  loss =  0.10235264152288437\n",
      "epoch =  74  step =  150  of total steps  201  loss =  0.15552984178066254\n",
      "epoch =  74  step =  200  of total steps  201  loss =  0.10757625102996826\n",
      "epoch :  75  /  100  | TL :  0.20907868052003395  | VL :  0.21063534310087562  | VA :  93.8\n",
      "epoch =  75  step =  50  of total steps  201  loss =  0.18161813914775848\n",
      "epoch =  75  step =  100  of total steps  201  loss =  0.40621671080589294\n",
      "epoch =  75  step =  150  of total steps  201  loss =  0.08995890617370605\n",
      "epoch =  75  step =  200  of total steps  201  loss =  0.1833151876926422\n",
      "epoch :  76  /  100  | TL :  0.21858527244471793  | VL :  0.2124094539321959  | VA :  93.60000000000001\n",
      "epoch =  76  step =  50  of total steps  201  loss =  0.13960428535938263\n",
      "epoch =  76  step =  100  of total steps  201  loss =  0.14702609181404114\n",
      "epoch =  76  step =  150  of total steps  201  loss =  0.20948508381843567\n",
      "epoch =  76  step =  200  of total steps  201  loss =  0.17504432797431946\n",
      "epoch :  77  /  100  | TL :  0.20671073858862493  | VL :  0.2076313327997923  | VA :  93.60000000000001\n",
      "epoch =  77  step =  50  of total steps  201  loss =  0.179934561252594\n",
      "epoch =  77  step =  100  of total steps  201  loss =  0.218841552734375\n",
      "epoch =  77  step =  150  of total steps  201  loss =  0.1836496889591217\n",
      "epoch =  77  step =  200  of total steps  201  loss =  0.0675603523850441\n",
      "epoch :  78  /  100  | TL :  0.2088256879751362  | VL :  0.22281363513320684  | VA :  92.60000000000001\n",
      "epoch =  78  step =  50  of total steps  201  loss =  0.12828785181045532\n",
      "epoch =  78  step =  100  of total steps  201  loss =  0.07472792267799377\n",
      "epoch =  78  step =  150  of total steps  201  loss =  0.1403287649154663\n",
      "epoch =  78  step =  200  of total steps  201  loss =  0.23746052384376526\n",
      "epoch :  79  /  100  | TL :  0.2070226641140174  | VL :  0.2186254202388227  | VA :  93.4\n",
      "epoch =  79  step =  50  of total steps  201  loss =  0.22086262702941895\n",
      "epoch =  79  step =  100  of total steps  201  loss =  0.184413880109787\n",
      "epoch =  79  step =  150  of total steps  201  loss =  0.1277010589838028\n",
      "epoch =  79  step =  200  of total steps  201  loss =  0.20196814835071564\n",
      "epoch :  80  /  100  | TL :  0.21251043347428686  | VL :  0.2101279622875154  | VA :  93.2\n",
      "epoch =  80  step =  50  of total steps  201  loss =  0.1890001744031906\n",
      "epoch =  80  step =  100  of total steps  201  loss =  0.17889681458473206\n",
      "epoch =  80  step =  150  of total steps  201  loss =  0.1468447893857956\n",
      "epoch =  80  step =  200  of total steps  201  loss =  0.32650893926620483\n",
      "epoch :  81  /  100  | TL :  0.20678493471362105  | VL :  0.21172756096348166  | VA :  92.60000000000001\n",
      "epoch =  81  step =  50  of total steps  201  loss =  0.20737861096858978\n",
      "epoch =  81  step =  100  of total steps  201  loss =  0.27274465560913086\n",
      "epoch =  81  step =  150  of total steps  201  loss =  0.20147119462490082\n",
      "epoch =  81  step =  200  of total steps  201  loss =  0.24447943270206451\n",
      "epoch :  82  /  100  | TL :  0.20711520026942984  | VL :  0.21040588058531284  | VA :  93.0\n",
      "epoch =  82  step =  50  of total steps  201  loss =  0.36732107400894165\n",
      "epoch =  82  step =  100  of total steps  201  loss =  0.09554411470890045\n",
      "epoch =  82  step =  150  of total steps  201  loss =  0.2928888201713562\n",
      "epoch =  82  step =  200  of total steps  201  loss =  0.16052526235580444\n",
      "epoch :  83  /  100  | TL :  0.21540232621763475  | VL :  0.21808390598744154  | VA :  92.80000000000001\n",
      "epoch =  83  step =  50  of total steps  201  loss =  0.2268902212381363\n",
      "epoch =  83  step =  100  of total steps  201  loss =  0.3237064480781555\n",
      "epoch =  83  step =  150  of total steps  201  loss =  0.20995323359966278\n",
      "epoch =  83  step =  200  of total steps  201  loss =  0.17768138647079468\n",
      "epoch :  84  /  100  | TL :  0.2097436836851177  | VL :  0.20723163848742843  | VA :  93.2\n",
      "epoch =  84  step =  50  of total steps  201  loss =  0.22802364826202393\n",
      "epoch =  84  step =  100  of total steps  201  loss =  0.22250762581825256\n",
      "epoch =  84  step =  150  of total steps  201  loss =  0.18287545442581177\n",
      "epoch =  84  step =  200  of total steps  201  loss =  0.17541901767253876\n",
      "epoch :  85  /  100  | TL :  0.20895601475416725  | VL :  0.22101373225450516  | VA :  92.60000000000001\n",
      "epoch =  85  step =  50  of total steps  201  loss =  0.2009541392326355\n",
      "epoch =  85  step =  100  of total steps  201  loss =  0.18628428876399994\n",
      "epoch =  85  step =  150  of total steps  201  loss =  0.221984401345253\n",
      "epoch =  85  step =  200  of total steps  201  loss =  0.10353030264377594\n",
      "epoch :  86  /  100  | TL :  0.21447170160673745  | VL :  0.21834596060216427  | VA :  92.4\n",
      "epoch =  86  step =  50  of total steps  201  loss =  0.1803966462612152\n",
      "epoch =  86  step =  100  of total steps  201  loss =  0.17879782617092133\n",
      "epoch =  86  step =  150  of total steps  201  loss =  0.19976168870925903\n",
      "epoch =  86  step =  200  of total steps  201  loss =  0.18834859132766724\n",
      "epoch :  87  /  100  | TL :  0.20189750628240072  | VL :  0.22733171982690692  | VA :  92.80000000000001\n",
      "epoch =  87  step =  50  of total steps  201  loss =  0.22987014055252075\n",
      "epoch =  87  step =  100  of total steps  201  loss =  0.17731836438179016\n",
      "epoch =  87  step =  150  of total steps  201  loss =  0.1737585812807083\n",
      "epoch =  87  step =  200  of total steps  201  loss =  0.1326458752155304\n",
      "epoch :  88  /  100  | TL :  0.20998782327222587  | VL :  0.23303969576954842  | VA :  92.80000000000001\n",
      "epoch =  88  step =  50  of total steps  201  loss =  0.09392301738262177\n",
      "epoch =  88  step =  100  of total steps  201  loss =  0.06981740891933441\n",
      "epoch =  88  step =  150  of total steps  201  loss =  0.0890650823712349\n",
      "epoch =  88  step =  200  of total steps  201  loss =  0.4492985010147095\n",
      "epoch :  89  /  100  | TL :  0.2021578306804842  | VL :  0.21658000629395247  | VA :  93.2\n",
      "epoch =  89  step =  50  of total steps  201  loss =  0.16005510091781616\n",
      "epoch =  89  step =  100  of total steps  201  loss =  0.22155512869358063\n",
      "epoch =  89  step =  150  of total steps  201  loss =  0.19806474447250366\n",
      "epoch =  89  step =  200  of total steps  201  loss =  0.2752523124217987\n",
      "epoch :  90  /  100  | TL :  0.20694533081271163  | VL :  0.21299751941114664  | VA :  93.0\n",
      "epoch =  90  step =  50  of total steps  201  loss =  0.17575955390930176\n",
      "epoch =  90  step =  100  of total steps  201  loss =  0.28462398052215576\n",
      "epoch =  90  step =  150  of total steps  201  loss =  0.28449511528015137\n",
      "epoch =  90  step =  200  of total steps  201  loss =  0.1497405767440796\n",
      "epoch :  91  /  100  | TL :  0.2049129093390199  | VL :  0.21291845617815852  | VA :  93.2\n",
      "epoch =  91  step =  50  of total steps  201  loss =  0.4348188638687134\n",
      "epoch =  91  step =  100  of total steps  201  loss =  0.3282663822174072\n",
      "epoch =  91  step =  150  of total steps  201  loss =  0.09024792909622192\n",
      "epoch =  91  step =  200  of total steps  201  loss =  0.19182337820529938\n",
      "epoch :  92  /  100  | TL :  0.2100016929172165  | VL :  0.24301638826727867  | VA :  91.60000000000001\n",
      "epoch =  92  step =  50  of total steps  201  loss =  0.16814398765563965\n",
      "epoch =  92  step =  100  of total steps  201  loss =  0.4801711440086365\n",
      "epoch =  92  step =  150  of total steps  201  loss =  0.26508629322052\n",
      "epoch =  92  step =  200  of total steps  201  loss =  0.14326629042625427\n",
      "epoch :  93  /  100  | TL :  0.20609083486285376  | VL :  0.22091306652873755  | VA :  93.0\n",
      "epoch =  93  step =  50  of total steps  201  loss =  0.27306801080703735\n",
      "epoch =  93  step =  100  of total steps  201  loss =  0.2846256196498871\n",
      "epoch =  93  step =  150  of total steps  201  loss =  0.14421488344669342\n",
      "epoch =  93  step =  200  of total steps  201  loss =  0.3129074275493622\n",
      "epoch :  94  /  100  | TL :  0.20687150410306987  | VL :  0.2175068105570972  | VA :  93.0\n",
      "epoch =  94  step =  50  of total steps  201  loss =  0.3238535225391388\n",
      "epoch =  94  step =  100  of total steps  201  loss =  0.13867956399917603\n",
      "epoch =  94  step =  150  of total steps  201  loss =  0.309222936630249\n",
      "epoch =  94  step =  200  of total steps  201  loss =  0.3274945616722107\n",
      "epoch :  95  /  100  | TL :  0.20710761785803744  | VL :  0.22020801110193133  | VA :  93.2\n",
      "epoch =  95  step =  50  of total steps  201  loss =  0.09229438751935959\n",
      "epoch =  95  step =  100  of total steps  201  loss =  0.0998733788728714\n",
      "epoch =  95  step =  150  of total steps  201  loss =  0.36737507581710815\n",
      "epoch =  95  step =  200  of total steps  201  loss =  0.15759417414665222\n",
      "epoch :  96  /  100  | TL :  0.20179957405325785  | VL :  0.20760731305927038  | VA :  93.4\n",
      "epoch =  96  step =  50  of total steps  201  loss =  0.09084869921207428\n",
      "epoch =  96  step =  100  of total steps  201  loss =  0.266315221786499\n",
      "epoch =  96  step =  150  of total steps  201  loss =  0.33568859100341797\n",
      "epoch =  96  step =  200  of total steps  201  loss =  0.26545682549476624\n",
      "epoch :  97  /  100  | TL :  0.21201560874261074  | VL :  0.20362842921167612  | VA :  93.4\n",
      "epoch =  97  step =  50  of total steps  201  loss =  0.12498249858617783\n",
      "epoch =  97  step =  100  of total steps  201  loss =  0.46154746413230896\n",
      "epoch =  97  step =  150  of total steps  201  loss =  0.08924416452646255\n",
      "epoch =  97  step =  200  of total steps  201  loss =  0.25829610228538513\n",
      "epoch :  98  /  100  | TL :  0.20151827829097635  | VL :  0.23604949982836843  | VA :  92.80000000000001\n",
      "epoch =  98  step =  50  of total steps  201  loss =  0.14088723063468933\n",
      "epoch =  98  step =  100  of total steps  201  loss =  0.1580897867679596\n",
      "epoch =  98  step =  150  of total steps  201  loss =  0.1972825974225998\n",
      "epoch =  98  step =  200  of total steps  201  loss =  0.1401061713695526\n",
      "epoch :  99  /  100  | TL :  0.20697332992779083  | VL :  0.21834748052060604  | VA :  93.0\n",
      "epoch =  99  step =  50  of total steps  201  loss =  0.08956532925367355\n",
      "epoch =  99  step =  100  of total steps  201  loss =  0.35676488280296326\n",
      "epoch =  99  step =  150  of total steps  201  loss =  0.26860085129737854\n",
      "epoch =  99  step =  200  of total steps  201  loss =  0.23194725811481476\n",
      "epoch :  100  /  100  | TL :  0.20996719928671473  | VL :  0.239178532268852  | VA :  92.80000000000001\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 0\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, labels)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, labels)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc = _get_accuracy(data.valid_dl, net)\n",
    "\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss, ' | VA : ', val_acc * 100)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "    experiment.log_metric(\"val_acc\", val_acc)\n",
    "\n",
    "    if (val_acc * 100) > min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_acc * 100\n",
    "        torch.save(net.state_dict(), '../saved_models/classifier/model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938\n",
      "0.936\n"
     ]
    }
   ],
   "source": [
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/classifier/model2.pt', map_location = 'cpu'))\n",
    "net.cuda()\n",
    "\n",
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "\n",
    "print(_get_accuracy(data.valid_dl, net))\n",
    "print(_get_accuracy(data.valid_dl, learn.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ak_fastai)",
   "language": "python",
   "name": "ak_fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
