{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "torch.cuda.set_device(1)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(3)\n",
    "\n",
    "# stage should be in 0 to 5 (5 for classifier stage)\n",
    "hyper_params = {\n",
    "    \"stage\": 5,\n",
    "    \"repeated\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "data = ImageDataBunch.from_folder(path, train = 'train', valid = 'val', bs = hyper_params[\"batch_size\"], size = 224, ds_tfms = tfms).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "True\n",
      "5.0.2.weight torch.Size([512])\n",
      "True\n",
      "5.0.2.bias torch.Size([512])\n",
      "True\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "True\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "True\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "True\n",
      "8.weight torch.Size([256, 1024])\n",
      "False\n",
      "8.bias torch.Size([256])\n",
      "False\n",
      "9.weight torch.Size([10, 256])\n",
      "False\n",
      "9.bias torch.Size([10])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "# learn.summary()\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "if hyper_params['stage'] != 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "    net.load_state_dict(torch.load(filename, map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == str(hyper_params['stage'] + 1) and hyper_params['stage'] != 0 :\n",
    "        param.requires_grad = True\n",
    "    elif name[0] == str(hyper_params['stage']) and hyper_params['stage'] == 0 : \n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "              ReLU-2         [-1, 64, 112, 112]               0\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-9           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "         ResBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12          [-1, 128, 28, 28]          73,728\n",
      "             ReLU-13          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-14          [-1, 128, 28, 28]             256\n",
      "           Conv2d-15          [-1, 128, 28, 28]         147,456\n",
      "             ReLU-16          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
      "         ResBlock-18          [-1, 128, 28, 28]               0\n",
      "           Conv2d-19          [-1, 256, 14, 14]         294,912\n",
      "             ReLU-20          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-21          [-1, 256, 14, 14]             512\n",
      "           Conv2d-22          [-1, 256, 14, 14]         589,824\n",
      "             ReLU-23          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-24          [-1, 256, 14, 14]             512\n",
      "         ResBlock-25          [-1, 256, 14, 14]               0\n",
      "           Conv2d-26            [-1, 512, 7, 7]       1,179,648\n",
      "             ReLU-27            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-28            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-29            [-1, 512, 7, 7]       2,359,296\n",
      "             ReLU-30            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-31            [-1, 512, 7, 7]           1,024\n",
      "         ResBlock-32            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-33            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-34            [-1, 512, 1, 1]               0\n",
      "AdaptiveConcatPool2d-35           [-1, 1024, 1, 1]               0\n",
      "          Flatten-36                 [-1, 1024]               0\n",
      "           Linear-37                  [-1, 256]         262,400\n",
      "           Linear-38                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,996,938\n",
      "Trainable params: 9,536\n",
      "Non-trainable params: 4,987,402\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 40.03\n",
      "Params size (MB): 19.06\n",
      "Estimated Total Size (MB): 59.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x, y = next(iter(data.train_dl))\n",
    "# net(torch.autograd.Variable(x).cuda())\n",
    "summary(net, (3, 224, 224))\n",
    "# print(learn.summary())\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures :\n",
    "    def __init__(self, m) : \n",
    "        self.handle = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, m, inp, outp) : \n",
    "        self.features = outp\n",
    "    def remove(self) :\n",
    "        self.handle.remove()\n",
    "        \n",
    "# saving outputs of all Basic Blocks\n",
    "mdl = learn.model\n",
    "sf = [SaveFeatures(m) for m in [mdl[0][2], mdl[0][4], mdl[0][5], mdl[0][6], mdl[0][7]]]\n",
    "sf2 = [SaveFeatures(m) for m in [net[0], net[2], net[3], net[4], net[5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 112, 112])\n",
      "torch.Size([64, 64, 56, 56])\n",
      "torch.Size([64, 128, 28, 28])\n",
      "torch.Size([64, 256, 14, 14])\n",
      "torch.Size([64, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(data.train_dl))\n",
    "x = torch.autograd.Variable(x).cuda()\n",
    "out1 = mdl(x)\n",
    "out2 = net(x)\n",
    "for i in range(5) : \n",
    "    print(sf[i].features.shape)\n",
    "    assert(sf[i].features.shape == sf2[i].features.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/akshaykvnit/kd0/d5b4b65c9a3c4ba6b252264557d516d2\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [2010]                   : (0.021085122600197792, 0.8147169947624207)\n",
      "COMET INFO:     sys.gpu.0.free_memory [88]    : (5101846528.0, 5139595264.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [88]: (0.0, 99.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory        : (11721506816.0, 11721506816.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [88]    : (6581911552.0, 6619660288.0)\n",
      "COMET INFO:     sys.gpu.1.free_memory [88]    : (1238106112.0, 6221987840.0)\n",
      "COMET INFO:     sys.gpu.1.gpu_utilization [88]: (0.0, 97.0)\n",
      "COMET INFO:     sys.gpu.1.total_memory        : (6233391104.0, 6233391104.0)\n",
      "COMET INFO:     sys.gpu.1.used_memory [88]    : (11403264.0, 4995284992.0)\n",
      "COMET INFO:     train_loss [100]              : (0.02236961764260311, 0.4373542653090918)\n",
      "COMET INFO:     val_loss [100]                : (0.021828623954206705, 0.28351984173059464)\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (2.0.9) detected. current: 2.0.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/3e8621a0783c4bfcbb67c4b4cf60f648\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1  step =  50  of total steps  201  loss =  1.273905873298645\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.8543833494186401\n",
      "epoch =  1  step =  150  of total steps  201  loss =  1.9610918760299683\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.7102090120315552\n",
      "epoch :  1  /  100  | TL :  1.460261870082931  | VL :  1.1243370547890663\n",
      "saving model\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.8931823968887329\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.8252297043800354\n",
      "epoch =  2  step =  150  of total steps  201  loss =  1.7569987773895264\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.9887546300888062\n",
      "epoch :  2  /  100  | TL :  1.0779924440146678  | VL :  0.9869095385074615\n",
      "saving model\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.611712634563446\n",
      "epoch =  3  step =  100  of total steps  201  loss =  1.44060218334198\n",
      "epoch =  3  step =  150  of total steps  201  loss =  1.8025075197219849\n",
      "epoch =  3  step =  200  of total steps  201  loss =  1.0458030700683594\n",
      "epoch :  3  /  100  | TL :  0.9432572858843637  | VL :  0.8165855072438717\n",
      "saving model\n",
      "epoch =  4  step =  50  of total steps  201  loss =  1.0709335803985596\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.7606213688850403\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.7903650999069214\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.5269346833229065\n",
      "epoch :  4  /  100  | TL :  0.9009965468401933  | VL :  0.7783596739172935\n",
      "saving model\n",
      "epoch =  5  step =  50  of total steps  201  loss =  1.3039181232452393\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.4280940294265747\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.5318881869316101\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.5257815718650818\n",
      "epoch :  5  /  100  | TL :  0.8094864604781516  | VL :  0.7364561520516872\n",
      "saving model\n",
      "epoch =  6  step =  50  of total steps  201  loss =  2.100296974182129\n",
      "epoch =  6  step =  100  of total steps  201  loss =  1.6141510009765625\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.4239449203014374\n",
      "epoch =  6  step =  200  of total steps  201  loss =  1.0384490489959717\n",
      "epoch :  6  /  100  | TL :  0.771615018892051  | VL :  0.670272309333086\n",
      "saving model\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.4033263325691223\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.8785440921783447\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.5773699283599854\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.40088126063346863\n",
      "epoch :  7  /  100  | TL :  0.6997126416483922  | VL :  0.6146602109074593\n",
      "saving model\n",
      "epoch =  8  step =  50  of total steps  201  loss =  1.8779785633087158\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.3874003291130066\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.848390519618988\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.6485363841056824\n",
      "epoch :  8  /  100  | TL :  0.6816270923733119  | VL :  0.5936354491859674\n",
      "saving model\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.4221392273902893\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.4105560779571533\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.43161725997924805\n",
      "epoch =  9  step =  200  of total steps  201  loss =  1.2705533504486084\n",
      "epoch :  9  /  100  | TL :  0.6525398780457417  | VL :  0.581926854327321\n",
      "saving model\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.29427480697631836\n",
      "epoch =  10  step =  100  of total steps  201  loss =  1.2952792644500732\n",
      "epoch =  10  step =  150  of total steps  201  loss =  1.0900686979293823\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.48826128244400024\n",
      "epoch :  10  /  100  | TL :  0.6305366624054031  | VL :  0.4934896435588598\n",
      "saving model\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.4500312805175781\n",
      "epoch =  11  step =  100  of total steps  201  loss =  1.1221455335617065\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.498919814825058\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.36322763562202454\n",
      "epoch :  11  /  100  | TL :  0.5985934551972062  | VL :  0.510127192363143\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.2616598308086395\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.38797664642333984\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.32171374559402466\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.5240250825881958\n",
      "epoch :  12  /  100  | TL :  0.592586743520267  | VL :  0.4523712061345577\n",
      "saving model\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.8206365704536438\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.32853978872299194\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.5414849519729614\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.3485201895236969\n",
      "epoch :  13  /  100  | TL :  0.533050712364823  | VL :  0.43940611369907856\n",
      "saving model\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.4187827408313751\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.25954657793045044\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.7732774019241333\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.296394407749176\n",
      "epoch :  14  /  100  | TL :  0.5444998308942093  | VL :  0.47644705325365067\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.3806043267250061\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.8870072364807129\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.5912668704986572\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.22932717204093933\n",
      "epoch :  15  /  100  | TL :  0.5298867929930711  | VL :  0.395844304934144\n",
      "saving model\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.6014696955680847\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.47407788038253784\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.6715388298034668\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.33889415860176086\n",
      "epoch :  16  /  100  | TL :  0.5095088105296615  | VL :  0.3846784410998225\n",
      "saving model\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.3238319456577301\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.4960181415081024\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.936896800994873\n",
      "epoch =  17  step =  200  of total steps  201  loss =  1.4998393058776855\n",
      "epoch :  17  /  100  | TL :  0.4842964243977817  | VL :  0.3848405834287405\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.26415762305259705\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.7549511790275574\n",
      "epoch =  18  step =  150  of total steps  201  loss =  1.0496705770492554\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.19281470775604248\n",
      "epoch :  18  /  100  | TL :  0.5028064101934433  | VL :  0.3874549148604274\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.5426076054573059\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.21028634905815125\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.211297869682312\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.28559866547584534\n",
      "epoch :  19  /  100  | TL :  0.4667181597271962  | VL :  0.37194468080997467\n",
      "saving model\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.2618909180164337\n",
      "epoch =  20  step =  100  of total steps  201  loss =  1.04337477684021\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.4334667921066284\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.23913106322288513\n",
      "epoch :  20  /  100  | TL :  0.4475876155925627  | VL :  0.3763844007626176\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.649196445941925\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.21901315450668335\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.1975056529045105\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.22887136042118073\n",
      "epoch :  21  /  100  | TL :  0.45253225723605844  | VL :  0.3817788576707244\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.799842357635498\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.21176092326641083\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.3904959261417389\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.31786319613456726\n",
      "epoch :  22  /  100  | TL :  0.44278781538579004  | VL :  0.35597958508878946\n",
      "saving model\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.1695030927658081\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.2293725609779358\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.9631608128547668\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.3781968653202057\n",
      "epoch :  23  /  100  | TL :  0.4426744988012077  | VL :  0.3235045848414302\n",
      "saving model\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.4738873839378357\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.1803208291530609\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.20626601576805115\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.26556894183158875\n",
      "epoch :  24  /  100  | TL :  0.44196407630372403  | VL :  0.3142313966527581\n",
      "saving model\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.5385630130767822\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.1865459680557251\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.1575833559036255\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.6709457039833069\n",
      "epoch :  25  /  100  | TL :  0.42853724622904366  | VL :  0.3586214706301689\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.6056458353996277\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.7850422859191895\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.3951433598995209\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.21519352495670319\n",
      "epoch :  26  /  100  | TL :  0.3853900826244212  | VL :  0.287258030846715\n",
      "saving model\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.2911556363105774\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.20873206853866577\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.23970343172550201\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.49221816658973694\n",
      "epoch :  27  /  100  | TL :  0.4132715846175578  | VL :  0.33214706368744373\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.20796054601669312\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.21620796620845795\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.25089535117149353\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.2133672833442688\n",
      "epoch :  28  /  100  | TL :  0.3971877102531604  | VL :  0.31795025151222944\n",
      "epoch =  29  step =  50  of total steps  201  loss =  1.0884391069412231\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.30743247270584106\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.34365227818489075\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.1907598078250885\n",
      "epoch :  29  /  100  | TL :  0.3738938008078295  | VL :  0.3302084803581238\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.17487990856170654\n",
      "epoch =  30  step =  100  of total steps  201  loss =  1.555212140083313\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.753603994846344\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.31519201397895813\n",
      "epoch :  30  /  100  | TL :  0.4023578287801932  | VL :  0.3163811443373561\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.19191540777683258\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.1815880388021469\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.21773606538772583\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.22403249144554138\n",
      "epoch :  31  /  100  | TL :  0.35057140011989063  | VL :  0.31697223614901304\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.18190878629684448\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.9963732361793518\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.22569461166858673\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.1507251113653183\n",
      "epoch :  32  /  100  | TL :  0.3819693702815184  | VL :  0.3115187706425786\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.41653192043304443\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.3640918731689453\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.36486437916755676\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.3077368438243866\n",
      "epoch :  33  /  100  | TL :  0.3652429469485781  | VL :  0.27128284331411123\n",
      "saving model\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.2535001039505005\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.21784217655658722\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.15746726095676422\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.16168375313282013\n",
      "epoch :  34  /  100  | TL :  0.3669183174176003  | VL :  0.26707000518217683\n",
      "saving model\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.16005435585975647\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.30238714814186096\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.34010088443756104\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.26057541370391846\n",
      "epoch :  35  /  100  | TL :  0.3794169814432438  | VL :  0.3053861130028963\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.5515002608299255\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.2103748619556427\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.2172526717185974\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.22783571481704712\n",
      "epoch :  36  /  100  | TL :  0.3879079506987363  | VL :  0.3033919990994036\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.15204878151416779\n",
      "epoch =  37  step =  100  of total steps  201  loss =  1.344142198562622\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.20968589186668396\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.1653449684381485\n",
      "epoch :  37  /  100  | TL :  0.373526034811836  | VL :  0.3163769990205765\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.18273897469043732\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.36882665753364563\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.21030262112617493\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.24062477052211761\n",
      "epoch :  38  /  100  | TL :  0.37091550053055605  | VL :  0.3223495576530695\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.14048424363136292\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.18839754164218903\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.29276758432388306\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.169806107878685\n",
      "epoch :  39  /  100  | TL :  0.3640294079460315  | VL :  0.2588020791299641\n",
      "saving model\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.2429904043674469\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.18454305827617645\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.42479172348976135\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.2779789865016937\n",
      "epoch :  40  /  100  | TL :  0.38104632133571664  | VL :  0.25579153513535857\n",
      "saving model\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.20078055560588837\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.22270835936069489\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.4041571319103241\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.151527538895607\n",
      "epoch :  41  /  100  | TL :  0.3666666124145783  | VL :  0.2604715130291879\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.17508789896965027\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.17920832335948944\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.17568525671958923\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.14330998063087463\n",
      "epoch :  42  /  100  | TL :  0.35086695182679306  | VL :  0.26222449308261275\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.8342626094818115\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.14765024185180664\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.3312012255191803\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.27640843391418457\n",
      "epoch :  43  /  100  | TL :  0.3396143190451522  | VL :  0.26432002941146493\n",
      "epoch =  44  step =  50  of total steps  201  loss =  1.1921671628952026\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.23574887216091156\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.35286641120910645\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.27119341492652893\n",
      "epoch :  44  /  100  | TL :  0.35188591843517264  | VL :  0.23522198107093573\n",
      "saving model\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.5542600154876709\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.3364824056625366\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.33688780665397644\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.22205212712287903\n",
      "epoch :  45  /  100  | TL :  0.3664550842782158  | VL :  0.2579932124353945\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.4324228763580322\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.203750878572464\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.4968254566192627\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.15869607031345367\n",
      "epoch :  46  /  100  | TL :  0.33185048890647606  | VL :  0.25869927695021033\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.554028332233429\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.24977563321590424\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.2913305163383484\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.19290485978126526\n",
      "epoch :  47  /  100  | TL :  0.35409063189776974  | VL :  0.2406193264760077\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.23131245374679565\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.2511237859725952\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.2961461842060089\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.1683422327041626\n",
      "epoch :  48  /  100  | TL :  0.3457893460840728  | VL :  0.2692787856794894\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.3104609549045563\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.275640606880188\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.15839655697345734\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.84759521484375\n",
      "epoch :  49  /  100  | TL :  0.3312603838334036  | VL :  0.27224679477512836\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.293634831905365\n",
      "epoch =  50  step =  100  of total steps  201  loss =  1.1332165002822876\n",
      "epoch =  50  step =  150  of total steps  201  loss =  0.1886720061302185\n",
      "epoch =  50  step =  200  of total steps  201  loss =  0.21695207059383392\n",
      "epoch :  50  /  100  | TL :  0.3474591059322974  | VL :  0.2342588333413005\n",
      "saving model\n",
      "epoch =  51  step =  50  of total steps  201  loss =  0.14630572497844696\n",
      "epoch =  51  step =  100  of total steps  201  loss =  0.14346590638160706\n",
      "epoch =  51  step =  150  of total steps  201  loss =  0.15440478920936584\n",
      "epoch =  51  step =  200  of total steps  201  loss =  0.5590506196022034\n",
      "epoch :  51  /  100  | TL :  0.35116709442577554  | VL :  0.240123909432441\n",
      "epoch =  52  step =  50  of total steps  201  loss =  0.20378796756267548\n",
      "epoch =  52  step =  100  of total steps  201  loss =  0.2592536211013794\n",
      "epoch =  52  step =  150  of total steps  201  loss =  0.8472344279289246\n",
      "epoch =  52  step =  200  of total steps  201  loss =  0.42338642477989197\n",
      "epoch :  52  /  100  | TL :  0.34411800812132914  | VL :  0.2526205498725176\n",
      "epoch =  53  step =  50  of total steps  201  loss =  0.19913645088672638\n",
      "epoch =  53  step =  100  of total steps  201  loss =  0.14322802424430847\n",
      "epoch =  53  step =  150  of total steps  201  loss =  0.7338689565658569\n",
      "epoch =  53  step =  200  of total steps  201  loss =  0.3886088728904724\n",
      "epoch :  53  /  100  | TL :  0.32294572181814346  | VL :  0.26702141808345914\n",
      "epoch =  54  step =  50  of total steps  201  loss =  0.22393646836280823\n",
      "epoch =  54  step =  100  of total steps  201  loss =  0.22544421255588531\n",
      "epoch =  54  step =  150  of total steps  201  loss =  0.19655746221542358\n",
      "epoch =  54  step =  200  of total steps  201  loss =  1.6369030475616455\n",
      "epoch :  54  /  100  | TL :  0.33500533828984447  | VL :  0.23438597563654184\n",
      "epoch =  55  step =  50  of total steps  201  loss =  0.20139293372631073\n",
      "epoch =  55  step =  100  of total steps  201  loss =  0.29811328649520874\n",
      "epoch =  55  step =  150  of total steps  201  loss =  0.32225725054740906\n",
      "epoch =  55  step =  200  of total steps  201  loss =  0.20323877036571503\n",
      "epoch :  55  /  100  | TL :  0.33816029817162463  | VL :  0.23383762780576944\n",
      "saving model\n",
      "epoch =  56  step =  50  of total steps  201  loss =  0.49582305550575256\n",
      "epoch =  56  step =  100  of total steps  201  loss =  0.17866238951683044\n",
      "epoch =  56  step =  150  of total steps  201  loss =  0.24453400075435638\n",
      "epoch =  56  step =  200  of total steps  201  loss =  0.16512952744960785\n",
      "epoch :  56  /  100  | TL :  0.32071802038606717  | VL :  0.2536951843649149\n",
      "epoch =  57  step =  50  of total steps  201  loss =  0.2644751965999603\n",
      "epoch =  57  step =  100  of total steps  201  loss =  0.14611934125423431\n",
      "epoch =  57  step =  150  of total steps  201  loss =  0.20257237553596497\n",
      "epoch =  57  step =  200  of total steps  201  loss =  0.27280014753341675\n",
      "epoch :  57  /  100  | TL :  0.33096419804873156  | VL :  0.24554121494293213\n",
      "epoch =  58  step =  50  of total steps  201  loss =  0.1479516327381134\n",
      "epoch =  58  step =  100  of total steps  201  loss =  0.20351678133010864\n",
      "epoch =  58  step =  150  of total steps  201  loss =  1.1182478666305542\n",
      "epoch =  58  step =  200  of total steps  201  loss =  0.35953372716903687\n",
      "epoch :  58  /  100  | TL :  0.3272969821347526  | VL :  0.24313768092542887\n",
      "epoch =  59  step =  50  of total steps  201  loss =  0.1933315247297287\n",
      "epoch =  59  step =  100  of total steps  201  loss =  0.33537569642066956\n",
      "epoch =  59  step =  150  of total steps  201  loss =  0.18249593675136566\n",
      "epoch =  59  step =  200  of total steps  201  loss =  0.1492420732975006\n",
      "epoch :  59  /  100  | TL :  0.31640807339059773  | VL :  0.23367381747812033\n",
      "saving model\n",
      "epoch =  60  step =  50  of total steps  201  loss =  0.1603693664073944\n",
      "epoch =  60  step =  100  of total steps  201  loss =  0.12470682710409164\n",
      "epoch =  60  step =  150  of total steps  201  loss =  0.16979719698429108\n",
      "epoch =  60  step =  200  of total steps  201  loss =  0.19857876002788544\n",
      "epoch :  60  /  100  | TL :  0.32818217453227116  | VL :  0.25362844206392765\n",
      "epoch =  61  step =  50  of total steps  201  loss =  0.16933679580688477\n",
      "epoch =  61  step =  100  of total steps  201  loss =  0.24177372455596924\n",
      "epoch =  61  step =  150  of total steps  201  loss =  0.24173814058303833\n",
      "epoch =  61  step =  200  of total steps  201  loss =  0.2876597046852112\n",
      "epoch :  61  /  100  | TL :  0.32453000630757106  | VL :  0.22555344039574265\n",
      "saving model\n",
      "epoch =  62  step =  50  of total steps  201  loss =  0.353016197681427\n",
      "epoch =  62  step =  100  of total steps  201  loss =  0.19695277512073517\n",
      "epoch =  62  step =  150  of total steps  201  loss =  0.5441747903823853\n",
      "epoch =  62  step =  200  of total steps  201  loss =  0.6415784955024719\n",
      "epoch :  62  /  100  | TL :  0.3163578874287914  | VL :  0.2551672710105777\n",
      "epoch =  63  step =  50  of total steps  201  loss =  0.5376628041267395\n",
      "epoch =  63  step =  100  of total steps  201  loss =  0.1886863261461258\n",
      "epoch =  63  step =  150  of total steps  201  loss =  0.3289477229118347\n",
      "epoch =  63  step =  200  of total steps  201  loss =  0.1887843757867813\n",
      "epoch :  63  /  100  | TL :  0.30689834829290114  | VL :  0.23421756038442254\n",
      "epoch =  64  step =  50  of total steps  201  loss =  0.25847744941711426\n",
      "epoch =  64  step =  100  of total steps  201  loss =  0.5394814610481262\n",
      "epoch =  64  step =  150  of total steps  201  loss =  0.18940508365631104\n",
      "epoch =  64  step =  200  of total steps  201  loss =  0.5608015060424805\n",
      "epoch :  64  /  100  | TL :  0.3285013893572845  | VL :  0.2386588235385716\n",
      "epoch =  65  step =  50  of total steps  201  loss =  0.24664613604545593\n",
      "epoch =  65  step =  100  of total steps  201  loss =  0.501479983329773\n",
      "epoch =  65  step =  150  of total steps  201  loss =  0.3168862462043762\n",
      "epoch =  65  step =  200  of total steps  201  loss =  0.1703994870185852\n",
      "epoch :  65  /  100  | TL :  0.3421732204322198  | VL :  0.2300665332004428\n",
      "epoch =  66  step =  50  of total steps  201  loss =  0.24505344033241272\n",
      "epoch =  66  step =  100  of total steps  201  loss =  0.6000304222106934\n",
      "epoch =  66  step =  150  of total steps  201  loss =  0.3396349549293518\n",
      "epoch =  66  step =  200  of total steps  201  loss =  0.6962200999259949\n",
      "epoch :  66  /  100  | TL :  0.3148815287508775  | VL :  0.23912197444587946\n",
      "epoch =  67  step =  50  of total steps  201  loss =  0.33691367506980896\n",
      "epoch =  67  step =  100  of total steps  201  loss =  0.39801687002182007\n",
      "epoch =  67  step =  150  of total steps  201  loss =  0.27425822615623474\n",
      "epoch =  67  step =  200  of total steps  201  loss =  0.155582457780838\n",
      "epoch :  67  /  100  | TL :  0.32334450472943227  | VL :  0.234548500739038\n",
      "epoch =  68  step =  50  of total steps  201  loss =  0.190167635679245\n",
      "epoch =  68  step =  100  of total steps  201  loss =  0.18478018045425415\n",
      "epoch =  68  step =  150  of total steps  201  loss =  0.31718048453330994\n",
      "epoch =  68  step =  200  of total steps  201  loss =  0.2865199148654938\n",
      "epoch :  68  /  100  | TL :  0.3270132325923265  | VL :  0.22969383792951703\n",
      "epoch =  69  step =  50  of total steps  201  loss =  0.19023369252681732\n",
      "epoch =  69  step =  100  of total steps  201  loss =  1.0676552057266235\n",
      "epoch =  69  step =  150  of total steps  201  loss =  0.2572728395462036\n",
      "epoch =  69  step =  200  of total steps  201  loss =  0.8964882493019104\n",
      "epoch :  69  /  100  | TL :  0.3312530896408641  | VL :  0.23968875966966152\n",
      "epoch =  70  step =  50  of total steps  201  loss =  0.14645126461982727\n",
      "epoch =  70  step =  100  of total steps  201  loss =  0.18226438760757446\n",
      "epoch =  70  step =  150  of total steps  201  loss =  0.1663394421339035\n",
      "epoch =  70  step =  200  of total steps  201  loss =  0.29995572566986084\n",
      "epoch :  70  /  100  | TL :  0.3274829065176978  | VL :  0.22511491319164634\n",
      "saving model\n",
      "epoch =  71  step =  50  of total steps  201  loss =  0.24306555092334747\n",
      "epoch =  71  step =  100  of total steps  201  loss =  0.17870517075061798\n",
      "epoch =  71  step =  150  of total steps  201  loss =  0.16457267105579376\n",
      "epoch =  71  step =  200  of total steps  201  loss =  0.40110641717910767\n",
      "epoch :  71  /  100  | TL :  0.3263729382584344  | VL :  0.23987749870866537\n",
      "epoch =  72  step =  50  of total steps  201  loss =  0.17320874333381653\n",
      "epoch =  72  step =  100  of total steps  201  loss =  0.20348860323429108\n",
      "epoch =  72  step =  150  of total steps  201  loss =  0.7036870121955872\n",
      "epoch =  72  step =  200  of total steps  201  loss =  0.6554113030433655\n",
      "epoch :  72  /  100  | TL :  0.3393132139868404  | VL :  0.24595042038708925\n",
      "epoch =  73  step =  50  of total steps  201  loss =  0.2464590221643448\n",
      "epoch =  73  step =  100  of total steps  201  loss =  0.2806236147880554\n",
      "epoch =  73  step =  150  of total steps  201  loss =  0.16367192566394806\n",
      "epoch =  73  step =  200  of total steps  201  loss =  0.27815937995910645\n",
      "epoch :  73  /  100  | TL :  0.29724266836002694  | VL :  0.23027447704225779\n",
      "epoch =  74  step =  50  of total steps  201  loss =  0.32494550943374634\n",
      "epoch =  74  step =  100  of total steps  201  loss =  0.14019151031970978\n",
      "epoch =  74  step =  150  of total steps  201  loss =  0.6063637733459473\n",
      "epoch =  74  step =  200  of total steps  201  loss =  0.24009926617145538\n",
      "epoch :  74  /  100  | TL :  0.31878906536606416  | VL :  0.23528428189456463\n",
      "epoch =  75  step =  50  of total steps  201  loss =  0.18385766446590424\n",
      "epoch =  75  step =  100  of total steps  201  loss =  0.18123619258403778\n",
      "epoch =  75  step =  150  of total steps  201  loss =  0.22491376101970673\n",
      "epoch =  75  step =  200  of total steps  201  loss =  0.1795874834060669\n",
      "epoch :  75  /  100  | TL :  0.311023929967216  | VL :  0.22692836076021194\n",
      "epoch =  76  step =  50  of total steps  201  loss =  0.18335911631584167\n",
      "epoch =  76  step =  100  of total steps  201  loss =  0.20551984012126923\n",
      "epoch =  76  step =  150  of total steps  201  loss =  0.5088341236114502\n",
      "epoch =  76  step =  200  of total steps  201  loss =  0.11520571261644363\n",
      "epoch :  76  /  100  | TL :  0.31606154851800766  | VL :  0.2295324793085456\n",
      "epoch =  77  step =  50  of total steps  201  loss =  0.20605862140655518\n",
      "epoch =  77  step =  100  of total steps  201  loss =  0.14058661460876465\n",
      "epoch =  77  step =  150  of total steps  201  loss =  0.2509869337081909\n",
      "epoch =  77  step =  200  of total steps  201  loss =  0.19874514639377594\n",
      "epoch :  77  /  100  | TL :  0.307597608078475  | VL :  0.2383341072127223\n",
      "epoch =  78  step =  50  of total steps  201  loss =  0.21648439764976501\n",
      "epoch =  78  step =  100  of total steps  201  loss =  0.23181460797786713\n",
      "epoch =  78  step =  150  of total steps  201  loss =  0.6550665497779846\n",
      "epoch =  78  step =  200  of total steps  201  loss =  1.092407464981079\n",
      "epoch :  78  /  100  | TL :  0.2972608525583993  | VL :  0.236059439368546\n",
      "epoch =  79  step =  50  of total steps  201  loss =  0.2073356658220291\n",
      "epoch =  79  step =  100  of total steps  201  loss =  0.3114677667617798\n",
      "epoch =  79  step =  150  of total steps  201  loss =  0.6287418007850647\n",
      "epoch =  79  step =  200  of total steps  201  loss =  0.23254071176052094\n",
      "epoch :  79  /  100  | TL :  0.31043979273506656  | VL :  0.2542811478488147\n",
      "epoch =  80  step =  50  of total steps  201  loss =  0.2021595984697342\n",
      "epoch =  80  step =  100  of total steps  201  loss =  0.21034714579582214\n",
      "epoch =  80  step =  150  of total steps  201  loss =  0.2220529019832611\n",
      "epoch =  80  step =  200  of total steps  201  loss =  0.3150079846382141\n",
      "epoch :  80  /  100  | TL :  0.3016640333393913  | VL :  0.2282364722341299\n",
      "epoch =  81  step =  50  of total steps  201  loss =  0.18485814332962036\n",
      "epoch =  81  step =  100  of total steps  201  loss =  0.4080662429332733\n",
      "epoch =  81  step =  150  of total steps  201  loss =  0.20473437011241913\n",
      "epoch =  81  step =  200  of total steps  201  loss =  0.36806347966194153\n",
      "epoch :  81  /  100  | TL :  0.30282718169303674  | VL :  0.2424935195595026\n",
      "epoch =  82  step =  50  of total steps  201  loss =  0.18011264503002167\n",
      "epoch =  82  step =  100  of total steps  201  loss =  0.15462300181388855\n",
      "epoch =  82  step =  150  of total steps  201  loss =  0.19904334843158722\n",
      "epoch =  82  step =  200  of total steps  201  loss =  0.17464320361614227\n",
      "epoch :  82  /  100  | TL :  0.3125668020539023  | VL :  0.228039741050452\n",
      "epoch =  83  step =  50  of total steps  201  loss =  0.42202937602996826\n",
      "epoch =  83  step =  100  of total steps  201  loss =  0.1582627296447754\n",
      "epoch =  83  step =  150  of total steps  201  loss =  0.3075190484523773\n",
      "epoch =  83  step =  200  of total steps  201  loss =  0.21758368611335754\n",
      "epoch :  83  /  100  | TL :  0.31853255344119236  | VL :  0.229285370092839\n",
      "epoch =  84  step =  50  of total steps  201  loss =  0.2112545371055603\n",
      "epoch =  84  step =  100  of total steps  201  loss =  0.5991966128349304\n",
      "epoch =  84  step =  150  of total steps  201  loss =  0.5200459361076355\n",
      "epoch =  84  step =  200  of total steps  201  loss =  0.18191632628440857\n",
      "epoch :  84  /  100  | TL :  0.29327977515423476  | VL :  0.24432192044332623\n",
      "epoch =  85  step =  50  of total steps  201  loss =  0.284702867269516\n",
      "epoch =  85  step =  100  of total steps  201  loss =  0.1189543604850769\n",
      "epoch =  85  step =  150  of total steps  201  loss =  0.2700139284133911\n",
      "epoch =  85  step =  200  of total steps  201  loss =  0.29826539754867554\n",
      "epoch :  85  /  100  | TL :  0.30793841498259883  | VL :  0.2181745539419353\n",
      "saving model\n",
      "epoch =  86  step =  50  of total steps  201  loss =  0.6127117872238159\n",
      "epoch =  86  step =  100  of total steps  201  loss =  0.278363972902298\n",
      "epoch =  86  step =  150  of total steps  201  loss =  0.1527496576309204\n",
      "epoch =  86  step =  200  of total steps  201  loss =  0.13249632716178894\n",
      "epoch :  86  /  100  | TL :  0.29573331980859463  | VL :  0.21850253734737635\n",
      "epoch =  87  step =  50  of total steps  201  loss =  0.2068813592195511\n",
      "epoch =  87  step =  100  of total steps  201  loss =  0.20309793949127197\n",
      "epoch =  87  step =  150  of total steps  201  loss =  0.25325843691825867\n",
      "epoch =  87  step =  200  of total steps  201  loss =  0.21546994149684906\n",
      "epoch :  87  /  100  | TL :  0.2978406936968144  | VL :  0.22527631931006908\n",
      "epoch =  88  step =  50  of total steps  201  loss =  0.3497507870197296\n",
      "epoch =  88  step =  100  of total steps  201  loss =  0.20697835087776184\n",
      "epoch =  88  step =  150  of total steps  201  loss =  0.23812811076641083\n",
      "epoch =  88  step =  200  of total steps  201  loss =  0.2316213697195053\n",
      "epoch :  88  /  100  | TL :  0.2990996762739485  | VL :  0.2264278340153396\n",
      "epoch =  89  step =  50  of total steps  201  loss =  0.19334933161735535\n",
      "epoch =  89  step =  100  of total steps  201  loss =  0.2925891876220703\n",
      "epoch =  89  step =  150  of total steps  201  loss =  0.36038970947265625\n",
      "epoch =  89  step =  200  of total steps  201  loss =  0.13217340409755707\n",
      "epoch :  89  /  100  | TL :  0.3102004316019182  | VL :  0.21996364183723927\n",
      "epoch =  90  step =  50  of total steps  201  loss =  1.362919569015503\n",
      "epoch =  90  step =  100  of total steps  201  loss =  0.29517319798469543\n",
      "epoch =  90  step =  150  of total steps  201  loss =  0.33574551343917847\n",
      "epoch =  90  step =  200  of total steps  201  loss =  0.29711467027664185\n",
      "epoch :  90  /  100  | TL :  0.3223458532966785  | VL :  0.23646545642986894\n",
      "epoch =  91  step =  50  of total steps  201  loss =  0.20453856885433197\n",
      "epoch =  91  step =  100  of total steps  201  loss =  0.23394078016281128\n",
      "epoch =  91  step =  150  of total steps  201  loss =  0.36868059635162354\n",
      "epoch =  91  step =  200  of total steps  201  loss =  0.2632615268230438\n",
      "epoch :  91  /  100  | TL :  0.317760366334844  | VL :  0.216023622546345\n",
      "saving model\n",
      "epoch =  92  step =  50  of total steps  201  loss =  0.18824920058250427\n",
      "epoch =  92  step =  100  of total steps  201  loss =  0.2129351943731308\n",
      "epoch =  92  step =  150  of total steps  201  loss =  0.3171725869178772\n",
      "epoch =  92  step =  200  of total steps  201  loss =  0.3439508378505707\n",
      "epoch :  92  /  100  | TL :  0.3028712416836871  | VL :  0.23336317762732506\n",
      "epoch =  93  step =  50  of total steps  201  loss =  0.6303055286407471\n",
      "epoch =  93  step =  100  of total steps  201  loss =  1.8286683559417725\n",
      "epoch =  93  step =  150  of total steps  201  loss =  0.22214655578136444\n",
      "epoch =  93  step =  200  of total steps  201  loss =  0.22748517990112305\n",
      "epoch :  93  /  100  | TL :  0.29612592237060936  | VL :  0.21799467084929347\n",
      "epoch =  94  step =  50  of total steps  201  loss =  0.37452220916748047\n",
      "epoch =  94  step =  100  of total steps  201  loss =  0.35241660475730896\n",
      "epoch =  94  step =  150  of total steps  201  loss =  0.29772064089775085\n",
      "epoch =  94  step =  200  of total steps  201  loss =  0.22948668897151947\n",
      "epoch :  94  /  100  | TL :  0.30074431406176505  | VL :  0.22906579915434122\n",
      "epoch =  95  step =  50  of total steps  201  loss =  0.15207456052303314\n",
      "epoch =  95  step =  100  of total steps  201  loss =  0.14256656169891357\n",
      "epoch =  95  step =  150  of total steps  201  loss =  0.25758036971092224\n",
      "epoch =  95  step =  200  of total steps  201  loss =  0.5116652250289917\n",
      "epoch :  95  /  100  | TL :  0.30232614842220323  | VL :  0.22160859173163772\n",
      "epoch =  96  step =  50  of total steps  201  loss =  0.171927347779274\n",
      "epoch =  96  step =  100  of total steps  201  loss =  0.35937264561653137\n",
      "epoch =  96  step =  150  of total steps  201  loss =  0.1204661875963211\n",
      "epoch =  96  step =  200  of total steps  201  loss =  0.1957198530435562\n",
      "epoch :  96  /  100  | TL :  0.2963289697816716  | VL :  0.22126971185207367\n",
      "epoch =  97  step =  50  of total steps  201  loss =  1.1092381477355957\n",
      "epoch =  97  step =  100  of total steps  201  loss =  0.2408420741558075\n",
      "epoch =  97  step =  150  of total steps  201  loss =  0.24326711893081665\n",
      "epoch =  97  step =  200  of total steps  201  loss =  0.19018729031085968\n",
      "epoch :  97  /  100  | TL :  0.29424630657802175  | VL :  0.2257811389863491\n",
      "epoch =  98  step =  50  of total steps  201  loss =  0.9095640182495117\n",
      "epoch =  98  step =  100  of total steps  201  loss =  0.23906771838665009\n",
      "epoch =  98  step =  150  of total steps  201  loss =  0.4966137111186981\n",
      "epoch =  98  step =  200  of total steps  201  loss =  0.18544450402259827\n",
      "epoch :  98  /  100  | TL :  0.30748737481103017  | VL :  0.23608849570155144\n",
      "epoch =  99  step =  50  of total steps  201  loss =  0.39526817202568054\n",
      "epoch =  99  step =  100  of total steps  201  loss =  0.30132460594177246\n",
      "epoch =  99  step =  150  of total steps  201  loss =  0.26162636280059814\n",
      "epoch =  99  step =  200  of total steps  201  loss =  0.22055239975452423\n",
      "epoch :  99  /  100  | TL :  0.2917176301206522  | VL :  0.21846559969708323\n",
      "epoch =  100  step =  50  of total steps  201  loss =  0.5993046760559082\n",
      "epoch =  100  step =  100  of total steps  201  loss =  0.14925126731395721\n",
      "epoch =  100  step =  150  of total steps  201  loss =  0.44312769174575806\n",
      "epoch =  100  step =  200  of total steps  201  loss =  0.4762505292892456\n",
      "epoch :  100  /  100  | TL :  0.2981526777889598  | VL :  0.22848335187882185\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "if hyper_params['stage'] == 0 : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage']) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "else : \n",
    "    filename = '../saved_models/stage' + str(hyper_params['stage'] + 1) + '/model' + str(hyper_params['repeated']) + '.pt'\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "        y_pred2 = mdl(images)\n",
    "\n",
    "        loss = F.mse_loss(sf2[hyper_params[\"stage\"]].features, sf[hyper_params[\"stage\"]].features)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch + 1, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            y_pred2 = mdl(images)\n",
    "            loss = F.mse_loss(sf[hyper_params[\"stage\"]].features, sf2[hyper_params[\"stage\"]].features)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "\n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = Learner(data, net, metrics = accuracy)\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model0.pt', map_location = 'cpu'))\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(hyper_params[\"num_epochs\"]), train_loss_list, label = 'train_loss') \n",
    "plt.plot(range(hyper_params[\"num_epochs\"]), val_loss_list, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/stage5/train_loss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier only after stage-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n",
      "0.0.weight torch.Size([64, 3, 7, 7])\n",
      "False\n",
      "0.2.weight torch.Size([64])\n",
      "False\n",
      "0.2.bias torch.Size([64])\n",
      "False\n",
      "2.0.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.0.2.weight torch.Size([64])\n",
      "False\n",
      "2.0.2.bias torch.Size([64])\n",
      "False\n",
      "2.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "False\n",
      "2.1.conv1.2.weight torch.Size([64])\n",
      "False\n",
      "2.1.conv1.2.bias torch.Size([64])\n",
      "False\n",
      "3.0.0.weight torch.Size([128, 64, 3, 3])\n",
      "False\n",
      "3.0.2.weight torch.Size([128])\n",
      "False\n",
      "3.0.2.bias torch.Size([128])\n",
      "False\n",
      "3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "False\n",
      "3.1.conv1.2.weight torch.Size([128])\n",
      "False\n",
      "3.1.conv1.2.bias torch.Size([128])\n",
      "False\n",
      "4.0.0.weight torch.Size([256, 128, 3, 3])\n",
      "False\n",
      "4.0.2.weight torch.Size([256])\n",
      "False\n",
      "4.0.2.bias torch.Size([256])\n",
      "False\n",
      "4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "False\n",
      "4.1.conv1.2.weight torch.Size([256])\n",
      "False\n",
      "4.1.conv1.2.bias torch.Size([256])\n",
      "False\n",
      "5.0.0.weight torch.Size([512, 256, 3, 3])\n",
      "False\n",
      "5.0.2.weight torch.Size([512])\n",
      "False\n",
      "5.0.2.bias torch.Size([512])\n",
      "False\n",
      "5.1.conv1.0.weight torch.Size([512, 512, 3, 3])\n",
      "False\n",
      "5.1.conv1.2.weight torch.Size([512])\n",
      "False\n",
      "5.1.conv1.2.bias torch.Size([512])\n",
      "False\n",
      "8.weight torch.Size([256, 1024])\n",
      "True\n",
      "8.bias torch.Size([256])\n",
      "True\n",
      "9.weight torch.Size([10, 256])\n",
      "True\n",
      "9.bias torch.Size([10])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    \"stage\": 5,\n",
    "    \"repeated\": 3,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4\n",
    "}\n",
    "\n",
    "class Flatten(nn.Module) :\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv2(ni, nf) : \n",
    "    return conv_layer(ni, nf, stride = 2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer(nf,nf)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return (x + self.conv1(x))\n",
    "\n",
    "def conv_and_res(ni, nf): \n",
    "    return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n",
    "\n",
    "def conv_(nf) : \n",
    "    return nn.Sequential(conv_layer(nf, nf), ResBlock(nf))\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    conv_layer(3, 64, ks = 7, stride = 2, padding = 3),\n",
    "    nn.MaxPool2d(3, 2, padding = 1),\n",
    "    conv_(64),\n",
    "    conv_and_res(64, 128),\n",
    "    conv_and_res(128, 256),\n",
    "    conv_and_res(256, 512),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(2 * 512, 256),\n",
    "    nn.Linear(256, hyper_params[\"num_classes\"])\n",
    ")\n",
    "\n",
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/stage5/model3.pt', map_location = 'cpu'))\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    net = net.cuda()\n",
    "    print('Model on GPU')\n",
    "    \n",
    "for name, param in net.named_parameters() : \n",
    "    print(name, param.shape)\n",
    "    param.requires_grad = False\n",
    "    if name[0] == '8' or name[0] == '9':\n",
    "        param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = torch.autograd.Variable(images).float()\n",
    "        labels = torch.autograd.Variable(labels).float()\n",
    "        \n",
    "        if torch.cuda.is_available() : \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = Net.forward(images)\n",
    "        outputs = F.log_softmax(outputs, dim = 1)\n",
    "\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        pred_ind = pred_ind.data.cpu().numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = labels - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akshaykvnit/kd0/29b62f58049d4e99949278bfe5013974\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  50  of total steps  201  loss =  0.8605021834373474\n",
      "epoch =  0  step =  100  of total steps  201  loss =  0.4823354184627533\n",
      "epoch =  0  step =  150  of total steps  201  loss =  0.42503225803375244\n",
      "epoch =  0  step =  200  of total steps  201  loss =  0.4054519534111023\n",
      "epoch :  1  /  100  | TL :  0.6975137760390097  | VL :  0.29722198378294706  | VA :  93.8\n",
      "saving model\n",
      "epoch =  1  step =  50  of total steps  201  loss =  0.3327433466911316\n",
      "epoch =  1  step =  100  of total steps  201  loss =  0.4114009737968445\n",
      "epoch =  1  step =  150  of total steps  201  loss =  0.35944247245788574\n",
      "epoch =  1  step =  200  of total steps  201  loss =  0.34988516569137573\n",
      "epoch :  2  /  100  | TL :  0.32773248636307406  | VL :  0.24840844329446554  | VA :  92.2\n",
      "epoch =  2  step =  50  of total steps  201  loss =  0.22623124718666077\n",
      "epoch =  2  step =  100  of total steps  201  loss =  0.2390250265598297\n",
      "epoch =  2  step =  150  of total steps  201  loss =  0.36456289887428284\n",
      "epoch =  2  step =  200  of total steps  201  loss =  0.3587043881416321\n",
      "epoch :  3  /  100  | TL :  0.2791830485881264  | VL :  0.21811397280544043  | VA :  93.0\n",
      "epoch =  3  step =  50  of total steps  201  loss =  0.3441251814365387\n",
      "epoch =  3  step =  100  of total steps  201  loss =  0.28291627764701843\n",
      "epoch =  3  step =  150  of total steps  201  loss =  0.33573445677757263\n",
      "epoch =  3  step =  200  of total steps  201  loss =  0.15834324061870575\n",
      "epoch :  4  /  100  | TL :  0.2758334277577661  | VL :  0.22920267470180988  | VA :  92.0\n",
      "epoch =  4  step =  50  of total steps  201  loss =  0.2642698585987091\n",
      "epoch =  4  step =  100  of total steps  201  loss =  0.35115620493888855\n",
      "epoch =  4  step =  150  of total steps  201  loss =  0.2212902009487152\n",
      "epoch =  4  step =  200  of total steps  201  loss =  0.22537681460380554\n",
      "epoch :  5  /  100  | TL :  0.2641844395826112  | VL :  0.2207262348383665  | VA :  93.0\n",
      "epoch =  5  step =  50  of total steps  201  loss =  0.3914414346218109\n",
      "epoch =  5  step =  100  of total steps  201  loss =  0.21498364210128784\n",
      "epoch =  5  step =  150  of total steps  201  loss =  0.3779449462890625\n",
      "epoch =  5  step =  200  of total steps  201  loss =  0.24528074264526367\n",
      "epoch :  6  /  100  | TL :  0.2599733169955104  | VL :  0.21632077125832438  | VA :  93.2\n",
      "epoch =  6  step =  50  of total steps  201  loss =  0.47462815046310425\n",
      "epoch =  6  step =  100  of total steps  201  loss =  0.29071715474128723\n",
      "epoch =  6  step =  150  of total steps  201  loss =  0.2575441300868988\n",
      "epoch =  6  step =  200  of total steps  201  loss =  0.2137860655784607\n",
      "epoch :  7  /  100  | TL :  0.2558794304617305  | VL :  0.21439151791855693  | VA :  92.80000000000001\n",
      "epoch =  7  step =  50  of total steps  201  loss =  0.12982448935508728\n",
      "epoch =  7  step =  100  of total steps  201  loss =  0.22436846792697906\n",
      "epoch =  7  step =  150  of total steps  201  loss =  0.19400322437286377\n",
      "epoch =  7  step =  200  of total steps  201  loss =  0.2154277116060257\n",
      "epoch :  8  /  100  | TL :  0.2499224913135097  | VL :  0.21945806499570608  | VA :  92.2\n",
      "epoch =  8  step =  50  of total steps  201  loss =  0.17324292659759521\n",
      "epoch =  8  step =  100  of total steps  201  loss =  0.19730597734451294\n",
      "epoch =  8  step =  150  of total steps  201  loss =  0.23856034874916077\n",
      "epoch =  8  step =  200  of total steps  201  loss =  0.3409159183502197\n",
      "epoch :  9  /  100  | TL :  0.24692055545932617  | VL :  0.2061937702819705  | VA :  93.2\n",
      "epoch =  9  step =  50  of total steps  201  loss =  0.23737777769565582\n",
      "epoch =  9  step =  100  of total steps  201  loss =  0.3139788806438446\n",
      "epoch =  9  step =  150  of total steps  201  loss =  0.20926058292388916\n",
      "epoch =  9  step =  200  of total steps  201  loss =  0.31798598170280457\n",
      "epoch :  10  /  100  | TL :  0.24733726355937583  | VL :  0.2197615373879671  | VA :  92.60000000000001\n",
      "epoch =  10  step =  50  of total steps  201  loss =  0.1995837688446045\n",
      "epoch =  10  step =  100  of total steps  201  loss =  0.0933903232216835\n",
      "epoch =  10  step =  150  of total steps  201  loss =  0.18390733003616333\n",
      "epoch =  10  step =  200  of total steps  201  loss =  0.15997394919395447\n",
      "epoch :  11  /  100  | TL :  0.23991858356850065  | VL :  0.21485727466642857  | VA :  93.4\n",
      "epoch =  11  step =  50  of total steps  201  loss =  0.13218989968299866\n",
      "epoch =  11  step =  100  of total steps  201  loss =  0.27991828322410583\n",
      "epoch =  11  step =  150  of total steps  201  loss =  0.34463372826576233\n",
      "epoch =  11  step =  200  of total steps  201  loss =  0.597347617149353\n",
      "epoch :  12  /  100  | TL :  0.2465387171699633  | VL :  0.21032472932711244  | VA :  93.2\n",
      "epoch =  12  step =  50  of total steps  201  loss =  0.21402865648269653\n",
      "epoch =  12  step =  100  of total steps  201  loss =  0.21215753257274628\n",
      "epoch =  12  step =  150  of total steps  201  loss =  0.1719476878643036\n",
      "epoch =  12  step =  200  of total steps  201  loss =  0.1425202488899231\n",
      "epoch :  13  /  100  | TL :  0.23874446319703438  | VL :  0.21504375711083412  | VA :  92.80000000000001\n",
      "epoch =  13  step =  50  of total steps  201  loss =  0.24468782544136047\n",
      "epoch =  13  step =  100  of total steps  201  loss =  0.17790797352790833\n",
      "epoch =  13  step =  150  of total steps  201  loss =  0.21676714718341827\n",
      "epoch =  13  step =  200  of total steps  201  loss =  0.2565978169441223\n",
      "epoch :  14  /  100  | TL :  0.23829422854072418  | VL :  0.21841336879879236  | VA :  91.8\n",
      "epoch =  14  step =  50  of total steps  201  loss =  0.18019326031208038\n",
      "epoch =  14  step =  100  of total steps  201  loss =  0.1889965981245041\n",
      "epoch =  14  step =  150  of total steps  201  loss =  0.4977933168411255\n",
      "epoch =  14  step =  200  of total steps  201  loss =  0.18359830975532532\n",
      "epoch :  15  /  100  | TL :  0.23888193663969562  | VL :  0.22544503631070256  | VA :  92.80000000000001\n",
      "epoch =  15  step =  50  of total steps  201  loss =  0.34361159801483154\n",
      "epoch =  15  step =  100  of total steps  201  loss =  0.24675585329532623\n",
      "epoch =  15  step =  150  of total steps  201  loss =  0.39987102150917053\n",
      "epoch =  15  step =  200  of total steps  201  loss =  0.1723470836877823\n",
      "epoch :  16  /  100  | TL :  0.2418065157288046  | VL :  0.21578491106629372  | VA :  92.0\n",
      "epoch =  16  step =  50  of total steps  201  loss =  0.022129088640213013\n",
      "epoch =  16  step =  100  of total steps  201  loss =  0.30307409167289734\n",
      "epoch =  16  step =  150  of total steps  201  loss =  0.17756882309913635\n",
      "epoch =  16  step =  200  of total steps  201  loss =  0.09729719161987305\n",
      "epoch :  17  /  100  | TL :  0.24029266074373948  | VL :  0.2137321517802775  | VA :  92.60000000000001\n",
      "epoch =  17  step =  50  of total steps  201  loss =  0.333645761013031\n",
      "epoch =  17  step =  100  of total steps  201  loss =  0.39898818731307983\n",
      "epoch =  17  step =  150  of total steps  201  loss =  0.3330373764038086\n",
      "epoch =  17  step =  200  of total steps  201  loss =  0.22934287786483765\n",
      "epoch :  18  /  100  | TL :  0.23815854438650075  | VL :  0.21598948119208217  | VA :  92.80000000000001\n",
      "epoch =  18  step =  50  of total steps  201  loss =  0.5008154511451721\n",
      "epoch =  18  step =  100  of total steps  201  loss =  0.19042478501796722\n",
      "epoch =  18  step =  150  of total steps  201  loss =  0.28100836277008057\n",
      "epoch =  18  step =  200  of total steps  201  loss =  0.1040240079164505\n",
      "epoch :  19  /  100  | TL :  0.23346490952283588  | VL :  0.22138324426487088  | VA :  92.4\n",
      "epoch =  19  step =  50  of total steps  201  loss =  0.3482971787452698\n",
      "epoch =  19  step =  100  of total steps  201  loss =  0.23363550007343292\n",
      "epoch =  19  step =  150  of total steps  201  loss =  0.11996384710073471\n",
      "epoch =  19  step =  200  of total steps  201  loss =  0.28058114647865295\n",
      "epoch :  20  /  100  | TL :  0.23394425698922047  | VL :  0.2358958823606372  | VA :  92.0\n",
      "epoch =  20  step =  50  of total steps  201  loss =  0.2590339481830597\n",
      "epoch =  20  step =  100  of total steps  201  loss =  0.1282569169998169\n",
      "epoch =  20  step =  150  of total steps  201  loss =  0.30402740836143494\n",
      "epoch =  20  step =  200  of total steps  201  loss =  0.4920940697193146\n",
      "epoch :  21  /  100  | TL :  0.23556592267247575  | VL :  0.2256129253655672  | VA :  93.0\n",
      "epoch =  21  step =  50  of total steps  201  loss =  0.34961259365081787\n",
      "epoch =  21  step =  100  of total steps  201  loss =  0.36592766642570496\n",
      "epoch =  21  step =  150  of total steps  201  loss =  0.14360937476158142\n",
      "epoch =  21  step =  200  of total steps  201  loss =  0.22530367970466614\n",
      "epoch :  22  /  100  | TL :  0.2352858996806453  | VL :  0.20845643896609545  | VA :  93.0\n",
      "epoch =  22  step =  50  of total steps  201  loss =  0.1391202211380005\n",
      "epoch =  22  step =  100  of total steps  201  loss =  0.27242952585220337\n",
      "epoch =  22  step =  150  of total steps  201  loss =  0.2624325156211853\n",
      "epoch =  22  step =  200  of total steps  201  loss =  0.2185412049293518\n",
      "epoch :  23  /  100  | TL :  0.21943515251895682  | VL :  0.21714520920068026  | VA :  92.80000000000001\n",
      "epoch =  23  step =  50  of total steps  201  loss =  0.24651022255420685\n",
      "epoch =  23  step =  100  of total steps  201  loss =  0.16030579805374146\n",
      "epoch =  23  step =  150  of total steps  201  loss =  0.13907970488071442\n",
      "epoch =  23  step =  200  of total steps  201  loss =  0.15136626362800598\n",
      "epoch :  24  /  100  | TL :  0.23488355793095939  | VL :  0.22300725243985653  | VA :  93.4\n",
      "epoch =  24  step =  50  of total steps  201  loss =  0.3725248873233795\n",
      "epoch =  24  step =  100  of total steps  201  loss =  0.1638016402721405\n",
      "epoch =  24  step =  150  of total steps  201  loss =  0.10492376238107681\n",
      "epoch =  24  step =  200  of total steps  201  loss =  0.12980899214744568\n",
      "epoch :  25  /  100  | TL :  0.23726140218439387  | VL :  0.22433972638100386  | VA :  92.4\n",
      "epoch =  25  step =  50  of total steps  201  loss =  0.255475252866745\n",
      "epoch =  25  step =  100  of total steps  201  loss =  0.20146377384662628\n",
      "epoch =  25  step =  150  of total steps  201  loss =  0.3340805768966675\n",
      "epoch =  25  step =  200  of total steps  201  loss =  0.20245236158370972\n",
      "epoch :  26  /  100  | TL :  0.22928317045023786  | VL :  0.22025948762893677  | VA :  92.60000000000001\n",
      "epoch =  26  step =  50  of total steps  201  loss =  0.15499362349510193\n",
      "epoch =  26  step =  100  of total steps  201  loss =  0.2022053450345993\n",
      "epoch =  26  step =  150  of total steps  201  loss =  0.3296336233615875\n",
      "epoch =  26  step =  200  of total steps  201  loss =  0.12313896417617798\n",
      "epoch :  27  /  100  | TL :  0.23275534137712783  | VL :  0.22125418111681938  | VA :  92.80000000000001\n",
      "epoch =  27  step =  50  of total steps  201  loss =  0.4195234775543213\n",
      "epoch =  27  step =  100  of total steps  201  loss =  0.3799945116043091\n",
      "epoch =  27  step =  150  of total steps  201  loss =  0.09231217205524445\n",
      "epoch =  27  step =  200  of total steps  201  loss =  0.08676744252443314\n",
      "epoch :  28  /  100  | TL :  0.22226090086114347  | VL :  0.22166707087308168  | VA :  92.2\n",
      "epoch =  28  step =  50  of total steps  201  loss =  0.39229685068130493\n",
      "epoch =  28  step =  100  of total steps  201  loss =  0.0916348323225975\n",
      "epoch =  28  step =  150  of total steps  201  loss =  0.2447509914636612\n",
      "epoch =  28  step =  200  of total steps  201  loss =  0.4415096342563629\n",
      "epoch :  29  /  100  | TL :  0.22419144289188125  | VL :  0.2226794520393014  | VA :  93.2\n",
      "epoch =  29  step =  50  of total steps  201  loss =  0.16776560246944427\n",
      "epoch =  29  step =  100  of total steps  201  loss =  0.27328410744667053\n",
      "epoch =  29  step =  150  of total steps  201  loss =  0.15271395444869995\n",
      "epoch =  29  step =  200  of total steps  201  loss =  0.264689564704895\n",
      "epoch :  30  /  100  | TL :  0.2265617732533175  | VL :  0.22320317663252354  | VA :  92.80000000000001\n",
      "epoch =  30  step =  50  of total steps  201  loss =  0.30538660287857056\n",
      "epoch =  30  step =  100  of total steps  201  loss =  0.3181937634944916\n",
      "epoch =  30  step =  150  of total steps  201  loss =  0.21314719319343567\n",
      "epoch =  30  step =  200  of total steps  201  loss =  0.289995014667511\n",
      "epoch :  31  /  100  | TL :  0.2286860583470532  | VL :  0.22408388555049896  | VA :  92.60000000000001\n",
      "epoch =  31  step =  50  of total steps  201  loss =  0.32401198148727417\n",
      "epoch =  31  step =  100  of total steps  201  loss =  0.25939759612083435\n",
      "epoch =  31  step =  150  of total steps  201  loss =  0.27904441952705383\n",
      "epoch =  31  step =  200  of total steps  201  loss =  0.15245211124420166\n",
      "epoch :  32  /  100  | TL :  0.22549229257957853  | VL :  0.2086065085604787  | VA :  92.4\n",
      "epoch =  32  step =  50  of total steps  201  loss =  0.41276752948760986\n",
      "epoch =  32  step =  100  of total steps  201  loss =  0.2988084852695465\n",
      "epoch =  32  step =  150  of total steps  201  loss =  0.1459382176399231\n",
      "epoch =  32  step =  200  of total steps  201  loss =  0.19616299867630005\n",
      "epoch :  33  /  100  | TL :  0.23006638156166717  | VL :  0.22088641859591007  | VA :  92.60000000000001\n",
      "epoch =  33  step =  50  of total steps  201  loss =  0.34489184617996216\n",
      "epoch =  33  step =  100  of total steps  201  loss =  0.162724569439888\n",
      "epoch =  33  step =  150  of total steps  201  loss =  0.12353485077619553\n",
      "epoch =  33  step =  200  of total steps  201  loss =  0.19383984804153442\n",
      "epoch :  34  /  100  | TL :  0.23053363462289175  | VL :  0.2122946698218584  | VA :  92.80000000000001\n",
      "epoch =  34  step =  50  of total steps  201  loss =  0.339540034532547\n",
      "epoch =  34  step =  100  of total steps  201  loss =  0.12754422426223755\n",
      "epoch =  34  step =  150  of total steps  201  loss =  0.14169614017009735\n",
      "epoch =  34  step =  200  of total steps  201  loss =  0.17448976635932922\n",
      "epoch :  35  /  100  | TL :  0.22754321154670334  | VL :  0.2251152116805315  | VA :  92.4\n",
      "epoch =  35  step =  50  of total steps  201  loss =  0.23882606625556946\n",
      "epoch =  35  step =  100  of total steps  201  loss =  0.2669126093387604\n",
      "epoch =  35  step =  150  of total steps  201  loss =  0.1348879039287567\n",
      "epoch =  35  step =  200  of total steps  201  loss =  0.30750367045402527\n",
      "epoch :  36  /  100  | TL :  0.21723409806985167  | VL :  0.23053952772170305  | VA :  92.60000000000001\n",
      "epoch =  36  step =  50  of total steps  201  loss =  0.11471601575613022\n",
      "epoch =  36  step =  100  of total steps  201  loss =  0.2632389962673187\n",
      "epoch =  36  step =  150  of total steps  201  loss =  0.24942950904369354\n",
      "epoch =  36  step =  200  of total steps  201  loss =  0.13438338041305542\n",
      "epoch :  37  /  100  | TL :  0.2263077964162945  | VL :  0.2507745996117592  | VA :  92.0\n",
      "epoch =  37  step =  50  of total steps  201  loss =  0.24452736973762512\n",
      "epoch =  37  step =  100  of total steps  201  loss =  0.10298436880111694\n",
      "epoch =  37  step =  150  of total steps  201  loss =  0.14836899936199188\n",
      "epoch =  37  step =  200  of total steps  201  loss =  0.11336924880743027\n",
      "epoch :  38  /  100  | TL :  0.21953208824221174  | VL :  0.23374854028224945  | VA :  92.2\n",
      "epoch =  38  step =  50  of total steps  201  loss =  0.2699812650680542\n",
      "epoch =  38  step =  100  of total steps  201  loss =  0.23516017198562622\n",
      "epoch =  38  step =  150  of total steps  201  loss =  0.14494439959526062\n",
      "epoch =  38  step =  200  of total steps  201  loss =  0.13070917129516602\n",
      "epoch :  39  /  100  | TL :  0.22775579293939605  | VL :  0.21439619082957506  | VA :  92.60000000000001\n",
      "epoch =  39  step =  50  of total steps  201  loss =  0.09213055670261383\n",
      "epoch =  39  step =  100  of total steps  201  loss =  0.31547990441322327\n",
      "epoch =  39  step =  150  of total steps  201  loss =  0.334818959236145\n",
      "epoch =  39  step =  200  of total steps  201  loss =  0.25117185711860657\n",
      "epoch :  40  /  100  | TL :  0.21494018799852377  | VL :  0.20628464967012405  | VA :  93.0\n",
      "epoch =  40  step =  50  of total steps  201  loss =  0.052021000534296036\n",
      "epoch =  40  step =  100  of total steps  201  loss =  0.07105865329504013\n",
      "epoch =  40  step =  150  of total steps  201  loss =  0.1381930559873581\n",
      "epoch =  40  step =  200  of total steps  201  loss =  0.20089538395404816\n",
      "epoch :  41  /  100  | TL :  0.22792881999665232  | VL :  0.20808778703212738  | VA :  92.80000000000001\n",
      "epoch =  41  step =  50  of total steps  201  loss =  0.2096603661775589\n",
      "epoch =  41  step =  100  of total steps  201  loss =  0.16433165967464447\n",
      "epoch =  41  step =  150  of total steps  201  loss =  0.16003531217575073\n",
      "epoch =  41  step =  200  of total steps  201  loss =  0.3269841969013214\n",
      "epoch :  42  /  100  | TL :  0.22544137117874563  | VL :  0.20752981770783663  | VA :  92.80000000000001\n",
      "epoch =  42  step =  50  of total steps  201  loss =  0.2685203552246094\n",
      "epoch =  42  step =  100  of total steps  201  loss =  0.312049001455307\n",
      "epoch =  42  step =  150  of total steps  201  loss =  0.09876894950866699\n",
      "epoch =  42  step =  200  of total steps  201  loss =  0.4285227358341217\n",
      "epoch :  43  /  100  | TL :  0.21220552050207384  | VL :  0.2295936718583107  | VA :  92.2\n",
      "epoch =  43  step =  50  of total steps  201  loss =  0.20464694499969482\n",
      "epoch =  43  step =  100  of total steps  201  loss =  0.42751315236091614\n",
      "epoch =  43  step =  150  of total steps  201  loss =  0.1182766705751419\n",
      "epoch =  43  step =  200  of total steps  201  loss =  0.12093503773212433\n",
      "epoch :  44  /  100  | TL :  0.21817420993871356  | VL :  0.22747873701155186  | VA :  92.60000000000001\n",
      "epoch =  44  step =  50  of total steps  201  loss =  0.22870399057865143\n",
      "epoch =  44  step =  100  of total steps  201  loss =  0.36159032583236694\n",
      "epoch =  44  step =  150  of total steps  201  loss =  0.05387438088655472\n",
      "epoch =  44  step =  200  of total steps  201  loss =  0.18605375289916992\n",
      "epoch :  45  /  100  | TL :  0.21861885777160303  | VL :  0.20625638775527477  | VA :  93.4\n",
      "epoch =  45  step =  50  of total steps  201  loss =  0.26070138812065125\n",
      "epoch =  45  step =  100  of total steps  201  loss =  0.21584409475326538\n",
      "epoch =  45  step =  150  of total steps  201  loss =  0.1645393967628479\n",
      "epoch =  45  step =  200  of total steps  201  loss =  0.22403579950332642\n",
      "epoch :  46  /  100  | TL :  0.22824053227234242  | VL :  0.22285156324505806  | VA :  92.2\n",
      "epoch =  46  step =  50  of total steps  201  loss =  0.416281521320343\n",
      "epoch =  46  step =  100  of total steps  201  loss =  0.1734999418258667\n",
      "epoch =  46  step =  150  of total steps  201  loss =  0.192703515291214\n",
      "epoch =  46  step =  200  of total steps  201  loss =  0.2065160572528839\n",
      "epoch :  47  /  100  | TL :  0.21756175620045828  | VL :  0.2264403561130166  | VA :  92.2\n",
      "epoch =  47  step =  50  of total steps  201  loss =  0.291595458984375\n",
      "epoch =  47  step =  100  of total steps  201  loss =  0.3121884763240814\n",
      "epoch =  47  step =  150  of total steps  201  loss =  0.10462312400341034\n",
      "epoch =  47  step =  200  of total steps  201  loss =  0.26681262254714966\n",
      "epoch :  48  /  100  | TL :  0.21988115504161634  | VL :  0.21393451932817698  | VA :  93.2\n",
      "epoch =  48  step =  50  of total steps  201  loss =  0.2982279360294342\n",
      "epoch =  48  step =  100  of total steps  201  loss =  0.23750324547290802\n",
      "epoch =  48  step =  150  of total steps  201  loss =  0.2190147340297699\n",
      "epoch =  48  step =  200  of total steps  201  loss =  0.14828644692897797\n",
      "epoch :  49  /  100  | TL :  0.21815755457353236  | VL :  0.2231615069322288  | VA :  92.4\n",
      "epoch =  49  step =  50  of total steps  201  loss =  0.3358266353607178\n",
      "epoch =  49  step =  100  of total steps  201  loss =  0.21193145215511322\n",
      "epoch =  49  step =  150  of total steps  201  loss =  0.19050054252147675\n",
      "epoch =  49  step =  200  of total steps  201  loss =  0.5957761406898499\n",
      "epoch :  50  /  100  | TL :  0.21520148254745636  | VL :  0.21805996168404818  | VA :  92.80000000000001\n",
      "epoch =  50  step =  50  of total steps  201  loss =  0.2654731869697571\n",
      "epoch =  50  step =  100  of total steps  201  loss =  0.07076113671064377\n",
      "epoch =  50  step =  150  of total steps  201  loss =  0.2936592400074005\n",
      "epoch =  50  step =  200  of total steps  201  loss =  0.2033647894859314\n",
      "epoch :  51  /  100  | TL :  0.22093130021694288  | VL :  0.22760869283229113  | VA :  93.60000000000001\n",
      "epoch =  51  step =  50  of total steps  201  loss =  0.18077388405799866\n",
      "epoch =  51  step =  100  of total steps  201  loss =  0.2455669343471527\n",
      "epoch =  51  step =  150  of total steps  201  loss =  0.1926443874835968\n",
      "epoch =  51  step =  200  of total steps  201  loss =  0.21252280473709106\n",
      "epoch :  52  /  100  | TL :  0.2202064628847203  | VL :  0.2152837417088449  | VA :  93.2\n",
      "epoch =  52  step =  50  of total steps  201  loss =  0.3884263336658478\n",
      "epoch =  52  step =  100  of total steps  201  loss =  0.16534975171089172\n",
      "epoch =  52  step =  150  of total steps  201  loss =  0.15925636887550354\n",
      "epoch =  52  step =  200  of total steps  201  loss =  0.1503724902868271\n",
      "epoch :  53  /  100  | TL :  0.2142174876855677  | VL :  0.21609236020594835  | VA :  93.0\n",
      "epoch =  53  step =  50  of total steps  201  loss =  0.16104280948638916\n",
      "epoch =  53  step =  100  of total steps  201  loss =  0.20055106282234192\n",
      "epoch =  53  step =  150  of total steps  201  loss =  0.09723889082670212\n",
      "epoch =  53  step =  200  of total steps  201  loss =  0.24375995993614197\n",
      "epoch :  54  /  100  | TL :  0.20565482896210543  | VL :  0.22362896148115396  | VA :  93.0\n",
      "epoch =  54  step =  50  of total steps  201  loss =  0.2579188346862793\n",
      "epoch =  54  step =  100  of total steps  201  loss =  0.16841797530651093\n",
      "epoch =  54  step =  150  of total steps  201  loss =  0.09713172167539597\n",
      "epoch =  54  step =  200  of total steps  201  loss =  0.17172178626060486\n",
      "epoch :  55  /  100  | TL :  0.21858713803673857  | VL :  0.2291319938376546  | VA :  92.2\n",
      "epoch =  55  step =  50  of total steps  201  loss =  0.1917172074317932\n",
      "epoch =  55  step =  100  of total steps  201  loss =  0.1104455292224884\n",
      "epoch =  55  step =  150  of total steps  201  loss =  0.34224939346313477\n",
      "epoch =  55  step =  200  of total steps  201  loss =  0.27915138006210327\n",
      "epoch :  56  /  100  | TL :  0.22148966403742928  | VL :  0.21778255701065063  | VA :  92.80000000000001\n",
      "epoch =  56  step =  50  of total steps  201  loss =  0.18691013753414154\n",
      "epoch =  56  step =  100  of total steps  201  loss =  0.19161748886108398\n",
      "epoch =  56  step =  150  of total steps  201  loss =  0.06738681346178055\n",
      "epoch =  56  step =  200  of total steps  201  loss =  0.2401604950428009\n",
      "epoch :  57  /  100  | TL :  0.2117205695133304  | VL :  0.2284459718503058  | VA :  92.4\n",
      "epoch =  57  step =  50  of total steps  201  loss =  0.28092503547668457\n",
      "epoch =  57  step =  100  of total steps  201  loss =  0.17239797115325928\n",
      "epoch =  57  step =  150  of total steps  201  loss =  0.1299731731414795\n",
      "epoch =  57  step =  200  of total steps  201  loss =  0.24767298996448517\n",
      "epoch :  58  /  100  | TL :  0.21608760719423864  | VL :  0.22498605586588383  | VA :  92.4\n",
      "epoch =  58  step =  50  of total steps  201  loss =  0.2836681604385376\n",
      "epoch =  58  step =  100  of total steps  201  loss =  0.20776335895061493\n",
      "epoch =  58  step =  150  of total steps  201  loss =  0.216918483376503\n",
      "epoch =  58  step =  200  of total steps  201  loss =  0.28914517164230347\n",
      "epoch :  59  /  100  | TL :  0.21803759516619925  | VL :  0.2207079604268074  | VA :  92.60000000000001\n",
      "epoch =  59  step =  50  of total steps  201  loss =  0.19201114773750305\n",
      "epoch =  59  step =  100  of total steps  201  loss =  0.07138589024543762\n",
      "epoch =  59  step =  150  of total steps  201  loss =  0.17820921540260315\n",
      "epoch =  59  step =  200  of total steps  201  loss =  0.18870970606803894\n",
      "epoch :  60  /  100  | TL :  0.20928277082704194  | VL :  0.21565290540456772  | VA :  93.0\n",
      "epoch =  60  step =  50  of total steps  201  loss =  0.1769704669713974\n",
      "epoch =  60  step =  100  of total steps  201  loss =  0.28953123092651367\n",
      "epoch =  60  step =  150  of total steps  201  loss =  0.25344258546829224\n",
      "epoch =  60  step =  200  of total steps  201  loss =  0.1915803849697113\n",
      "epoch :  61  /  100  | TL :  0.21744137261044327  | VL :  0.22485547978430986  | VA :  92.4\n",
      "epoch =  61  step =  50  of total steps  201  loss =  0.3333417773246765\n",
      "epoch =  61  step =  100  of total steps  201  loss =  0.144189715385437\n",
      "epoch =  61  step =  150  of total steps  201  loss =  0.12881778180599213\n",
      "epoch =  61  step =  200  of total steps  201  loss =  0.16269418597221375\n",
      "epoch :  62  /  100  | TL :  0.2177966427595461  | VL :  0.2104581231251359  | VA :  93.0\n",
      "epoch =  62  step =  50  of total steps  201  loss =  0.1333276480436325\n",
      "epoch =  62  step =  100  of total steps  201  loss =  0.29234158992767334\n",
      "epoch =  62  step =  150  of total steps  201  loss =  0.18165385723114014\n",
      "epoch =  62  step =  200  of total steps  201  loss =  0.14804910123348236\n",
      "epoch :  63  /  100  | TL :  0.21546987397827913  | VL :  0.22841631574556231  | VA :  92.4\n",
      "epoch =  63  step =  50  of total steps  201  loss =  0.23997241258621216\n",
      "epoch =  63  step =  100  of total steps  201  loss =  0.18280941247940063\n",
      "epoch =  63  step =  150  of total steps  201  loss =  0.18118789792060852\n",
      "epoch =  63  step =  200  of total steps  201  loss =  0.20223203301429749\n",
      "epoch :  64  /  100  | TL :  0.21719172576544296  | VL :  0.22419441305100918  | VA :  92.4\n",
      "epoch =  64  step =  50  of total steps  201  loss =  0.21017895638942719\n",
      "epoch =  64  step =  100  of total steps  201  loss =  0.27946120500564575\n",
      "epoch =  64  step =  150  of total steps  201  loss =  0.18104198575019836\n",
      "epoch =  64  step =  200  of total steps  201  loss =  0.09440480917692184\n",
      "epoch :  65  /  100  | TL :  0.21749934643062194  | VL :  0.21916726557537913  | VA :  92.2\n",
      "epoch =  65  step =  50  of total steps  201  loss =  0.17396895587444305\n",
      "epoch =  65  step =  100  of total steps  201  loss =  0.18588674068450928\n",
      "epoch =  65  step =  150  of total steps  201  loss =  0.16082359850406647\n",
      "epoch =  65  step =  200  of total steps  201  loss =  0.23129048943519592\n",
      "epoch :  66  /  100  | TL :  0.21281435755799658  | VL :  0.21577622462064028  | VA :  92.80000000000001\n",
      "epoch =  66  step =  50  of total steps  201  loss =  0.17683108150959015\n",
      "epoch =  66  step =  100  of total steps  201  loss =  0.15186068415641785\n",
      "epoch =  66  step =  150  of total steps  201  loss =  0.13808977603912354\n",
      "epoch =  66  step =  200  of total steps  201  loss =  0.25855544209480286\n",
      "epoch :  67  /  100  | TL :  0.21215427694703215  | VL :  0.22336095152422786  | VA :  91.8\n",
      "epoch =  67  step =  50  of total steps  201  loss =  0.07317440956830978\n",
      "epoch =  67  step =  100  of total steps  201  loss =  0.20823624730110168\n",
      "epoch =  67  step =  150  of total steps  201  loss =  0.11879788339138031\n",
      "epoch =  67  step =  200  of total steps  201  loss =  0.1553768813610077\n",
      "epoch :  68  /  100  | TL :  0.20572228824247174  | VL :  0.22614010609686375  | VA :  91.4\n",
      "epoch =  68  step =  50  of total steps  201  loss =  0.18065650761127472\n",
      "epoch =  68  step =  100  of total steps  201  loss =  0.2120911180973053\n",
      "epoch =  68  step =  150  of total steps  201  loss =  0.27641886472702026\n",
      "epoch =  68  step =  200  of total steps  201  loss =  0.3476862609386444\n",
      "epoch :  69  /  100  | TL :  0.21230200876421595  | VL :  0.21756866993382573  | VA :  92.60000000000001\n",
      "epoch =  69  step =  50  of total steps  201  loss =  0.24446305632591248\n",
      "epoch =  69  step =  100  of total steps  201  loss =  0.30281031131744385\n",
      "epoch =  69  step =  150  of total steps  201  loss =  0.17391563951969147\n",
      "epoch =  69  step =  200  of total steps  201  loss =  0.08276718854904175\n",
      "epoch :  70  /  100  | TL :  0.20575653778305694  | VL :  0.20943471882492304  | VA :  93.2\n",
      "epoch =  70  step =  50  of total steps  201  loss =  0.0944705680012703\n",
      "epoch =  70  step =  100  of total steps  201  loss =  0.07961170375347137\n",
      "epoch =  70  step =  150  of total steps  201  loss =  0.21267735958099365\n",
      "epoch =  70  step =  200  of total steps  201  loss =  0.1912517547607422\n",
      "epoch :  71  /  100  | TL :  0.21100289367176406  | VL :  0.23531039245426655  | VA :  92.2\n",
      "epoch =  71  step =  50  of total steps  201  loss =  0.18985918164253235\n",
      "epoch =  71  step =  100  of total steps  201  loss =  0.21138346195220947\n",
      "epoch =  71  step =  150  of total steps  201  loss =  0.1262834072113037\n",
      "epoch =  71  step =  200  of total steps  201  loss =  0.21421770751476288\n",
      "epoch :  72  /  100  | TL :  0.21614671648660702  | VL :  0.21209061658009887  | VA :  93.2\n",
      "epoch =  72  step =  50  of total steps  201  loss =  0.18677785992622375\n",
      "epoch =  72  step =  100  of total steps  201  loss =  0.25770097970962524\n",
      "epoch =  72  step =  150  of total steps  201  loss =  0.26926660537719727\n",
      "epoch =  72  step =  200  of total steps  201  loss =  0.28091293573379517\n",
      "epoch :  73  /  100  | TL :  0.2082543729697887  | VL :  0.2235303451307118  | VA :  92.80000000000001\n",
      "epoch =  73  step =  50  of total steps  201  loss =  0.17151236534118652\n",
      "epoch =  73  step =  100  of total steps  201  loss =  0.19491204619407654\n",
      "epoch =  73  step =  150  of total steps  201  loss =  0.24532856047153473\n",
      "epoch =  73  step =  200  of total steps  201  loss =  0.3232874870300293\n",
      "epoch :  74  /  100  | TL :  0.21060266032518438  | VL :  0.22558353887870908  | VA :  92.2\n",
      "epoch =  74  step =  50  of total steps  201  loss =  0.14356231689453125\n",
      "epoch =  74  step =  100  of total steps  201  loss =  0.28075772523880005\n",
      "epoch =  74  step =  150  of total steps  201  loss =  0.07519910484552383\n",
      "epoch =  74  step =  200  of total steps  201  loss =  0.38788750767707825\n",
      "epoch :  75  /  100  | TL :  0.2198769312111003  | VL :  0.22951475251466036  | VA :  92.0\n",
      "epoch =  75  step =  50  of total steps  201  loss =  0.17488346993923187\n",
      "epoch =  75  step =  100  of total steps  201  loss =  0.17523139715194702\n",
      "epoch =  75  step =  150  of total steps  201  loss =  0.2323533594608307\n",
      "epoch =  75  step =  200  of total steps  201  loss =  0.219395250082016\n",
      "epoch :  76  /  100  | TL :  0.2067797794122601  | VL :  0.21791109908372164  | VA :  92.80000000000001\n",
      "epoch =  76  step =  50  of total steps  201  loss =  0.2694617807865143\n",
      "epoch =  76  step =  100  of total steps  201  loss =  0.19884580373764038\n",
      "epoch =  76  step =  150  of total steps  201  loss =  0.23579667508602142\n",
      "epoch =  76  step =  200  of total steps  201  loss =  0.13792239129543304\n",
      "epoch :  77  /  100  | TL :  0.2082736003421136  | VL :  0.22396691981703043  | VA :  92.80000000000001\n",
      "epoch =  77  step =  50  of total steps  201  loss =  0.2092711627483368\n",
      "epoch =  77  step =  100  of total steps  201  loss =  0.2867559790611267\n",
      "epoch =  77  step =  150  of total steps  201  loss =  0.19536888599395752\n",
      "epoch =  77  step =  200  of total steps  201  loss =  0.3286011815071106\n",
      "epoch :  78  /  100  | TL :  0.205061453677232  | VL :  0.23042046278715134  | VA :  92.60000000000001\n",
      "epoch =  78  step =  50  of total steps  201  loss =  0.20537510514259338\n",
      "epoch =  78  step =  100  of total steps  201  loss =  0.14318199455738068\n",
      "epoch =  78  step =  150  of total steps  201  loss =  0.37473469972610474\n",
      "epoch =  78  step =  200  of total steps  201  loss =  0.28022730350494385\n",
      "epoch :  79  /  100  | TL :  0.213494157817085  | VL :  0.225599919911474  | VA :  92.60000000000001\n",
      "epoch =  79  step =  50  of total steps  201  loss =  0.24928951263427734\n",
      "epoch =  79  step =  100  of total steps  201  loss =  0.3101727366447449\n",
      "epoch =  79  step =  150  of total steps  201  loss =  0.21320727467536926\n",
      "epoch =  79  step =  200  of total steps  201  loss =  0.1716231107711792\n",
      "epoch :  80  /  100  | TL :  0.21388874614416664  | VL :  0.2245024088770151  | VA :  92.60000000000001\n",
      "epoch =  80  step =  50  of total steps  201  loss =  0.19956505298614502\n",
      "epoch =  80  step =  100  of total steps  201  loss =  0.21733781695365906\n",
      "epoch =  80  step =  150  of total steps  201  loss =  0.22679829597473145\n",
      "epoch =  80  step =  200  of total steps  201  loss =  0.23212534189224243\n",
      "epoch :  81  /  100  | TL :  0.2176493759920348  | VL :  0.22186923446133733  | VA :  92.2\n",
      "epoch =  81  step =  50  of total steps  201  loss =  0.123618483543396\n",
      "epoch =  81  step =  100  of total steps  201  loss =  0.30834951996803284\n",
      "epoch =  81  step =  150  of total steps  201  loss =  0.19695226848125458\n",
      "epoch =  81  step =  200  of total steps  201  loss =  0.23227918148040771\n",
      "epoch :  82  /  100  | TL :  0.20547723649672014  | VL :  0.22713953582569957  | VA :  92.60000000000001\n",
      "epoch =  82  step =  50  of total steps  201  loss =  0.07657729834318161\n",
      "epoch =  82  step =  100  of total steps  201  loss =  0.2268591821193695\n",
      "epoch =  82  step =  150  of total steps  201  loss =  0.1640445590019226\n",
      "epoch =  82  step =  200  of total steps  201  loss =  0.111177958548069\n",
      "epoch :  83  /  100  | TL :  0.2103200164711594  | VL :  0.21109152026474476  | VA :  92.80000000000001\n",
      "epoch =  83  step =  50  of total steps  201  loss =  0.18188142776489258\n",
      "epoch =  83  step =  100  of total steps  201  loss =  0.17850661277770996\n",
      "epoch =  83  step =  150  of total steps  201  loss =  0.1794665902853012\n",
      "epoch =  83  step =  200  of total steps  201  loss =  0.2127312868833542\n",
      "epoch :  84  /  100  | TL :  0.20450656605300618  | VL :  0.22554271388798952  | VA :  92.4\n",
      "epoch =  84  step =  50  of total steps  201  loss =  0.13326400518417358\n",
      "epoch =  84  step =  100  of total steps  201  loss =  0.32451581954956055\n",
      "epoch =  84  step =  150  of total steps  201  loss =  0.19068333506584167\n",
      "epoch =  84  step =  200  of total steps  201  loss =  0.23370476067066193\n",
      "epoch :  85  /  100  | TL :  0.2034654439941271  | VL :  0.22388161905109882  | VA :  92.0\n",
      "epoch =  85  step =  50  of total steps  201  loss =  0.2264157384634018\n",
      "epoch =  85  step =  100  of total steps  201  loss =  0.17930103838443756\n",
      "epoch =  85  step =  150  of total steps  201  loss =  0.28643620014190674\n",
      "epoch =  85  step =  200  of total steps  201  loss =  0.0823061391711235\n",
      "epoch :  86  /  100  | TL :  0.2143633349136037  | VL :  0.21005285438150167  | VA :  93.0\n",
      "epoch =  86  step =  50  of total steps  201  loss =  0.17814862728118896\n",
      "epoch =  86  step =  100  of total steps  201  loss =  0.12402208894491196\n",
      "epoch =  86  step =  150  of total steps  201  loss =  0.22491148114204407\n",
      "epoch =  86  step =  200  of total steps  201  loss =  0.09789026528596878\n",
      "epoch :  87  /  100  | TL :  0.20417668314567253  | VL :  0.22954990901052952  | VA :  92.80000000000001\n",
      "epoch =  87  step =  50  of total steps  201  loss =  0.24117319285869598\n",
      "epoch =  87  step =  100  of total steps  201  loss =  0.21433082222938538\n",
      "epoch =  87  step =  150  of total steps  201  loss =  0.09054842591285706\n",
      "epoch =  87  step =  200  of total steps  201  loss =  0.11260703951120377\n",
      "epoch :  88  /  100  | TL :  0.20974703362925134  | VL :  0.24320258479565382  | VA :  91.60000000000001\n",
      "epoch =  88  step =  50  of total steps  201  loss =  0.09938456863164902\n",
      "epoch =  88  step =  100  of total steps  201  loss =  0.20118078589439392\n",
      "epoch =  88  step =  150  of total steps  201  loss =  0.29939693212509155\n",
      "epoch =  88  step =  200  of total steps  201  loss =  0.013164497911930084\n",
      "epoch :  89  /  100  | TL :  0.20065677379123606  | VL :  0.22022621287032962  | VA :  92.2\n",
      "epoch =  89  step =  50  of total steps  201  loss =  0.3385891318321228\n",
      "epoch =  89  step =  100  of total steps  201  loss =  0.3317672312259674\n",
      "epoch =  89  step =  150  of total steps  201  loss =  0.08227661997079849\n",
      "epoch =  89  step =  200  of total steps  201  loss =  0.18702168762683868\n",
      "epoch :  90  /  100  | TL :  0.20265428720051376  | VL :  0.2158018909394741  | VA :  92.2\n",
      "epoch =  90  step =  50  of total steps  201  loss =  0.22040458023548126\n",
      "epoch =  90  step =  100  of total steps  201  loss =  0.08705513179302216\n",
      "epoch =  90  step =  150  of total steps  201  loss =  0.41403940320014954\n",
      "epoch =  90  step =  200  of total steps  201  loss =  0.4170619249343872\n",
      "epoch :  91  /  100  | TL :  0.20734668515659682  | VL :  0.2311275526881218  | VA :  93.0\n",
      "epoch =  91  step =  50  of total steps  201  loss =  0.1403256058692932\n",
      "epoch =  91  step =  100  of total steps  201  loss =  0.3317403793334961\n",
      "epoch =  91  step =  150  of total steps  201  loss =  0.1374824047088623\n",
      "epoch =  91  step =  200  of total steps  201  loss =  0.12162411957979202\n",
      "epoch :  92  /  100  | TL :  0.20979169680185578  | VL :  0.21275818068534136  | VA :  92.80000000000001\n",
      "epoch =  92  step =  50  of total steps  201  loss =  0.18673966825008392\n",
      "epoch =  92  step =  100  of total steps  201  loss =  0.25262266397476196\n",
      "epoch =  92  step =  150  of total steps  201  loss =  0.03552558645606041\n",
      "epoch =  92  step =  200  of total steps  201  loss =  0.2393452227115631\n",
      "epoch :  93  /  100  | TL :  0.2026126131934313  | VL :  0.2162389401346445  | VA :  92.60000000000001\n",
      "epoch =  93  step =  50  of total steps  201  loss =  0.2600654661655426\n",
      "epoch =  93  step =  100  of total steps  201  loss =  0.28647541999816895\n",
      "epoch =  93  step =  150  of total steps  201  loss =  0.2428654432296753\n",
      "epoch =  93  step =  200  of total steps  201  loss =  0.20070718228816986\n",
      "epoch :  94  /  100  | TL :  0.2016612942242504  | VL :  0.21577192237600684  | VA :  93.2\n",
      "epoch =  94  step =  50  of total steps  201  loss =  0.0754617378115654\n",
      "epoch =  94  step =  100  of total steps  201  loss =  0.17771326005458832\n",
      "epoch =  94  step =  150  of total steps  201  loss =  0.2138068825006485\n",
      "epoch =  94  step =  200  of total steps  201  loss =  0.2033987045288086\n",
      "epoch :  95  /  100  | TL :  0.20629653981446627  | VL :  0.2250111335888505  | VA :  91.60000000000001\n",
      "epoch =  95  step =  50  of total steps  201  loss =  0.39347022771835327\n",
      "epoch =  95  step =  100  of total steps  201  loss =  0.20541876554489136\n",
      "epoch =  95  step =  150  of total steps  201  loss =  0.14931735396385193\n",
      "epoch =  95  step =  200  of total steps  201  loss =  0.1864319145679474\n",
      "epoch :  96  /  100  | TL :  0.20897940262707312  | VL :  0.21529705123975873  | VA :  92.60000000000001\n",
      "epoch =  96  step =  50  of total steps  201  loss =  0.1503058820962906\n",
      "epoch =  96  step =  100  of total steps  201  loss =  0.11603047698736191\n",
      "epoch =  96  step =  150  of total steps  201  loss =  0.2402314841747284\n",
      "epoch =  96  step =  200  of total steps  201  loss =  0.1159212589263916\n",
      "epoch :  97  /  100  | TL :  0.21012226340189502  | VL :  0.22431956324726343  | VA :  93.0\n",
      "epoch =  97  step =  50  of total steps  201  loss =  0.09696374088525772\n",
      "epoch =  97  step =  100  of total steps  201  loss =  0.24621430039405823\n",
      "epoch =  97  step =  150  of total steps  201  loss =  0.24428413808345795\n",
      "epoch =  97  step =  200  of total steps  201  loss =  0.19212937355041504\n",
      "epoch :  98  /  100  | TL :  0.2006781366815911  | VL :  0.2121526994742453  | VA :  92.2\n",
      "epoch =  98  step =  50  of total steps  201  loss =  0.1489926427602768\n",
      "epoch =  98  step =  100  of total steps  201  loss =  0.2750423848628998\n",
      "epoch =  98  step =  150  of total steps  201  loss =  0.2103877067565918\n",
      "epoch =  98  step =  200  of total steps  201  loss =  0.3412286043167114\n",
      "epoch :  99  /  100  | TL :  0.20803386571617863  | VL :  0.22973662102594972  | VA :  92.0\n",
      "epoch =  99  step =  50  of total steps  201  loss =  0.17238685488700867\n",
      "epoch =  99  step =  100  of total steps  201  loss =  0.2040010392665863\n",
      "epoch =  99  step =  150  of total steps  201  loss =  0.329231858253479\n",
      "epoch =  99  step =  200  of total steps  201  loss =  0.24196979403495789\n",
      "epoch :  100  /  100  | TL :  0.21192237371532477  | VL :  0.22136135585606098  | VA :  92.4\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"IOZ5docSriEdGRdQmdXQn9kpu\",\n",
    "                        project_name=\"kd0\", workspace=\"akshaykvnit\")\n",
    "experiment.log_parameters(hyper_params)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = hyper_params[\"learning_rate\"])\n",
    "total_step = len(data.train_ds) // hyper_params[\"batch_size\"]\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 0\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    trn = []\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(data.train_dl) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = torch.autograd.Variable(images).cuda().float()\n",
    "            labels = torch.autograd.Variable(labels).cuda()\n",
    "        else : \n",
    "            images = torch.autograd.Variable(images).float()\n",
    "            labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        y_pred = net(images)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, labels)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49 :\n",
    "            print('epoch = ', epoch, ' step = ', i + 1, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "\n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(data.valid_dl) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = torch.autograd.Variable(images).cuda().float()\n",
    "                labels = torch.autograd.Variable(labels).cuda()\n",
    "            else : \n",
    "                images = torch.autograd.Variable(images).float()\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(images)\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, labels)\n",
    "            val.append(loss.item())\n",
    "\n",
    "    val_loss = sum(val) / len(val)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc = _get_accuracy(data.valid_dl, net)\n",
    "\n",
    "    print('epoch : ', epoch + 1, ' / ', hyper_params[\"num_epochs\"], ' | TL : ', train_loss, ' | VL : ', val_loss, ' | VA : ', val_acc * 100)\n",
    "    experiment.log_metric(\"train_loss\", train_loss)\n",
    "    experiment.log_metric(\"val_loss\", val_loss)\n",
    "    experiment.log_metric(\"val_acc\", val_acc)\n",
    "\n",
    "    if (val_acc * 100) > min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_acc * 100\n",
    "        torch.save(net.state_dict(), '../saved_models/classifier/model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942\n",
      "0.936\n"
     ]
    }
   ],
   "source": [
    "net.cpu()\n",
    "net.load_state_dict(torch.load('../saved_models/classifier/model4.pt', map_location = 'cpu'))\n",
    "net.cuda()\n",
    "\n",
    "learn = cnn_learner(data, models.resnet34, metrics = accuracy)\n",
    "learn = learn.load('unfreeze_imagenet_bs64')\n",
    "learn.freeze()\n",
    "\n",
    "print(_get_accuracy(data.valid_dl, net))\n",
    "print(_get_accuracy(data.valid_dl, learn.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ak_fastai)",
   "language": "python",
   "name": "ak_fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
